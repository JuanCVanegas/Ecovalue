{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Interpolation/InterpolatedDenMonth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATES</th>\n",
       "      <th>D REVENUE</th>\n",
       "      <th>U CR</th>\n",
       "      <th>D OE</th>\n",
       "      <th>D NOI</th>\n",
       "      <th>U CAPEX</th>\n",
       "      <th>U CWK</th>\n",
       "      <th>D FCF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>1884.372544</td>\n",
       "      <td>976.202014</td>\n",
       "      <td>475.249997</td>\n",
       "      <td>757.519678</td>\n",
       "      <td>207.477947</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>856.600959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1884.566826</td>\n",
       "      <td>983.762225</td>\n",
       "      <td>485.004015</td>\n",
       "      <td>734.017979</td>\n",
       "      <td>207.303532</td>\n",
       "      <td>3638.472896</td>\n",
       "      <td>810.859727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1884.761107</td>\n",
       "      <td>991.322435</td>\n",
       "      <td>494.758033</td>\n",
       "      <td>710.516281</td>\n",
       "      <td>207.129117</td>\n",
       "      <td>3676.945791</td>\n",
       "      <td>765.118495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1884.955389</td>\n",
       "      <td>998.882646</td>\n",
       "      <td>504.512051</td>\n",
       "      <td>687.014582</td>\n",
       "      <td>206.954702</td>\n",
       "      <td>3715.418687</td>\n",
       "      <td>719.377263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1880.767673</td>\n",
       "      <td>1006.377690</td>\n",
       "      <td>481.542613</td>\n",
       "      <td>511.922217</td>\n",
       "      <td>207.283705</td>\n",
       "      <td>3792.839197</td>\n",
       "      <td>732.414605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>1785.193900</td>\n",
       "      <td>972.274049</td>\n",
       "      <td>461.469501</td>\n",
       "      <td>414.377788</td>\n",
       "      <td>98.125058</td>\n",
       "      <td>2795.231076</td>\n",
       "      <td>733.164793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>1730.706270</td>\n",
       "      <td>953.165883</td>\n",
       "      <td>488.056175</td>\n",
       "      <td>424.307526</td>\n",
       "      <td>95.323030</td>\n",
       "      <td>2858.452805</td>\n",
       "      <td>729.764211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>1667.921494</td>\n",
       "      <td>856.900384</td>\n",
       "      <td>471.261535</td>\n",
       "      <td>434.874583</td>\n",
       "      <td>92.144220</td>\n",
       "      <td>2849.606546</td>\n",
       "      <td>737.713784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1605.136718</td>\n",
       "      <td>760.634885</td>\n",
       "      <td>454.466895</td>\n",
       "      <td>445.441639</td>\n",
       "      <td>88.965411</td>\n",
       "      <td>2840.760287</td>\n",
       "      <td>745.663358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1542.351942</td>\n",
       "      <td>664.369385</td>\n",
       "      <td>437.672255</td>\n",
       "      <td>456.008696</td>\n",
       "      <td>85.786601</td>\n",
       "      <td>2831.914028</td>\n",
       "      <td>753.612931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DATES    D REVENUE         U CR        D OE       D NOI     U CAPEX  \\\n",
       "0    2009-12-31  1884.372544   976.202014  475.249997  757.519678  207.477947   \n",
       "1    2010-01-31  1884.566826   983.762225  485.004015  734.017979  207.303532   \n",
       "2    2010-02-28  1884.761107   991.322435  494.758033  710.516281  207.129117   \n",
       "3    2010-03-31  1884.955389   998.882646  504.512051  687.014582  206.954702   \n",
       "4    2010-04-30  1880.767673  1006.377690  481.542613  511.922217  207.283705   \n",
       "..          ...          ...          ...         ...         ...         ...   \n",
       "104  2018-08-31  1785.193900   972.274049  461.469501  414.377788   98.125058   \n",
       "105  2018-09-30  1730.706270   953.165883  488.056175  424.307526   95.323030   \n",
       "106  2018-10-31  1667.921494   856.900384  471.261535  434.874583   92.144220   \n",
       "107  2018-11-30  1605.136718   760.634885  454.466895  445.441639   88.965411   \n",
       "108  2018-12-31  1542.351942   664.369385  437.672255  456.008696   85.786601   \n",
       "\n",
       "           U CWK       D FCF  \n",
       "0    3600.000000  856.600959  \n",
       "1    3638.472896  810.859727  \n",
       "2    3676.945791  765.118495  \n",
       "3    3715.418687  719.377263  \n",
       "4    3792.839197  732.414605  \n",
       "..           ...         ...  \n",
       "104  2795.231076  733.164793  \n",
       "105  2858.452805  729.764211  \n",
       "106  2849.606546  737.713784  \n",
       "107  2840.760287  745.663358  \n",
       "108  2831.914028  753.612931  \n",
       "\n",
       "[109 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2009-12-31', 1884.372544, 976.2020142, 475.24999739999987,\n",
       "        757.519678, 207.4779469, 3600.0, 856.6009594000002],\n",
       "       ['2010-01-31', 1884.5668256666668, 983.7622247333335, 485.0040154,\n",
       "        734.0179793333333, 207.30353190000002, 3638.472895666667,\n",
       "        810.8597272333334],\n",
       "       ['2010-02-28', 1884.761107333333, 991.3224352666666,\n",
       "        494.75803339999993, 710.5162806666667, 207.12911689999999,\n",
       "        3676.945791333333, 765.1184950666667],\n",
       "       ['2010-03-31', 1884.955389, 998.8826458, 504.5120514, 687.014582,\n",
       "        206.9547019, 3715.418687, 719.3772629],\n",
       "       ['2010-04-30', 1880.767673333333, 1006.3776901999997,\n",
       "        481.5426128333333, 511.92221730000006, 207.28370463333331,\n",
       "        3792.8391966666654, 732.4146046],\n",
       "       ['2010-05-31', 1876.5799576666668, 1013.8727346000001,\n",
       "        458.5731742666666, 336.8298526000001, 207.61270736666665,\n",
       "        3870.2597063333333, 745.4519462999999],\n",
       "       ['2010-06-30', 1872.392242, 1021.367779, 435.6037357, 161.7374879,\n",
       "        207.9417101, 3947.680216, 758.489288],\n",
       "       ['2010-07-31', 1853.240114, 1015.1698886666666, 421.8261523,\n",
       "        187.6358415, 207.7144804333333, 3965.748594666666,\n",
       "        756.6899263333332],\n",
       "       ['2010-08-31', 1834.087986, 1008.9719983333333, 408.0485689,\n",
       "        213.5341951, 207.48725076666665, 3983.816973333333,\n",
       "        754.8905646666667],\n",
       "       ['2010-09-30', 1814.935858, 1002.774108, 394.2709855, 239.4325487,\n",
       "        207.2600211, 4001.885352, 753.0912030000002],\n",
       "       ['2010-10-31', 1851.320774, 1008.5464486666666, 411.1460836666666,\n",
       "        246.73378476666667, 212.21753106666665, 4093.646761333333,\n",
       "        715.3372166666667],\n",
       "       ['2010-11-30', 1887.70569, 1014.3187893333335, 428.0211818333333,\n",
       "        254.03502083333333, 217.1750410333333, 4185.408170666667,\n",
       "        677.5832303333334],\n",
       "       ['2010-12-31', 1924.090606, 1020.09113, 444.89628, 261.3362569,\n",
       "        222.132551, 4277.16958, 639.829244],\n",
       "       ['2011-01-31', 1840.631611, 1004.3123142000001, 439.1857453333333,\n",
       "        253.3520595, 221.3439679, 4255.687593333333, 625.4113584666667],\n",
       "       ['2011-02-28', 1757.172616, 988.5334983999999, 433.47521066666667,\n",
       "        245.36786209999997, 220.5553848, 4234.205606666666,\n",
       "        610.9934729333334],\n",
       "       ['2011-03-31', 1673.713621, 972.7546826, 427.764676, 237.3836647,\n",
       "        219.7668017, 4212.723620000002, 596.5755874],\n",
       "       ['2011-04-30', 1697.5040236666666, 943.8024453, 430.2420961,\n",
       "        237.04147333333333, 223.6841742333333, 4253.569522333334,\n",
       "        596.6860897999999],\n",
       "       ['2011-05-31', 1721.2944263333334, 914.850208, 432.71951619999993,\n",
       "        236.69928196666663, 227.60154676666667, 4294.415424666667,\n",
       "        596.7965922000002],\n",
       "       ['2011-06-30', 1745.084829, 885.8979707000002, 435.1969363,\n",
       "        236.3570906, 231.5189193, 4335.261327, 596.9070946],\n",
       "       ['2011-07-31', 1775.998593, 875.2949231666668, 444.6121419,\n",
       "        236.2388491, 227.72962546666668, 4329.278552666667,\n",
       "        628.3878588333333],\n",
       "       ['2011-08-31', 1806.912357, 864.6918756333333, 454.02734749999996,\n",
       "        236.1206076, 223.94033163333333, 4323.295778333333,\n",
       "        659.8686230666667],\n",
       "       ['2011-09-30', 1837.826121, 854.0888281, 463.44255310000005,\n",
       "        236.0023661, 220.1510378, 4317.313004, 691.3493873],\n",
       "       ['2011-10-31', 1885.0705423333332, 857.2014145333335,\n",
       "        470.0062231666667, 244.5463731, 212.89141523333333,\n",
       "        4309.167693666666, 690.0065109333333],\n",
       "       ['2011-11-30', 1932.314963666667, 860.3140009666665,\n",
       "        476.56989323333335, 253.0903801, 205.63179266666666,\n",
       "        4301.022383333333, 688.6636345666667],\n",
       "       ['2011-12-31', 1979.559385, 863.4265874, 483.13356330000005,\n",
       "        261.6343871, 198.3721701, 4292.877073, 687.3207582],\n",
       "       ['2012-01-31', 1910.3834399999998, 858.4811889666668,\n",
       "        483.72593293333335, 263.6296255, 198.3738698333333, 4291.272806,\n",
       "        711.6253066666667],\n",
       "       ['2012-02-29', 1841.2074949999999, 853.5357905333334,\n",
       "        484.3183025666667, 265.6248639, 198.37556956666668,\n",
       "        4289.668538999998, 735.9298551333335],\n",
       "       ['2012-03-31', 1772.03155, 848.5903921, 484.9106722, 267.6201023,\n",
       "        198.3772693, 4288.064272, 760.2344036000002],\n",
       "       ['2012-04-30', 1805.3226533333332, 852.7747259666667,\n",
       "        500.10971653333337, 262.53434903333334, 203.80507153333332,\n",
       "        4248.180273333333, 760.4925157666668],\n",
       "       ['2012-05-31', 1838.6137566666666, 856.9590598333334,\n",
       "        515.3087608666667, 257.44859576666664, 209.23287376666664,\n",
       "        4208.296274666666, 760.7506279333335],\n",
       "       ['2012-06-30', 1871.90486, 861.1433937, 530.5078052, 252.3628425,\n",
       "        214.660676, 4168.412276, 761.0087401000002],\n",
       "       ['2012-07-31', 1870.0189613333332, 888.6962120999999,\n",
       "        512.9147551333333, 274.75087963333334, 210.84279596666664,\n",
       "        4100.139316, 761.8369197333334],\n",
       "       ['2012-08-31', 1868.1330626666668, 916.2490305,\n",
       "        495.32170506666665, 297.1389167666666, 207.02491593333332,\n",
       "        4031.8663560000005, 762.6650993666667],\n",
       "       ['2012-09-30', 1866.247164, 943.8018489, 477.728655, 319.5269539,\n",
       "        203.2070359, 3963.593396, 763.493279],\n",
       "       ['2012-10-31', 1872.4550559999998, 871.4747433666665,\n",
       "        485.99178150000006, 323.89898873333334, 198.78250549999998,\n",
       "        3581.357899333333, 776.451154],\n",
       "       ['2012-11-30', 1878.6629480000001, 799.1476378333333, 494.254908,\n",
       "        328.27102356666666, 194.3579751, 3199.1224026666664, 789.409029],\n",
       "       ['2012-12-31', 1884.87084, 726.8205323, 502.5180345, 332.6430584,\n",
       "        189.9334447, 2816.886906, 802.366904],\n",
       "       ['2013-01-31', 1896.30791, 753.0427803333333, 514.5447965666667,\n",
       "        333.5149285333333, 192.01279616666667, 2893.339288,\n",
       "        793.3964250333332],\n",
       "       ['2013-02-28', 1907.7449800000002, 779.2650283666666,\n",
       "        526.5715586333333, 334.38679866666666, 194.09214763333333,\n",
       "        2969.79167, 784.4259460666666],\n",
       "       ['2013-03-31', 1919.18205, 805.4872763999998, 538.5983207,\n",
       "        335.2586688, 196.1714991, 3046.244052, 775.4554671],\n",
       "       ['2013-04-30', 1867.7861953333331, 783.4079046999999,\n",
       "        530.8571651000001, 335.86746453333336, 195.6670658333333,\n",
       "        3048.921046333333, 766.0225778666667],\n",
       "       ['2013-05-31', 1816.3903406666666, 761.328533, 523.1160095,\n",
       "        336.47626026666666, 195.16263256666667, 3051.5980406666667,\n",
       "        756.5896886333333],\n",
       "       ['2013-06-30', 1764.994486, 739.2491613, 515.3748539, 337.085056,\n",
       "        194.6581993, 3054.275035, 747.1567994],\n",
       "       ['2013-07-31', 1768.6169343333331, 757.7170030999998, 520.4034373,\n",
       "        329.2451542333333, 198.2425908, 2952.306149666667, 757.3171348],\n",
       "       ['2013-08-31', 1772.2393826666666, 776.1848449, 525.4320207000002,\n",
       "        321.40525246666664, 201.8269823, 2850.337264333333,\n",
       "        767.4774702000001],\n",
       "       ['2013-09-30', 1775.861831, 794.6526867, 530.4606041000002,\n",
       "        313.5653506999999, 205.4113738, 2748.368379, 777.6378056],\n",
       "       ['2013-10-31', 1781.312525333333, 813.7964820333333,\n",
       "        531.6057520333334, 320.4643399333333, 203.1795522,\n",
       "        2446.6776800000002, 776.7867944],\n",
       "       ['2013-11-30', 1786.7632196666668, 832.9402773666667,\n",
       "        532.7508999666667, 327.36332916666663, 200.9477306, 2144.986981,\n",
       "        775.9357832000001],\n",
       "       ['2013-12-31', 1792.2139140000004, 852.0840727, 533.8960479,\n",
       "        334.2623184, 198.715909, 1843.296282, 775.0847719999998],\n",
       "       ['2014-01-31', 1772.1792736666669, 861.7296213999998, 528.3798523,\n",
       "        342.6429393, 191.0202412333333, 1839.5208699999998,\n",
       "        774.5279383333333],\n",
       "       ['2014-02-28', 1752.144633333333, 871.3751701, 522.8636567,\n",
       "        351.0235602, 183.32457346666664, 1835.7454579999999,\n",
       "        773.9711046666665],\n",
       "       ['2014-03-31', 1732.109993, 881.0207187999998, 517.3474611,\n",
       "        359.4041811, 175.6289057, 1831.970046, 773.414271],\n",
       "       ['2014-04-30', 1721.8949903333335, 890.6855609999999,\n",
       "        507.92859023333335, 349.91219946666666, 168.82089786666666,\n",
       "        1848.738663666667, 760.1008118333333],\n",
       "       ['2014-05-31', 1711.6799876666666, 900.3504032,\n",
       "        498.50971936666673, 340.4202178333333, 162.01289003333332,\n",
       "        1865.507281333333, 746.7873526666666],\n",
       "       ['2014-06-30', 1701.464985, 910.0152454, 489.09084850000005,\n",
       "        330.9282362, 155.2048822, 1882.275899, 733.4738934999998],\n",
       "       ['2014-07-31', 1728.4231946666666, 914.2662125, 481.6856930666667,\n",
       "        331.95640223333334, 150.8917052333333, 1896.6301780000001,\n",
       "        727.0572944999999],\n",
       "       ['2014-08-31', 1755.3814043333334, 918.5171796,\n",
       "        474.28053763333327, 332.98456826666666, 146.57852826666667,\n",
       "        1910.984457, 720.6406955],\n",
       "       ['2014-09-30', 1782.339614, 922.7681467, 466.8753822,\n",
       "        334.0127343000001, 142.2653513, 1925.338736, 714.2240965],\n",
       "       ['2014-10-31', 1725.137324333333, 965.4992654666668,\n",
       "        478.28002593333326, 415.6834258, 141.41725303333334,\n",
       "        1946.0452103333328, 728.1806636666665],\n",
       "       ['2014-11-30', 1667.935034666667, 1008.2303842333333,\n",
       "        489.68466966666665, 497.35411730000004, 140.56915476666666,\n",
       "        1966.751684666667, 742.1372308333333],\n",
       "       ['2014-12-31', 1610.732745, 1050.961503, 501.0893134, 579.0248088,\n",
       "        139.7210565, 1987.458159, 756.093798],\n",
       "       ['2015-01-31', 1746.422448666667, 1054.429111, 508.5803035333333,\n",
       "        579.2034107000001, 137.07026573333334, 2005.468421,\n",
       "        771.8714318333333],\n",
       "       ['2015-02-28', 1882.112152333333, 1057.8967189999998,\n",
       "        516.0712936666666, 579.3820125999998, 134.41947496666666,\n",
       "        2023.478683, 787.6490656666667],\n",
       "       ['2015-03-31', 2017.801856, 1061.364327, 523.5622838,\n",
       "        579.5606144999998, 131.7686842, 2041.488945, 803.4266995],\n",
       "       ['2015-04-30', 2107.344116, 1096.44801, 526.1169451000002,\n",
       "        577.4851386666667, 131.0318083, 2038.723757, 790.2704643666667],\n",
       "       ['2015-05-31', 2196.8863760000004, 1131.531693, 528.6716064,\n",
       "        575.4096628333333, 130.2949324, 2035.9585690000001,\n",
       "        777.1142292333334],\n",
       "       ['2015-06-30', 2286.428636, 1166.615376, 531.2262677, 573.334187,\n",
       "        129.5580565, 2033.193381, 763.9579941000002],\n",
       "       ['2015-07-31', 2528.4880129999997, 1194.063472, 562.655131,\n",
       "        551.8155191666667, 128.9129271, 2038.7990923333327, 725.2341621],\n",
       "       ['2015-08-31', 2770.5473899999997, 1221.511568, 594.0839943000002,\n",
       "        530.2968513333334, 128.2677977, 2044.4048036666666, 686.5103301],\n",
       "       ['2015-09-30', 3012.606767, 1248.959664, 625.5128576000002,\n",
       "        508.7781835, 127.6226683, 2050.010515, 647.7864981],\n",
       "       ['2015-10-31', 2903.7366663333332, 1269.6701176666666,\n",
       "        641.4096041, 508.9514627, 117.39104201, 2031.2492326666666,\n",
       "        646.7970369666667],\n",
       "       ['2015-11-30', 2794.8665656666667, 1290.3805713333334,\n",
       "        657.3063506000002, 509.1247419, 107.15941572, 2012.4879503333332,\n",
       "        645.8075758333333],\n",
       "       ['2015-12-31', 2685.996465, 1311.091025, 673.2030971, 509.2980211,\n",
       "        96.92778943, 1993.726668, 644.8181147000001],\n",
       "       ['2016-01-31', 2834.140780666666, 1366.4444733333332,\n",
       "        674.0792667333334, 500.8613563, 382.7185262866667, 2146.965891,\n",
       "        645.8685558666667],\n",
       "       ['2016-02-29', 2982.285096333333, 1421.7979216666665,\n",
       "        674.9554363666666, 492.4246915, 668.5092631433333, 2300.205114,\n",
       "        646.9189970333333],\n",
       "       ['2016-03-31', 3130.429412, 1477.1513699999996, 675.831606,\n",
       "        483.9880267, 954.3, 2453.444337, 647.9694382],\n",
       "       ['2016-04-30', 3060.8571920000004, 1508.7480799999996,\n",
       "        653.6836567666667, 473.8452801, 669.9162089666665,\n",
       "        2448.972803333333, 637.6159774666667],\n",
       "       ['2016-05-31', 2991.284972, 1540.34479, 631.5357075333333,\n",
       "        463.7025335, 385.5324179333333, 2444.5012696666668,\n",
       "        627.2625167333333],\n",
       "       ['2016-06-30', 2921.712752, 1571.9415, 609.3877583, 453.5597869,\n",
       "        101.1486269, 2440.029736, 616.909056],\n",
       "       ['2016-07-31', 2979.422658, 1553.679082333333, 612.0762081333334,\n",
       "        473.88566743333337, 103.19915363333334, 2475.9393219999997,\n",
       "        608.7162381333333],\n",
       "       ['2016-08-31', 3037.132564, 1535.416664666667, 614.7646579666666,\n",
       "        494.2115479666666, 105.24968036666668, 2511.848908,\n",
       "        600.5234202666667],\n",
       "       ['2016-09-30', 3094.84247, 1517.1542470000004, 617.4531078,\n",
       "        514.5374285, 107.3002071, 2547.758494, 592.3306024],\n",
       "       ['2016-10-31', 2941.914112333333, 1493.7506856666669,\n",
       "        601.3888980666667, 497.1393087333333, 106.27214706666666,\n",
       "        2492.278597666667, 605.3801967666667],\n",
       "       ['2016-11-30', 2788.9857546666667, 1470.347124333333,\n",
       "        585.3246883333334, 479.7411889666667, 105.24408703333332,\n",
       "        2436.798701333333, 618.4297911333333],\n",
       "       ['2016-12-31', 2636.057397, 1446.943563, 569.2604786, 462.3430692,\n",
       "        104.216027, 2381.318805, 631.4793855],\n",
       "       ['2017-01-31', 2629.386708333333, 1421.7832626666666,\n",
       "        553.3237412999999, 463.41115033333335, 105.35134376666666,\n",
       "        2394.287123333333, 669.1150419666667],\n",
       "       ['2017-02-28', 2622.7160196666664, 1396.6229623333334,\n",
       "        537.3870039999998, 464.47923146666665, 106.48666053333334,\n",
       "        2407.2554416666667, 706.7506984333335],\n",
       "       ['2017-03-31', 2616.045331, 1371.462662, 521.4502666999998,\n",
       "        465.5473126, 107.6219773, 2420.2237600000008, 744.3863549],\n",
       "       ['2017-04-30', 2582.999953333333, 1385.6997396666666,\n",
       "        514.2855105000001, 456.63566276666666, 110.51347520000002,\n",
       "        2409.258488666667, 756.4180495333335],\n",
       "       ['2017-05-31', 2549.9545756666666, 1399.9368173333332,\n",
       "        507.12075430000004, 447.72401293333337, 113.4049731,\n",
       "        2398.293217333333, 768.4497441666666],\n",
       "       ['2017-06-30', 2516.909198, 1414.173895, 499.9559981, 438.8123631,\n",
       "        116.296471, 2387.327946, 780.4814388],\n",
       "       ['2017-07-31', 2529.782464, 1411.1019353333334,\n",
       "        495.39090156666674, 419.9090564333334, 113.3286994,\n",
       "        2482.2845416666664, 773.9692328666665],\n",
       "       ['2017-08-31', 2542.65573, 1408.0299756666666, 490.8258050333334,\n",
       "        401.00574976666667, 110.3609278, 2577.241137333333,\n",
       "        767.4570269333334],\n",
       "       ['2017-09-30', 2555.528996, 1404.958016, 486.2607085, 382.1024431,\n",
       "        107.3931562, 2672.197733, 760.944821],\n",
       "       ['2017-10-31', 2493.5604476666667, 1340.4287853333333,\n",
       "        485.59986143333333, 386.4377233, 106.82599553333333,\n",
       "        2640.667416333333, 748.7807167999999],\n",
       "       ['2017-11-30', 2431.591899333333, 1275.8995546666667,\n",
       "        484.9390143666667, 390.7730035, 106.25883486666667,\n",
       "        2609.1370996666665, 736.6166125999998],\n",
       "       ['2017-12-31', 2369.623351, 1211.370324, 484.27816730000006,\n",
       "        395.1082837, 105.6916742, 2577.606783, 724.4525083999998],\n",
       "       ['2018-01-31', 2307.4944116666666, 1212.1657916666666,\n",
       "        479.0572197666667, 392.9066318, 102.59903074333334,\n",
       "        2577.6159756666666, 729.9813198666666],\n",
       "       ['2018-02-28', 2245.365472333333, 1212.9612593333334,\n",
       "        473.83627223333326, 390.7049799, 99.50638728666668,\n",
       "        2577.625168333333, 735.5101313333333],\n",
       "       ['2018-03-31', 2183.236533, 1213.756727, 468.6153247, 388.503328,\n",
       "        96.41374383, 2577.634361, 741.0389428],\n",
       "       ['2018-04-30', 2086.8807423333333, 1146.0012783333334,\n",
       "        448.50893473333326, 390.50832306666666, 98.85220068666668,\n",
       "        2608.0187796666664, 740.6812809333335],\n",
       "       ['2018-05-31', 1990.5249516666665, 1078.2458296666666,\n",
       "        428.40254476666667, 392.51331813333326, 101.29065754333334,\n",
       "        2638.403198333333, 740.3236190666668],\n",
       "       ['2018-06-30', 1894.169161, 1010.490381, 408.2961548, 394.5183132,\n",
       "        103.7291144, 2668.787617, 739.9659572],\n",
       "       ['2018-07-31', 1839.681530666667, 991.3822151, 434.8828281333333,\n",
       "        404.4480508333333, 100.92708618333334, 2732.0093463333333,\n",
       "        736.5653751333334],\n",
       "       ['2018-08-31', 1785.1939003333332, 972.2740492,\n",
       "        461.46950146666666, 414.37778846666674, 98.12505796666667,\n",
       "        2795.231075666666, 733.1647930666667],\n",
       "       ['2018-09-30', 1730.70627, 953.1658833, 488.0561748, 424.3075261,\n",
       "        95.32302975, 2858.452805, 729.764211],\n",
       "       ['2018-10-31', 1667.921494, 856.9003839999998, 471.2615347666666,\n",
       "        434.87458276666666, 92.14422016333332, 2849.606546,\n",
       "        737.7137843333335],\n",
       "       ['2018-11-30', 1605.136718, 760.6348846999998, 454.4668947333333,\n",
       "        445.44163943333336, 88.96541057666668, 2840.760287,\n",
       "        745.6633576666667],\n",
       "       ['2018-12-31', 1542.351942, 664.3693853999998, 437.67225470000005,\n",
       "        456.0086961, 85.78660099, 2831.914028, 753.612931]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,1:7]\n",
    "Y = dataset[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21536771, 0.34358992, 0.28760773, 1.        , 0.14011453,\n",
       "        0.70628215],\n",
       "       [0.21549004, 0.35192007, 0.32225043, 0.9605532 , 0.13991371,\n",
       "        0.72165108],\n",
       "       [0.21561238, 0.36025022, 0.35689312, 0.92110641, 0.13971289,\n",
       "        0.73702   ],\n",
       "       [0.21573472, 0.36858036, 0.39153581, 0.88165961, 0.13951207,\n",
       "        0.75238893],\n",
       "       [0.21309775, 0.37683871, 0.30995679, 0.58777307, 0.13989088,\n",
       "        0.78331641],\n",
       "       [0.21046078, 0.38509706, 0.22837778, 0.29388654, 0.14026969,\n",
       "        0.8142439 ],\n",
       "       [0.2078238 , 0.3933554 , 0.14679876, 0.        , 0.1406485 ,\n",
       "        0.84517139],\n",
       "       [0.19576386, 0.38652631, 0.09786584, 0.0434695 , 0.14038687,\n",
       "        0.85238924],\n",
       "       [0.18370391, 0.37969722, 0.04893292, 0.086939  , 0.14012524,\n",
       "        0.85960709],\n",
       "       [0.17164397, 0.37286814, 0.        , 0.1304085 , 0.13986361,\n",
       "        0.86682494],\n",
       "       [0.19455526, 0.37922834, 0.05993416, 0.14266337, 0.14557165,\n",
       "        0.90348124],\n",
       "       [0.21746656, 0.38558854, 0.11986831, 0.15491825, 0.15127969,\n",
       "        0.94013755],\n",
       "       [0.24037786, 0.39194874, 0.17980247, 0.16717312, 0.15698773,\n",
       "        0.97679385],\n",
       "       [0.18782438, 0.37456299, 0.15952074, 0.15377192, 0.15607976,\n",
       "        0.96821236],\n",
       "       [0.1352709 , 0.35717725, 0.13923902, 0.14037072, 0.1551718 ,\n",
       "        0.95963086],\n",
       "       [0.08271742, 0.33979151, 0.1189573 , 0.12696952, 0.15426383,\n",
       "        0.95104936],\n",
       "       [0.09769806, 0.30789075, 0.12775618, 0.12639516, 0.15877426,\n",
       "        0.96736624],\n",
       "       [0.11267869, 0.27598999, 0.13655507, 0.1258208 , 0.1632847 ,\n",
       "        0.98368312],\n",
       "       [0.12765932, 0.24408924, 0.14535396, 0.12524645, 0.16779513,\n",
       "        1.        ],\n",
       "       [0.14712547, 0.23240637, 0.17879331, 0.12504798, 0.16343216,\n",
       "        0.99761004],\n",
       "       [0.16659163, 0.2207235 , 0.21223267, 0.12484952, 0.1590692 ,\n",
       "        0.99522007],\n",
       "       [0.18605779, 0.20904063, 0.24567202, 0.12465105, 0.15470623,\n",
       "        0.99283011],\n",
       "       [0.21580723, 0.2124702 , 0.26898377, 0.13899188, 0.14634756,\n",
       "        0.98957627],\n",
       "       [0.24555667, 0.21589978, 0.29229552, 0.1533327 , 0.13798888,\n",
       "        0.98632243],\n",
       "       [0.27530612, 0.21932935, 0.31560727, 0.16767352, 0.1296302 ,\n",
       "        0.98306859],\n",
       "       [0.23174656, 0.21388031, 0.31771115, 0.17102246, 0.12963216,\n",
       "        0.98242773],\n",
       "       [0.18818701, 0.20843127, 0.31981503, 0.1743714 , 0.12963412,\n",
       "        0.98178686],\n",
       "       [0.14462746, 0.20298222, 0.32191891, 0.17772034, 0.12963608,\n",
       "        0.981146  ],\n",
       "       [0.16559061, 0.20759269, 0.37590033, 0.16918408, 0.13588561,\n",
       "        0.96521338],\n",
       "       [0.18655375, 0.21220316, 0.42988176, 0.16064782, 0.14213514,\n",
       "        0.94928075],\n",
       "       [0.2075169 , 0.21681363, 0.48386319, 0.15211155, 0.14838467,\n",
       "        0.93334813],\n",
       "       [0.20632937, 0.24717245, 0.42137913, 0.18968911, 0.14398879,\n",
       "        0.90607485],\n",
       "       [0.20514183, 0.27753127, 0.35889507, 0.22726666, 0.13959291,\n",
       "        0.87880157],\n",
       "       [0.2039543 , 0.30789009, 0.29641102, 0.26484421, 0.13519703,\n",
       "        0.85152829],\n",
       "       [0.20786336, 0.22819714, 0.32575861, 0.27218252, 0.13010266,\n",
       "        0.69883512],\n",
       "       [0.21177242, 0.14850418, 0.3551062 , 0.27952084, 0.12500829,\n",
       "        0.54614194],\n",
       "       [0.21568148, 0.06881122, 0.38445379, 0.28685915, 0.11991392,\n",
       "        0.39344876],\n",
       "       [0.22288331, 0.09770397, 0.42716844, 0.28832255, 0.12230807,\n",
       "        0.42398951],\n",
       "       [0.23008515, 0.12659671, 0.46988309, 0.28978595, 0.12470222,\n",
       "        0.45453025],\n",
       "       [0.23728698, 0.15548945, 0.51259773, 0.29124936, 0.12709637,\n",
       "        0.485071  ],\n",
       "       [0.20492341, 0.1311615 , 0.48510399, 0.2922712 , 0.12651557,\n",
       "        0.48614039],\n",
       "       [0.17255984, 0.10683355, 0.45761024, 0.29329304, 0.12593477,\n",
       "        0.48720978],\n",
       "       [0.14019627, 0.08250559, 0.4301165 , 0.29431489, 0.12535397,\n",
       "        0.48827917],\n",
       "       [0.1424773 , 0.10285422, 0.44797618, 0.28115588, 0.12948101,\n",
       "        0.44754524],\n",
       "       [0.14475833, 0.12320284, 0.46583586, 0.26799687, 0.13360805,\n",
       "        0.40681132],\n",
       "       [0.14703936, 0.14355146, 0.48369555, 0.25483787, 0.13773509,\n",
       "        0.36607739],\n",
       "       [0.15047162, 0.16464487, 0.48776269, 0.26641758, 0.13516539,\n",
       "        0.24555977],\n",
       "       [0.15390388, 0.18573829, 0.49182984, 0.2779973 , 0.13259569,\n",
       "        0.12504215],\n",
       "       [0.15733614, 0.2068317 , 0.49589698, 0.28957702, 0.13002598,\n",
       "        0.00452454],\n",
       "       [0.14472048, 0.21745956, 0.47630548, 0.3036436 , 0.12116525,\n",
       "        0.00301636],\n",
       "       [0.13210482, 0.22808742, 0.45671398, 0.31771019, 0.11230451,\n",
       "        0.00150818],\n",
       "       [0.11948917, 0.23871528, 0.43712248, 0.33177677, 0.10344378,\n",
       "        0.        ],\n",
       "       [0.11305686, 0.2493644 , 0.4036701 , 0.31584481, 0.09560508,\n",
       "        0.00669863],\n",
       "       [0.10662455, 0.26001352, 0.37021773, 0.29991284, 0.08776639,\n",
       "        0.01339726],\n",
       "       [0.10019224, 0.27066264, 0.33676536, 0.28398088, 0.0799277 ,\n",
       "        0.02009588],\n",
       "       [0.11716762, 0.27534652, 0.31046496, 0.28570662, 0.07496154,\n",
       "        0.02583005],\n",
       "       [0.13414299, 0.28003041, 0.28416457, 0.28743236, 0.06999538,\n",
       "        0.03156421],\n",
       "       [0.15111837, 0.2847143 , 0.25786417, 0.2891581 , 0.06502922,\n",
       "        0.03729837],\n",
       "       [0.11509853, 0.33179719, 0.29836928, 0.42623956, 0.06405273,\n",
       "        0.04557007],\n",
       "       [0.07907869, 0.37888008, 0.33887439, 0.56332102, 0.06307623,\n",
       "        0.05384177],\n",
       "       [0.04305886, 0.42596297, 0.3793795 , 0.70040248, 0.06209974,\n",
       "        0.06211347],\n",
       "       [0.12850161, 0.42978373, 0.40598475, 0.70070225, 0.05904764,\n",
       "        0.06930811],\n",
       "       [0.21394436, 0.43360448, 0.43259   , 0.70100203, 0.05599554,\n",
       "        0.07650274],\n",
       "       [0.2993871 , 0.43742523, 0.45919525, 0.70130181, 0.05294344,\n",
       "        0.08369737],\n",
       "       [0.35577117, 0.47608186, 0.46826847, 0.69781819, 0.052095  ,\n",
       "        0.08259275],\n",
       "       [0.41215523, 0.5147385 , 0.47734168, 0.69433458, 0.05124657,\n",
       "        0.08148813],\n",
       "       [0.46853929, 0.55339513, 0.4864149 , 0.69085096, 0.05039813,\n",
       "        0.08038351],\n",
       "       [0.62096219, 0.58363857, 0.59803869, 0.65473261, 0.04965534,\n",
       "        0.08262284],\n",
       "       [0.7733851 , 0.613882  , 0.70966248, 0.61861427, 0.04891254,\n",
       "        0.08486218],\n",
       "       [0.925808  , 0.64412543, 0.82128627, 0.58249592, 0.04816974,\n",
       "        0.08710152],\n",
       "       [0.85725335, 0.66694505, 0.87774568, 0.58278676, 0.03638912,\n",
       "        0.07960687],\n",
       "       [0.78869869, 0.68976468, 0.93420509, 0.58307761, 0.0246085 ,\n",
       "        0.07211223],\n",
       "       [0.72014404, 0.7125843 , 0.9906645 , 0.58336845, 0.01282788,\n",
       "        0.06461758],\n",
       "       [0.81342936, 0.77357499, 0.99377633, 0.5692078 , 0.34188526,\n",
       "        0.12583268],\n",
       "       [0.90671468, 0.83456568, 0.99688817, 0.55504714, 0.67094263,\n",
       "        0.18704778],\n",
       "       [1.        , 0.89555637, 1.        , 0.54088649, 1.        ,\n",
       "        0.24826288],\n",
       "       [0.95619092, 0.93037091, 0.92133861, 0.52386224, 0.67256257,\n",
       "        0.24647661],\n",
       "       [0.91238183, 0.96518546, 0.84267722, 0.50683799, 0.34512515,\n",
       "        0.24469035],\n",
       "       [0.86857275, 1.        , 0.76401584, 0.48981373, 0.01768772,\n",
       "        0.24290409],\n",
       "       [0.90491222, 0.97987772, 0.77356422, 0.52393003, 0.02004869,\n",
       "        0.25724904],\n",
       "       [0.9412517 , 0.95975545, 0.78311261, 0.55804632, 0.02240965,\n",
       "        0.27159399],\n",
       "       [0.97759118, 0.93963317, 0.792661  , 0.59216262, 0.02477061,\n",
       "        0.28593894],\n",
       "       [0.88129339, 0.91384617, 0.73560682, 0.56296047, 0.02358691,\n",
       "        0.26377616],\n",
       "       [0.78499559, 0.88805917, 0.67855264, 0.53375832, 0.02240321,\n",
       "        0.24161338],\n",
       "       [0.6886978 , 0.86227217, 0.62149846, 0.50455617, 0.02121951,\n",
       "        0.21945059],\n",
       "       [0.68449732, 0.83454953, 0.56489702, 0.50634891, 0.0225267 ,\n",
       "        0.2246311 ],\n",
       "       [0.68029684, 0.80682688, 0.50829558, 0.50814165, 0.0238339 ,\n",
       "        0.22981161],\n",
       "       [0.67609636, 0.77910423, 0.45169414, 0.50993438, 0.02514109,\n",
       "        0.23499212],\n",
       "       [0.65528794, 0.79479123, 0.42624755, 0.49497649, 0.02847034,\n",
       "        0.23061177],\n",
       "       [0.63447952, 0.81047822, 0.40080097, 0.48001859, 0.03179959,\n",
       "        0.22623143],\n",
       "       [0.61367111, 0.82616521, 0.37535438, 0.46506069, 0.03512884,\n",
       "        0.22185109],\n",
       "       [0.6217773 , 0.8227804 , 0.35914083, 0.43333214, 0.03171177,\n",
       "        0.25978379],\n",
       "       [0.6298835 , 0.81939559, 0.34292729, 0.40160358, 0.0282947 ,\n",
       "        0.29771649],\n",
       "       [0.63798969, 0.81601078, 0.32671374, 0.36987503, 0.02487763,\n",
       "        0.33564919],\n",
       "       [0.59896858, 0.74490984, 0.32436665, 0.37715165, 0.02422461,\n",
       "        0.32305364],\n",
       "       [0.55994747, 0.6738089 , 0.32201957, 0.38442827, 0.02357158,\n",
       "        0.3104581 ],\n",
       "       [0.52092636, 0.60270796, 0.31967248, 0.39170489, 0.02291856,\n",
       "        0.29786256],\n",
       "       [0.48180425, 0.60358444, 0.30112959, 0.38800949, 0.01935771,\n",
       "        0.29786623],\n",
       "       [0.44268214, 0.60446092, 0.2825867 , 0.38431409, 0.01579686,\n",
       "        0.2978699 ],\n",
       "       [0.40356003, 0.6053374 , 0.26404381, 0.3806187 , 0.01223601,\n",
       "        0.29787357],\n",
       "       [0.34288554, 0.53068168, 0.19263329, 0.38398401, 0.01504364,\n",
       "        0.31001136],\n",
       "       [0.28221105, 0.45602596, 0.12122277, 0.38734933, 0.01785126,\n",
       "        0.32214915],\n",
       "       [0.22153656, 0.38137024, 0.04981225, 0.39071464, 0.02065888,\n",
       "        0.33428694],\n",
       "       [0.18722612, 0.36031608, 0.14423836, 0.40738137, 0.01743264,\n",
       "        0.35954238],\n",
       "       [0.15291569, 0.33926193, 0.23866447, 0.42404809, 0.01420641,\n",
       "        0.38479782],\n",
       "       [0.11860525, 0.31820777, 0.33309058, 0.44071482, 0.01098017,\n",
       "        0.41005326],\n",
       "       [0.07907017, 0.21213851, 0.27344218, 0.45845126, 0.00732012,\n",
       "        0.40651941],\n",
       "       [0.03953508, 0.10606926, 0.21379378, 0.4761877 , 0.00366006,\n",
       "        0.40298556],\n",
       "       [0.        , 0.        , 0.15414538, 0.49392414, 0.        ,\n",
       "        0.39945171]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 6) (11, 6) (11, 6) (87,) (11,) (11,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(6, activation='softplus', input_shape=(6,)),\n",
    "    Dense(32, activation='softplus'),\n",
    "    Dense(12, activation='softplus'),\n",
    "    Dense(1, activation='softplus'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87 samples, validate on 11 samples\n",
      "Epoch 1/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 717.9362 - val_loss: 737.0974\n",
      "Epoch 2/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 717.8226 - val_loss: 737.0064\n",
      "Epoch 3/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 717.7321 - val_loss: 736.9250\n",
      "Epoch 4/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.6511 - val_loss: 736.8487\n",
      "Epoch 5/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.5754 - val_loss: 736.7756\n",
      "Epoch 6/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 717.5026 - val_loss: 736.7045\n",
      "Epoch 7/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 717.4319 - val_loss: 736.6345\n",
      "Epoch 8/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.3624 - val_loss: 736.5654\n",
      "Epoch 9/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.2937 - val_loss: 736.4965\n",
      "Epoch 10/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 717.2253 - val_loss: 736.4279\n",
      "Epoch 11/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.1571 - val_loss: 736.3592\n",
      "Epoch 12/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.0889 - val_loss: 736.2904\n",
      "Epoch 13/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.0205 - val_loss: 736.2213\n",
      "Epoch 14/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 716.9518 - val_loss: 736.1517\n",
      "Epoch 15/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 716.8828 - val_loss: 736.0817\n",
      "Epoch 16/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 716.8133 - val_loss: 736.0112\n",
      "Epoch 17/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 716.7433 - val_loss: 735.9401\n",
      "Epoch 18/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 716.6726 - val_loss: 735.8684\n",
      "Epoch 19/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 716.6015 - val_loss: 735.7961\n",
      "Epoch 20/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 716.5297 - val_loss: 735.7230\n",
      "Epoch 21/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 716.4571 - val_loss: 735.6493\n",
      "Epoch 22/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 716.3840 - val_loss: 735.5748\n",
      "Epoch 23/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 716.3101 - val_loss: 735.4996\n",
      "Epoch 24/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 716.2355 - val_loss: 735.4237\n",
      "Epoch 25/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 716.1602 - val_loss: 735.3470\n",
      "Epoch 26/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 716.0841 - val_loss: 735.2696\n",
      "Epoch 27/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 716.0073 - val_loss: 735.1913\n",
      "Epoch 28/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 715.9297 - val_loss: 735.1123\n",
      "Epoch 29/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 715.8513 - val_loss: 735.0325\n",
      "Epoch 30/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 715.7722 - val_loss: 734.9519\n",
      "Epoch 31/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 715.6923 - val_loss: 734.8705\n",
      "Epoch 32/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 715.6115 - val_loss: 734.7883\n",
      "Epoch 33/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 715.5300 - val_loss: 734.7051\n",
      "Epoch 34/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 715.4476 - val_loss: 734.6211\n",
      "Epoch 35/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 715.3643 - val_loss: 734.5362\n",
      "Epoch 36/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 715.2803 - val_loss: 734.4504\n",
      "Epoch 37/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 715.1953 - val_loss: 734.3636\n",
      "Epoch 38/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 715.1092 - val_loss: 734.2759\n",
      "Epoch 39/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 715.0223 - val_loss: 734.1873\n",
      "Epoch 40/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 714.9345 - val_loss: 734.0976\n",
      "Epoch 41/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 714.8456 - val_loss: 734.0068\n",
      "Epoch 42/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 714.7559 - val_loss: 733.9152\n",
      "Epoch 43/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 714.6650 - val_loss: 733.8224\n",
      "Epoch 44/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 714.5733 - val_loss: 733.7288\n",
      "Epoch 45/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 714.4805 - val_loss: 733.6340\n",
      "Epoch 46/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 714.3868 - val_loss: 733.5383\n",
      "Epoch 47/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 714.2920 - val_loss: 733.4415\n",
      "Epoch 48/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 714.1961 - val_loss: 733.3437\n",
      "Epoch 49/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 714.0994 - val_loss: 733.2448\n",
      "Epoch 50/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 714.0015 - val_loss: 733.1448\n",
      "Epoch 51/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 713.9026 - val_loss: 733.0439\n",
      "Epoch 52/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 713.8027 - val_loss: 732.9418\n",
      "Epoch 53/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 713.7017 - val_loss: 732.8386\n",
      "Epoch 54/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 713.5997 - val_loss: 732.7344\n",
      "Epoch 55/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 713.4966 - val_loss: 732.6290\n",
      "Epoch 56/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 713.3924 - val_loss: 732.5225\n",
      "Epoch 57/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 713.2872 - val_loss: 732.4150\n",
      "Epoch 58/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 713.1808 - val_loss: 732.3062\n",
      "Epoch 59/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 713.0732 - val_loss: 732.1964\n",
      "Epoch 60/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 712.9647 - val_loss: 732.0853\n",
      "Epoch 61/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.8549 - val_loss: 731.9731\n",
      "Epoch 62/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.7439 - val_loss: 731.8596\n",
      "Epoch 63/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.6318 - val_loss: 731.7449\n",
      "Epoch 64/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.5184 - val_loss: 731.6290\n",
      "Epoch 65/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.4039 - val_loss: 731.5119\n",
      "Epoch 66/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 712.2883 - val_loss: 731.3935\n",
      "Epoch 67/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.1713 - val_loss: 731.2738\n",
      "Epoch 68/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 712.0531 - val_loss: 731.1529\n",
      "Epoch 69/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 711.9336 - val_loss: 731.0305\n",
      "Epoch 70/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 711.8129 - val_loss: 730.9070\n",
      "Epoch 71/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 711.6908 - val_loss: 730.7820\n",
      "Epoch 72/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 711.5674 - val_loss: 730.6557\n",
      "Epoch 73/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 711.4427 - val_loss: 730.5280\n",
      "Epoch 74/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 711.3167 - val_loss: 730.3989\n",
      "Epoch 75/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 711.1892 - val_loss: 730.2683\n",
      "Epoch 76/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 711.0604 - val_loss: 730.1364\n",
      "Epoch 77/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 710.9301 - val_loss: 730.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 710.7985 - val_loss: 729.8681\n",
      "Epoch 79/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 710.6653 - val_loss: 729.7316\n",
      "Epoch 80/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 710.5308 - val_loss: 729.5937\n",
      "Epoch 81/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 710.3947 - val_loss: 729.4542\n",
      "Epoch 82/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 710.2571 - val_loss: 729.3132\n",
      "Epoch 83/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 710.1180 - val_loss: 729.1707\n",
      "Epoch 84/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 709.9774 - val_loss: 729.0264\n",
      "Epoch 85/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 709.8351 - val_loss: 728.8806\n",
      "Epoch 86/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 709.6913 - val_loss: 728.7332\n",
      "Epoch 87/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 709.5460 - val_loss: 728.5840\n",
      "Epoch 88/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 709.3989 - val_loss: 728.4332\n",
      "Epoch 89/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 709.2502 - val_loss: 728.2805\n",
      "Epoch 90/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 709.0998 - val_loss: 728.1262\n",
      "Epoch 91/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 708.9476 - val_loss: 727.9702\n",
      "Epoch 92/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 708.7938 - val_loss: 727.8124\n",
      "Epoch 93/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 708.6382 - val_loss: 727.6527\n",
      "Epoch 94/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 708.4809 - val_loss: 727.4912\n",
      "Epoch 95/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 708.3218 - val_loss: 727.3279\n",
      "Epoch 96/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 708.1609 - val_loss: 727.1627\n",
      "Epoch 97/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 707.9981 - val_loss: 726.9956\n",
      "Epoch 98/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 707.8335 - val_loss: 726.8266\n",
      "Epoch 99/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 707.6671 - val_loss: 726.6557\n",
      "Epoch 100/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 707.4987 - val_loss: 726.4828\n",
      "Epoch 101/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 707.3284 - val_loss: 726.3079\n",
      "Epoch 102/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 707.1562 - val_loss: 726.1310\n",
      "Epoch 103/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 706.9820 - val_loss: 725.9521\n",
      "Epoch 104/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 706.8058 - val_loss: 725.7711\n",
      "Epoch 105/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 706.6276 - val_loss: 725.5881\n",
      "Epoch 106/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 706.4473 - val_loss: 725.4029\n",
      "Epoch 107/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 706.2651 - val_loss: 725.2156\n",
      "Epoch 108/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 706.0807 - val_loss: 725.0261\n",
      "Epoch 109/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 705.8941 - val_loss: 724.8344\n",
      "Epoch 110/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 705.7056 - val_loss: 724.6404\n",
      "Epoch 111/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 705.5148 - val_loss: 724.4443\n",
      "Epoch 112/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 705.3217 - val_loss: 724.2460\n",
      "Epoch 113/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 705.1265 - val_loss: 724.0453\n",
      "Epoch 114/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 704.9291 - val_loss: 723.8422\n",
      "Epoch 115/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 704.7294 - val_loss: 723.6368\n",
      "Epoch 116/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 704.5272 - val_loss: 723.4290\n",
      "Epoch 117/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 704.3228 - val_loss: 723.2188\n",
      "Epoch 118/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 704.1161 - val_loss: 723.0062\n",
      "Epoch 119/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 703.9070 - val_loss: 722.7911\n",
      "Epoch 120/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 703.6955 - val_loss: 722.5736\n",
      "Epoch 121/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 703.4815 - val_loss: 722.3535\n",
      "Epoch 122/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 703.2652 - val_loss: 722.1309\n",
      "Epoch 123/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 703.0463 - val_loss: 721.9058\n",
      "Epoch 124/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 702.8249 - val_loss: 721.6781\n",
      "Epoch 125/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 702.6010 - val_loss: 721.4477\n",
      "Epoch 126/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 702.3745 - val_loss: 721.2147\n",
      "Epoch 127/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 702.1454 - val_loss: 720.9791\n",
      "Epoch 128/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 701.9139 - val_loss: 720.7408\n",
      "Epoch 129/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 701.6796 - val_loss: 720.4998\n",
      "Epoch 130/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 701.4427 - val_loss: 720.2559\n",
      "Epoch 131/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 701.2031 - val_loss: 720.0094\n",
      "Epoch 132/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 700.9609 - val_loss: 719.7600\n",
      "Epoch 133/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 700.7159 - val_loss: 719.5079\n",
      "Epoch 134/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 700.4681 - val_loss: 719.2529\n",
      "Epoch 135/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 700.2175 - val_loss: 718.9949\n",
      "Epoch 136/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 699.9641 - val_loss: 718.7341\n",
      "Epoch 137/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 699.7078 - val_loss: 718.4703\n",
      "Epoch 138/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 699.4486 - val_loss: 718.2036\n",
      "Epoch 139/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 699.1866 - val_loss: 717.9338\n",
      "Epoch 140/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 698.9216 - val_loss: 717.6610\n",
      "Epoch 141/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 698.6536 - val_loss: 717.3852\n",
      "Epoch 142/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 698.3828 - val_loss: 717.1062\n",
      "Epoch 143/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 698.1088 - val_loss: 716.8242\n",
      "Epoch 144/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 697.8318 - val_loss: 716.5391\n",
      "Epoch 145/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 697.5517 - val_loss: 716.2509\n",
      "Epoch 146/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 697.2686 - val_loss: 715.9594\n",
      "Epoch 147/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 696.9824 - val_loss: 715.6648\n",
      "Epoch 148/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 696.6931 - val_loss: 715.3669\n",
      "Epoch 149/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 696.4006 - val_loss: 715.0659\n",
      "Epoch 150/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 696.1050 - val_loss: 714.7616\n",
      "Epoch 151/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 695.8062 - val_loss: 714.4540\n",
      "Epoch 152/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 695.5042 - val_loss: 714.1431\n",
      "Epoch 153/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 695.1990 - val_loss: 713.8289\n",
      "Epoch 154/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 694.8904 - val_loss: 713.5114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 694.5787 - val_loss: 713.1904\n",
      "Epoch 156/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 694.2637 - val_loss: 712.8661\n",
      "Epoch 157/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 693.9451 - val_loss: 712.5383\n",
      "Epoch 158/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 693.6234 - val_loss: 712.2072\n",
      "Epoch 159/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 693.2983 - val_loss: 711.8725\n",
      "Epoch 160/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 692.9698 - val_loss: 711.5343\n",
      "Epoch 161/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 692.6379 - val_loss: 711.1927\n",
      "Epoch 162/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 692.3024 - val_loss: 710.8475\n",
      "Epoch 163/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 691.9636 - val_loss: 710.4986\n",
      "Epoch 164/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 691.6212 - val_loss: 710.1462\n",
      "Epoch 165/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 691.2753 - val_loss: 709.7902\n",
      "Epoch 166/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 690.9258 - val_loss: 709.4306\n",
      "Epoch 167/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 690.5729 - val_loss: 709.0672\n",
      "Epoch 168/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 690.2163 - val_loss: 708.7001\n",
      "Epoch 169/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 689.8560 - val_loss: 708.3293\n",
      "Epoch 170/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 689.4921 - val_loss: 707.9548\n",
      "Epoch 171/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 689.1246 - val_loss: 707.5764\n",
      "Epoch 172/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 688.7532 - val_loss: 707.1943\n",
      "Epoch 173/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 688.3782 - val_loss: 706.8082\n",
      "Epoch 174/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 687.9994 - val_loss: 706.4183\n",
      "Epoch 175/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 687.6168 - val_loss: 706.0245\n",
      "Epoch 176/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 687.2303 - val_loss: 705.6268\n",
      "Epoch 177/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 686.8401 - val_loss: 705.2251\n",
      "Epoch 178/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 686.4459 - val_loss: 704.8194\n",
      "Epoch 179/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 686.0479 - val_loss: 704.4097\n",
      "Epoch 180/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 685.6459 - val_loss: 703.9960\n",
      "Epoch 181/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 685.2399 - val_loss: 703.5782\n",
      "Epoch 182/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 684.8300 - val_loss: 703.1562\n",
      "Epoch 183/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 684.4160 - val_loss: 702.7302\n",
      "Epoch 184/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 683.9979 - val_loss: 702.2999\n",
      "Epoch 185/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 683.5759 - val_loss: 701.8655\n",
      "Epoch 186/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 683.1497 - val_loss: 701.4268\n",
      "Epoch 187/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 682.7194 - val_loss: 700.9839\n",
      "Epoch 188/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 682.2848 - val_loss: 700.5367\n",
      "Epoch 189/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 681.8461 - val_loss: 700.0853\n",
      "Epoch 190/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 681.4031 - val_loss: 699.6294\n",
      "Epoch 191/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 680.9559 - val_loss: 699.1691\n",
      "Epoch 192/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 680.5043 - val_loss: 698.7045\n",
      "Epoch 193/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 680.0485 - val_loss: 698.2353\n",
      "Epoch 194/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 679.5884 - val_loss: 697.7618\n",
      "Epoch 195/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 679.1237 - val_loss: 697.2836\n",
      "Epoch 196/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 678.6547 - val_loss: 696.8010\n",
      "Epoch 197/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 678.1813 - val_loss: 696.3138\n",
      "Epoch 198/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 677.7033 - val_loss: 695.8220\n",
      "Epoch 199/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 677.2209 - val_loss: 695.3255\n",
      "Epoch 200/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 676.7339 - val_loss: 694.8244\n",
      "Epoch 201/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 676.2424 - val_loss: 694.3186\n",
      "Epoch 202/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 675.7462 - val_loss: 693.8080\n",
      "Epoch 203/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 675.2454 - val_loss: 693.2928\n",
      "Epoch 204/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 674.7399 - val_loss: 692.7726\n",
      "Epoch 205/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 674.2297 - val_loss: 692.2477\n",
      "Epoch 206/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 673.7148 - val_loss: 691.7178\n",
      "Epoch 207/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 673.1952 - val_loss: 691.1832\n",
      "Epoch 208/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 672.6707 - val_loss: 690.6435\n",
      "Epoch 209/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 672.1412 - val_loss: 690.0989\n",
      "Epoch 210/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 671.6071 - val_loss: 689.5493\n",
      "Epoch 211/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 671.0679 - val_loss: 688.9945\n",
      "Epoch 212/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 670.5239 - val_loss: 688.4348\n",
      "Epoch 213/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 669.9748 - val_loss: 687.8699\n",
      "Epoch 214/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 669.4208 - val_loss: 687.2999\n",
      "Epoch 215/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 668.8617 - val_loss: 686.7247\n",
      "Epoch 216/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 668.2975 - val_loss: 686.1443\n",
      "Epoch 217/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 667.7281 - val_loss: 685.5586\n",
      "Epoch 218/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 667.1537 - val_loss: 684.9676\n",
      "Epoch 219/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 666.5740 - val_loss: 684.3713\n",
      "Epoch 220/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 665.9891 - val_loss: 683.7697\n",
      "Epoch 221/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 665.3990 - val_loss: 683.1627\n",
      "Epoch 222/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 664.8036 - val_loss: 682.5502\n",
      "Epoch 223/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 664.2029 - val_loss: 681.9323\n",
      "Epoch 224/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 663.5967 - val_loss: 681.3088\n",
      "Epoch 225/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 662.9853 - val_loss: 680.6799\n",
      "Epoch 226/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 662.3683 - val_loss: 680.0453\n",
      "Epoch 227/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 661.7459 - val_loss: 679.4052\n",
      "Epoch 228/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 661.1180 - val_loss: 678.7594\n",
      "Epoch 229/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 660.4846 - val_loss: 678.1080\n",
      "Epoch 230/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 659.8456 - val_loss: 677.4507\n",
      "Epoch 231/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 659.2010 - val_loss: 676.7878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 658.5507 - val_loss: 676.1192\n",
      "Epoch 233/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 657.8948 - val_loss: 675.4447\n",
      "Epoch 234/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 657.2332 - val_loss: 674.7643\n",
      "Epoch 235/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 656.5658 - val_loss: 674.0781\n",
      "Epoch 236/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 655.8927 - val_loss: 673.3858\n",
      "Epoch 237/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 655.2137 - val_loss: 672.6877\n",
      "Epoch 238/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 654.5289 - val_loss: 671.9836\n",
      "Epoch 239/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 653.8381 - val_loss: 671.2734\n",
      "Epoch 240/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 653.1415 - val_loss: 670.5571\n",
      "Epoch 241/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 652.4388 - val_loss: 669.8347\n",
      "Epoch 242/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 651.7302 - val_loss: 669.1062\n",
      "Epoch 243/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 651.0156 - val_loss: 668.3715\n",
      "Epoch 244/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 650.2949 - val_loss: 667.6306\n",
      "Epoch 245/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 649.5680 - val_loss: 666.8834\n",
      "Epoch 246/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 648.8350 - val_loss: 666.1299\n",
      "Epoch 247/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 648.0959 - val_loss: 665.3701\n",
      "Epoch 248/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 647.3505 - val_loss: 664.6038\n",
      "Epoch 249/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 646.5988 - val_loss: 663.8311\n",
      "Epoch 250/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 645.8409 - val_loss: 663.0521\n",
      "Epoch 251/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 645.0766 - val_loss: 662.2666\n",
      "Epoch 252/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 644.3058 - val_loss: 661.4745\n",
      "Epoch 253/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 643.5288 - val_loss: 660.6758\n",
      "Epoch 254/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 642.7453 - val_loss: 659.8705\n",
      "Epoch 255/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 641.9552 - val_loss: 659.0586\n",
      "Epoch 256/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 641.1587 - val_loss: 658.2400\n",
      "Epoch 257/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 640.3556 - val_loss: 657.4147\n",
      "Epoch 258/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 639.5458 - val_loss: 656.5826\n",
      "Epoch 259/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 638.7296 - val_loss: 655.7437\n",
      "Epoch 260/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 637.9065 - val_loss: 654.8980\n",
      "Epoch 261/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 637.0767 - val_loss: 654.0454\n",
      "Epoch 262/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 636.2401 - val_loss: 653.1859\n",
      "Epoch 263/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 635.3969 - val_loss: 652.3194\n",
      "Epoch 264/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 634.5466 - val_loss: 651.4459\n",
      "Epoch 265/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 633.6896 - val_loss: 650.5654\n",
      "Epoch 266/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 632.8256 - val_loss: 649.6777\n",
      "Epoch 267/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 631.9547 - val_loss: 648.7830\n",
      "Epoch 268/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 631.0768 - val_loss: 647.8812\n",
      "Epoch 269/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 630.1918 - val_loss: 646.9721\n",
      "Epoch 270/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 629.2998 - val_loss: 646.0557\n",
      "Epoch 271/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 628.4006 - val_loss: 645.1322\n",
      "Epoch 272/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 627.4943 - val_loss: 644.2012\n",
      "Epoch 273/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 626.5808 - val_loss: 643.2629\n",
      "Epoch 274/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 625.6601 - val_loss: 642.3173\n",
      "Epoch 275/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 624.7321 - val_loss: 641.3641\n",
      "Epoch 276/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 623.7968 - val_loss: 640.4035\n",
      "Epoch 277/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 622.8540 - val_loss: 639.4354\n",
      "Epoch 278/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 621.9039 - val_loss: 638.4597\n",
      "Epoch 279/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 620.9464 - val_loss: 637.4764\n",
      "Epoch 280/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 619.9814 - val_loss: 636.4854\n",
      "Epoch 281/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 619.0089 - val_loss: 635.4868\n",
      "Epoch 282/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 618.0287 - val_loss: 634.4803\n",
      "Epoch 283/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 617.0410 - val_loss: 633.4661\n",
      "Epoch 284/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 616.0457 - val_loss: 632.4442\n",
      "Epoch 285/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 615.0426 - val_loss: 631.4143\n",
      "Epoch 286/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 614.0318 - val_loss: 630.3765\n",
      "Epoch 287/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 613.0133 - val_loss: 629.3308\n",
      "Epoch 288/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 611.9868 - val_loss: 628.2771\n",
      "Epoch 289/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 610.9527 - val_loss: 627.2154\n",
      "Epoch 290/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 609.9106 - val_loss: 626.1456\n",
      "Epoch 291/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 608.8605 - val_loss: 625.0676\n",
      "Epoch 292/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 607.8024 - val_loss: 623.9816\n",
      "Epoch 293/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 606.7363 - val_loss: 622.8873\n",
      "Epoch 294/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 605.6622 - val_loss: 621.7848\n",
      "Epoch 295/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 604.5799 - val_loss: 620.6740\n",
      "Epoch 296/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 603.4895 - val_loss: 619.5549\n",
      "Epoch 297/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 602.3909 - val_loss: 618.4274\n",
      "Epoch 298/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 601.2841 - val_loss: 617.2914\n",
      "Epoch 299/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 600.1689 - val_loss: 616.1470\n",
      "Epoch 300/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 599.0454 - val_loss: 614.9942\n",
      "Epoch 301/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 597.9136 - val_loss: 613.8328\n",
      "Epoch 302/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 596.7734 - val_loss: 612.6627\n",
      "Epoch 303/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 595.6247 - val_loss: 611.4840\n",
      "Epoch 304/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 594.4675 - val_loss: 610.2966\n",
      "Epoch 305/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 593.3018 - val_loss: 609.1006\n",
      "Epoch 306/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 592.1275 - val_loss: 607.8958\n",
      "Epoch 307/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 590.9445 - val_loss: 606.6821\n",
      "Epoch 308/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 589.7529 - val_loss: 605.4596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 588.5526 - val_loss: 604.2281\n",
      "Epoch 310/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 587.3435 - val_loss: 602.9878\n",
      "Epoch 311/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 586.1256 - val_loss: 601.7383\n",
      "Epoch 312/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 584.8987 - val_loss: 600.4800\n",
      "Epoch 313/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 583.6631 - val_loss: 599.2125\n",
      "Epoch 314/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 582.4186 - val_loss: 597.9359\n",
      "Epoch 315/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 581.1649 - val_loss: 596.6501\n",
      "Epoch 316/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 579.9023 - val_loss: 595.3551\n",
      "Epoch 317/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 578.6307 - val_loss: 594.0508\n",
      "Epoch 318/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 577.3498 - val_loss: 592.7373\n",
      "Epoch 319/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 576.0598 - val_loss: 591.4144\n",
      "Epoch 320/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 574.7606 - val_loss: 590.0820\n",
      "Epoch 321/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 573.4521 - val_loss: 588.7403\n",
      "Epoch 322/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 572.1344 - val_loss: 587.3890\n",
      "Epoch 323/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 570.8073 - val_loss: 586.0283\n",
      "Epoch 324/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 569.4708 - val_loss: 584.6578\n",
      "Epoch 325/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 568.1249 - val_loss: 583.2778\n",
      "Epoch 326/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 566.7695 - val_loss: 581.8881\n",
      "Epoch 327/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 565.4045 - val_loss: 580.4887\n",
      "Epoch 328/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 564.0300 - val_loss: 579.0795\n",
      "Epoch 329/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 562.6458 - val_loss: 577.6605\n",
      "Epoch 330/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 561.2520 - val_loss: 576.2316\n",
      "Epoch 331/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 559.8484 - val_loss: 574.7928\n",
      "Epoch 332/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 558.4352 - val_loss: 573.3441\n",
      "Epoch 333/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 557.0121 - val_loss: 571.8853\n",
      "Epoch 334/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 555.5792 - val_loss: 570.4165\n",
      "Epoch 335/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 554.1362 - val_loss: 568.9376\n",
      "Epoch 336/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 552.6835 - val_loss: 567.4485\n",
      "Epoch 337/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 551.2206 - val_loss: 565.9493\n",
      "Epoch 338/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 549.7477 - val_loss: 564.4398\n",
      "Epoch 339/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 548.2648 - val_loss: 562.9201\n",
      "Epoch 340/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 546.7717 - val_loss: 561.3900\n",
      "Epoch 341/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 545.2685 - val_loss: 559.8494\n",
      "Epoch 342/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 543.7550 - val_loss: 558.2986\n",
      "Epoch 343/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 542.2313 - val_loss: 556.7371\n",
      "Epoch 344/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 540.6971 - val_loss: 555.1652\n",
      "Epoch 345/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 539.1526 - val_loss: 553.5826\n",
      "Epoch 346/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 537.5977 - val_loss: 551.9895\n",
      "Epoch 347/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 536.0323 - val_loss: 550.3857\n",
      "Epoch 348/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 534.4564 - val_loss: 548.7711\n",
      "Epoch 349/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 532.8699 - val_loss: 547.1458\n",
      "Epoch 350/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 531.2728 - val_loss: 545.5097\n",
      "Epoch 351/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 529.6651 - val_loss: 543.8627\n",
      "Epoch 352/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 528.0467 - val_loss: 542.2047\n",
      "Epoch 353/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 526.4174 - val_loss: 540.5358\n",
      "Epoch 354/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 524.7773 - val_loss: 538.8559\n",
      "Epoch 355/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 523.1265 - val_loss: 537.1649\n",
      "Epoch 356/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 521.4647 - val_loss: 535.4628\n",
      "Epoch 357/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 519.7919 - val_loss: 533.7496\n",
      "Epoch 358/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 518.1081 - val_loss: 532.0251\n",
      "Epoch 359/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 516.4133 - val_loss: 530.2893\n",
      "Epoch 360/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 514.7075 - val_loss: 528.5423\n",
      "Epoch 361/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 512.9902 - val_loss: 526.7839\n",
      "Epoch 362/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 511.2620 - val_loss: 525.0140\n",
      "Epoch 363/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 509.5224 - val_loss: 523.2327\n",
      "Epoch 364/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 507.7715 - val_loss: 521.4400\n",
      "Epoch 365/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 506.0093 - val_loss: 519.6355\n",
      "Epoch 366/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 504.2358 - val_loss: 517.8195\n",
      "Epoch 367/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 502.4507 - val_loss: 515.9919\n",
      "Epoch 368/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 500.6542 - val_loss: 514.1525\n",
      "Epoch 369/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 498.8460 - val_loss: 512.3013\n",
      "Epoch 370/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 497.0263 - val_loss: 510.4384\n",
      "Epoch 371/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 495.1949 - val_loss: 508.5635\n",
      "Epoch 372/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 493.3518 - val_loss: 506.6768\n",
      "Epoch 373/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 491.4970 - val_loss: 504.7780\n",
      "Epoch 374/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 489.6302 - val_loss: 502.8673\n",
      "Epoch 375/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 487.7518 - val_loss: 500.9445\n",
      "Epoch 376/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 485.8614 - val_loss: 499.0095\n",
      "Epoch 377/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 483.9590 - val_loss: 497.0624\n",
      "Epoch 378/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 482.0446 - val_loss: 495.1030\n",
      "Epoch 379/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 480.1182 - val_loss: 493.1314\n",
      "Epoch 380/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 478.1796 - val_loss: 491.1474\n",
      "Epoch 381/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 476.2289 - val_loss: 489.1510\n",
      "Epoch 382/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 474.2659 - val_loss: 487.1422\n",
      "Epoch 383/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 472.2907 - val_loss: 485.1210\n",
      "Epoch 384/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 470.3032 - val_loss: 483.0871\n",
      "Epoch 385/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 468.3033 - val_loss: 481.0407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 466.2910 - val_loss: 478.9816\n",
      "Epoch 387/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 464.2661 - val_loss: 476.9099\n",
      "Epoch 388/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 462.2288 - val_loss: 474.8254\n",
      "Epoch 389/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 460.1789 - val_loss: 472.7280\n",
      "Epoch 390/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 458.1163 - val_loss: 470.6179\n",
      "Epoch 391/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 456.0411 - val_loss: 468.4948\n",
      "Epoch 392/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 453.9530 - val_loss: 466.3587\n",
      "Epoch 393/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 451.8523 - val_loss: 464.2097\n",
      "Epoch 394/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 449.7387 - val_loss: 462.0475\n",
      "Epoch 395/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 447.6122 - val_loss: 459.8723\n",
      "Epoch 396/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 445.4727 - val_loss: 457.6838\n",
      "Epoch 397/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 443.3203 - val_loss: 455.4821\n",
      "Epoch 398/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 441.1548 - val_loss: 453.2672\n",
      "Epoch 399/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 438.9761 - val_loss: 451.0389\n",
      "Epoch 400/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 436.7844 - val_loss: 448.7972\n",
      "Epoch 401/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 434.5793 - val_loss: 446.5421\n",
      "Epoch 402/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 432.3610 - val_loss: 444.2734\n",
      "Epoch 403/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 430.1294 - val_loss: 441.9912\n",
      "Epoch 404/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 427.8844 - val_loss: 439.6955\n",
      "Epoch 405/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 425.6260 - val_loss: 437.3860\n",
      "Epoch 406/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 423.3541 - val_loss: 435.0628\n",
      "Epoch 407/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 421.0687 - val_loss: 432.7259\n",
      "Epoch 408/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 418.7697 - val_loss: 430.3751\n",
      "Epoch 409/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 416.4570 - val_loss: 428.0104\n",
      "Epoch 410/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 414.1306 - val_loss: 425.6318\n",
      "Epoch 411/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 411.7905 - val_loss: 423.2393\n",
      "Epoch 412/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 409.4365 - val_loss: 420.8326\n",
      "Epoch 413/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 407.0687 - val_loss: 418.4119\n",
      "Epoch 414/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 404.6870 - val_loss: 415.9770\n",
      "Epoch 415/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 402.2912 - val_loss: 413.5279\n",
      "Epoch 416/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 399.8814 - val_loss: 411.0645\n",
      "Epoch 417/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 397.4576 - val_loss: 408.5868\n",
      "Epoch 418/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 395.0196 - val_loss: 406.0948\n",
      "Epoch 419/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 392.5674 - val_loss: 403.5883\n",
      "Epoch 420/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 390.1010 - val_loss: 401.0673\n",
      "Epoch 421/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 387.6202 - val_loss: 398.5317\n",
      "Epoch 422/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 385.1251 - val_loss: 395.9815\n",
      "Epoch 423/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 382.6156 - val_loss: 393.4167\n",
      "Epoch 424/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 380.0916 - val_loss: 390.8372\n",
      "Epoch 425/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 377.5530 - val_loss: 388.2429\n",
      "Epoch 426/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 374.9999 - val_loss: 385.6338\n",
      "Epoch 427/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 372.4321 - val_loss: 383.0098\n",
      "Epoch 428/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 369.8496 - val_loss: 380.3708\n",
      "Epoch 429/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 367.2524 - val_loss: 377.7168\n",
      "Epoch 430/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 364.6403 - val_loss: 375.0478\n",
      "Epoch 431/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 362.0134 - val_loss: 372.3637\n",
      "Epoch 432/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 359.3716 - val_loss: 369.6643\n",
      "Epoch 433/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 356.7147 - val_loss: 366.9498\n",
      "Epoch 434/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 354.0428 - val_loss: 364.2200\n",
      "Epoch 435/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 351.3559 - val_loss: 361.4748\n",
      "Epoch 436/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 348.6537 - val_loss: 358.7143\n",
      "Epoch 437/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 345.9364 - val_loss: 355.9383\n",
      "Epoch 438/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 343.2038 - val_loss: 353.1467\n",
      "Epoch 439/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 340.4558 - val_loss: 350.3396\n",
      "Epoch 440/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 337.6925 - val_loss: 347.5169\n",
      "Epoch 441/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 334.9138 - val_loss: 344.6784\n",
      "Epoch 442/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 332.1195 - val_loss: 341.8242\n",
      "Epoch 443/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 329.3096 - val_loss: 339.0094\n",
      "Epoch 444/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 326.4842 - val_loss: 337.0222\n",
      "Epoch 445/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 323.6430 - val_loss: 335.0244\n",
      "Epoch 446/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 320.7862 - val_loss: 333.0157\n",
      "Epoch 447/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 317.9136 - val_loss: 330.9963\n",
      "Epoch 448/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 315.0250 - val_loss: 328.9660\n",
      "Epoch 449/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 312.1206 - val_loss: 326.9249\n",
      "Epoch 450/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 309.2003 - val_loss: 324.8728\n",
      "Epoch 451/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 306.2639 - val_loss: 322.8097\n",
      "Epoch 452/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 303.3114 - val_loss: 320.7356\n",
      "Epoch 453/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 300.3428 - val_loss: 318.6505\n",
      "Epoch 454/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 297.3579 - val_loss: 316.5543\n",
      "Epoch 455/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 294.4368 - val_loss: 314.5146\n",
      "Epoch 456/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 291.6255 - val_loss: 312.4575\n",
      "Epoch 457/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 288.8617 - val_loss: 310.5150\n",
      "Epoch 458/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 286.3724 - val_loss: 308.5458\n",
      "Epoch 459/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 283.8486 - val_loss: 306.5512\n",
      "Epoch 460/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 281.2919 - val_loss: 304.5323\n",
      "Epoch 461/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 278.7468 - val_loss: 302.5574\n",
      "Epoch 462/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 276.3062 - val_loss: 300.5551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 463/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 273.8801 - val_loss: 298.5952\n",
      "Epoch 464/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 271.5462 - val_loss: 296.6055\n",
      "Epoch 465/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 269.2049 - val_loss: 294.6587\n",
      "Epoch 466/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 266.9743 - val_loss: 292.8909\n",
      "Epoch 467/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 264.7068 - val_loss: 291.6366\n",
      "Epoch 468/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 262.4041 - val_loss: 290.3645\n",
      "Epoch 469/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 260.0674 - val_loss: 289.0754\n",
      "Epoch 470/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 257.6984 - val_loss: 287.7701\n",
      "Epoch 471/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 255.2982 - val_loss: 286.4492\n",
      "Epoch 472/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 252.8681 - val_loss: 285.1134\n",
      "Epoch 473/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 250.4095 - val_loss: 283.7633\n",
      "Epoch 474/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 248.0137 - val_loss: 282.4453\n",
      "Epoch 475/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 245.6797 - val_loss: 281.1107\n",
      "Epoch 476/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 243.3152 - val_loss: 279.7601\n",
      "Epoch 477/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 240.9212 - val_loss: 278.3941\n",
      "Epoch 478/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 238.4990 - val_loss: 277.0135\n",
      "Epoch 479/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 236.0499 - val_loss: 275.6188\n",
      "Epoch 480/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 233.5803 - val_loss: 274.2594\n",
      "Epoch 481/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 231.4609 - val_loss: 273.0349\n",
      "Epoch 482/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 229.6295 - val_loss: 271.7849\n",
      "Epoch 483/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 227.7600 - val_loss: 270.5105\n",
      "Epoch 484/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 225.8536 - val_loss: 269.2127\n",
      "Epoch 485/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 223.9839 - val_loss: 267.9453\n",
      "Epoch 486/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 222.1708 - val_loss: 266.6533\n",
      "Epoch 487/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 220.3223 - val_loss: 265.3376\n",
      "Epoch 488/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 218.4397 - val_loss: 263.9993\n",
      "Epoch 489/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 216.5244 - val_loss: 262.6392\n",
      "Epoch 490/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 214.5774 - val_loss: 261.2582\n",
      "Epoch 491/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 212.6004 - val_loss: 259.8571\n",
      "Epoch 492/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 210.5942 - val_loss: 258.4371\n",
      "Epoch 493/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 208.5602 - val_loss: 256.9988\n",
      "Epoch 494/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 206.4996 - val_loss: 255.5429\n",
      "Epoch 495/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 204.4135 - val_loss: 254.0704\n",
      "Epoch 496/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 202.3028 - val_loss: 252.5818\n",
      "Epoch 497/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 200.1917 - val_loss: 251.1395\n",
      "Epoch 498/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 198.2177 - val_loss: 249.6778\n",
      "Epoch 499/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 196.2168 - val_loss: 248.1976\n",
      "Epoch 500/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 194.2254 - val_loss: 246.7647\n",
      "Epoch 501/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 192.3567 - val_loss: 245.3103\n",
      "Epoch 502/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 190.4598 - val_loss: 243.8354\n",
      "Epoch 503/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 188.5360 - val_loss: 242.3408\n",
      "Epoch 504/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 186.5863 - val_loss: 240.8274\n",
      "Epoch 505/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 184.6119 - val_loss: 239.2961\n",
      "Epoch 506/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 182.6138 - val_loss: 237.7476\n",
      "Epoch 507/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 180.5929 - val_loss: 236.1826\n",
      "Epoch 508/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 178.5502 - val_loss: 234.6018\n",
      "Epoch 509/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 176.5101 - val_loss: 233.0769\n",
      "Epoch 510/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 174.6161 - val_loss: 231.5320\n",
      "Epoch 511/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 172.6972 - val_loss: 229.9682\n",
      "Epoch 512/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 170.7545 - val_loss: 228.3860\n",
      "Epoch 513/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 168.7890 - val_loss: 226.7864\n",
      "Epoch 514/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 166.8016 - val_loss: 225.1701\n",
      "Epoch 515/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 164.7932 - val_loss: 223.5378\n",
      "Epoch 516/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 162.7647 - val_loss: 221.8901\n",
      "Epoch 517/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 160.7167 - val_loss: 220.2276\n",
      "Epoch 518/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 158.6502 - val_loss: 218.5510\n",
      "Epoch 519/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 156.5658 - val_loss: 216.9373\n",
      "Epoch 520/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 154.6570 - val_loss: 215.3044\n",
      "Epoch 521/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 152.7916 - val_loss: 213.7338\n",
      "Epoch 522/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 151.0294 - val_loss: 212.1399\n",
      "Epoch 523/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 149.2414 - val_loss: 210.5237\n",
      "Epoch 524/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 147.4286 - val_loss: 208.8862\n",
      "Epoch 525/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 145.6855 - val_loss: 207.3977\n",
      "Epoch 526/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 144.1913 - val_loss: 205.8795\n",
      "Epoch 527/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 142.6679 - val_loss: 204.3327\n",
      "Epoch 528/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 141.1164 - val_loss: 202.7585\n",
      "Epoch 529/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 139.5379 - val_loss: 201.1579\n",
      "Epoch 530/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 137.9455 - val_loss: 199.6269\n",
      "Epoch 531/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 136.5060 - val_loss: 198.1443\n",
      "Epoch 532/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 135.1760 - val_loss: 196.6297\n",
      "Epoch 533/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 133.9288 - val_loss: 195.6857\n",
      "Epoch 534/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 132.8381 - val_loss: 194.9231\n",
      "Epoch 535/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 131.9011 - val_loss: 194.2239\n",
      "Epoch 536/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 131.2663 - val_loss: 193.7348\n",
      "Epoch 537/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 130.9008 - val_loss: 193.2328\n",
      "Epoch 538/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 130.5293 - val_loss: 192.7808\n",
      "Epoch 539/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 130.2242 - val_loss: 192.3135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 540/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 129.9107 - val_loss: 191.8294\n",
      "Epoch 541/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 129.5874 - val_loss: 191.3272\n",
      "Epoch 542/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 129.2534 - val_loss: 190.8058\n",
      "Epoch 543/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 128.9200 - val_loss: 190.3374\n",
      "Epoch 544/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 128.6263 - val_loss: 189.8517\n",
      "Epoch 545/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 128.3242 - val_loss: 189.3476\n",
      "Epoch 546/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 128.0132 - val_loss: 188.8244\n",
      "Epoch 547/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 127.6927 - val_loss: 188.2813\n",
      "Epoch 548/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 127.3622 - val_loss: 187.7176\n",
      "Epoch 549/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 127.0213 - val_loss: 187.1329\n",
      "Epoch 550/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 126.6834 - val_loss: 186.6270\n",
      "Epoch 551/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 126.4359 - val_loss: 186.3096\n",
      "Epoch 552/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 126.2490 - val_loss: 185.9807\n",
      "Epoch 553/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 126.0606 - val_loss: 185.6584\n",
      "Epoch 554/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 125.8701 - val_loss: 185.4141\n",
      "Epoch 555/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 125.6767 - val_loss: 185.1668\n",
      "Epoch 556/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 125.4801 - val_loss: 184.9160\n",
      "Epoch 557/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 125.2799 - val_loss: 184.6613\n",
      "Epoch 558/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 125.0757 - val_loss: 184.4022\n",
      "Epoch 559/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 124.8672 - val_loss: 184.1385\n",
      "Epoch 560/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 124.6542 - val_loss: 183.8697\n",
      "Epoch 561/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 124.4365 - val_loss: 183.5957\n",
      "Epoch 562/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 124.2137 - val_loss: 183.3162\n",
      "Epoch 563/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 123.9859 - val_loss: 183.0310\n",
      "Epoch 564/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 123.7804 - val_loss: 182.7743\n",
      "Epoch 565/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 123.5996 - val_loss: 182.5135\n",
      "Epoch 566/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 123.4161 - val_loss: 182.2483\n",
      "Epoch 567/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 123.2297 - val_loss: 181.9784\n",
      "Epoch 568/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 123.0401 - val_loss: 181.7034\n",
      "Epoch 569/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 122.8473 - val_loss: 181.4232\n",
      "Epoch 570/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 122.6509 - val_loss: 181.1375\n",
      "Epoch 571/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 122.4606 - val_loss: 180.9428\n",
      "Epoch 572/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 122.3007 - val_loss: 180.6469\n",
      "Epoch 573/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 122.1195 - val_loss: 180.4012\n",
      "Epoch 574/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 121.9522 - val_loss: 180.1518\n",
      "Epoch 575/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 121.7890 - val_loss: 179.9631\n",
      "Epoch 576/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 121.6268 - val_loss: 179.7060\n",
      "Epoch 577/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 121.4627 - val_loss: 179.5163\n",
      "Epoch 578/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 121.2981 - val_loss: 179.2504\n",
      "Epoch 579/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 121.1354 - val_loss: 179.0595\n",
      "Epoch 580/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 120.9653 - val_loss: 178.8680\n",
      "Epoch 581/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 120.8070 - val_loss: 178.5870\n",
      "Epoch 582/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 120.6365 - val_loss: 178.3937\n",
      "Epoch 583/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 120.4674 - val_loss: 178.1016\n",
      "Epoch 584/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 120.3053 - val_loss: 177.9063\n",
      "Epoch 585/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 120.1333 - val_loss: 177.7101\n",
      "Epoch 586/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 119.9614 - val_loss: 177.5128\n",
      "Epoch 587/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 119.7995 - val_loss: 177.1954\n",
      "Epoch 588/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 119.6272 - val_loss: 176.9959\n",
      "Epoch 589/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 119.4539 - val_loss: 176.8805\n",
      "Epoch 590/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 119.2807 - val_loss: 176.9221\n",
      "Epoch 591/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 119.1155 - val_loss: 176.3943\n",
      "Epoch 592/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 118.9428 - val_loss: 176.4351\n",
      "Epoch 593/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 118.7681 - val_loss: 176.4707\n",
      "Epoch 594/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 118.5934 - val_loss: 176.5009\n",
      "Epoch 595/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 118.4187 - val_loss: 176.5255\n",
      "Epoch 596/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 118.2484 - val_loss: 175.2582\n",
      "Epoch 597/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 118.1014 - val_loss: 175.9339\n",
      "Epoch 598/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 117.9051 - val_loss: 175.9337\n",
      "Epoch 599/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 117.7315 - val_loss: 175.9275\n",
      "Epoch 600/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 117.5577 - val_loss: 175.9151\n",
      "Epoch 601/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 117.3837 - val_loss: 175.8960\n",
      "Epoch 602/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 117.2159 - val_loss: 175.0475\n",
      "Epoch 603/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 117.0540 - val_loss: 175.0257\n",
      "Epoch 604/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 116.8772 - val_loss: 174.9955\n",
      "Epoch 605/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 116.7003 - val_loss: 174.9564\n",
      "Epoch 606/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 116.5331 - val_loss: 173.9864\n",
      "Epoch 607/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 116.4086 - val_loss: 174.7935\n",
      "Epoch 608/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 116.2235 - val_loss: 173.8320\n",
      "Epoch 609/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 116.0652 - val_loss: 174.5931\n",
      "Epoch 610/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 115.9110 - val_loss: 173.6338\n",
      "Epoch 611/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 115.7264 - val_loss: 174.3586\n",
      "Epoch 612/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 115.5955 - val_loss: 173.3972\n",
      "Epoch 613/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 115.4130 - val_loss: 173.2511\n",
      "Epoch 614/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 115.2600 - val_loss: 173.0956\n",
      "Epoch 615/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 115.1061 - val_loss: 172.9297\n",
      "Epoch 616/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 114.9511 - val_loss: 172.7528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 114.7953 - val_loss: 172.5641\n",
      "Epoch 618/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 114.6386 - val_loss: 172.3627\n",
      "Epoch 619/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 114.4809 - val_loss: 172.1478\n",
      "Epoch 620/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 114.3222 - val_loss: 171.9185\n",
      "Epoch 621/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 114.1627 - val_loss: 171.6738\n",
      "Epoch 622/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 114.0022 - val_loss: 171.4128\n",
      "Epoch 623/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 113.8408 - val_loss: 171.1345\n",
      "Epoch 624/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 113.6785 - val_loss: 170.8378\n",
      "Epoch 625/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 113.5151 - val_loss: 170.5219\n",
      "Epoch 626/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 113.3507 - val_loss: 170.1857\n",
      "Epoch 627/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 113.1853 - val_loss: 169.8283\n",
      "Epoch 628/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 113.0188 - val_loss: 169.4489\n",
      "Epoch 629/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.8624 - val_loss: 170.5093\n",
      "Epoch 630/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.7159 - val_loss: 168.8338\n",
      "Epoch 631/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.5872 - val_loss: 169.6468\n",
      "Epoch 632/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 112.3994 - val_loss: 169.2738\n",
      "Epoch 633/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.2329 - val_loss: 168.8828\n",
      "Epoch 634/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 112.0822 - val_loss: 169.6916\n",
      "Epoch 635/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 112.0093 - val_loss: 167.5365\n",
      "Epoch 636/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.8705 - val_loss: 168.0804\n",
      "Epoch 637/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 111.6704 - val_loss: 168.6117\n",
      "Epoch 638/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 111.5155 - val_loss: 168.1476\n",
      "Epoch 639/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.3770 - val_loss: 167.6676\n",
      "Epoch 640/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 111.2364 - val_loss: 167.1710\n",
      "Epoch 641/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.1106 - val_loss: 167.7553\n",
      "Epoch 642/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 110.9741 - val_loss: 166.3085\n",
      "Epoch 643/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 110.8564 - val_loss: 166.8413\n",
      "Epoch 644/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 110.6889 - val_loss: 166.3216\n",
      "Epoch 645/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 110.5441 - val_loss: 165.7851\n",
      "Epoch 646/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 110.4121 - val_loss: 166.3439\n",
      "Epoch 647/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 110.2762 - val_loss: 165.7663\n",
      "Epoch 648/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 110.1274 - val_loss: 165.1720\n",
      "Epoch 649/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 109.9767 - val_loss: 164.5607\n",
      "Epoch 650/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 109.8463 - val_loss: 165.1571\n",
      "Epoch 651/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 109.7035 - val_loss: 164.5043\n",
      "Epoch 652/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 109.5490 - val_loss: 163.8356\n",
      "Epoch 653/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 109.4238 - val_loss: 165.3208\n",
      "Epoch 654/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 109.3899 - val_loss: 163.1935\n",
      "Epoch 655/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 109.1977 - val_loss: 164.2506\n",
      "Epoch 656/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 109.0377 - val_loss: 163.0217\n",
      "Epoch 657/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.9082 - val_loss: 163.2656\n",
      "Epoch 658/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.7554 - val_loss: 163.5084\n",
      "Epoch 659/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.6417 - val_loss: 162.2219\n",
      "Epoch 660/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 108.5293 - val_loss: 162.4652\n",
      "Epoch 661/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 108.3751 - val_loss: 162.7068\n",
      "Epoch 662/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.2353 - val_loss: 162.1378\n",
      "Epoch 663/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.1047 - val_loss: 162.3907\n",
      "Epoch 664/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 107.9795 - val_loss: 160.9751\n",
      "Epoch 665/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 107.8943 - val_loss: 161.9841\n",
      "Epoch 666/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 107.7370 - val_loss: 160.0586\n",
      "Epoch 667/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 107.7188 - val_loss: 161.4762\n",
      "Epoch 668/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 107.4792 - val_loss: 160.3063\n",
      "Epoch 669/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 107.3721 - val_loss: 161.0313\n",
      "Epoch 670/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 107.2450 - val_loss: 159.8568\n",
      "Epoch 671/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 107.1275 - val_loss: 160.5700\n",
      "Epoch 672/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 107.0076 - val_loss: 159.3901\n",
      "Epoch 673/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.8834 - val_loss: 160.0928\n",
      "Epoch 674/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 106.7672 - val_loss: 158.9070\n",
      "Epoch 675/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 106.6399 - val_loss: 159.6004\n",
      "Epoch 676/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.5238 - val_loss: 158.4081\n",
      "Epoch 677/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.3968 - val_loss: 159.0928\n",
      "Epoch 678/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.2775 - val_loss: 157.8937\n",
      "Epoch 679/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.1544 - val_loss: 158.5702\n",
      "Epoch 680/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 106.0284 - val_loss: 157.3642\n",
      "Epoch 681/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 105.9125 - val_loss: 158.0330\n",
      "Epoch 682/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 105.7765 - val_loss: 156.8195\n",
      "Epoch 683/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 105.6712 - val_loss: 157.4810\n",
      "Epoch 684/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 105.5219 - val_loss: 156.2601\n",
      "Epoch 685/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 105.4305 - val_loss: 156.9144\n",
      "Epoch 686/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 105.2647 - val_loss: 155.6858\n",
      "Epoch 687/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 105.1906 - val_loss: 156.3332\n",
      "Epoch 688/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 105.0047 - val_loss: 155.0967\n",
      "Epoch 689/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 104.9512 - val_loss: 155.7374\n",
      "Epoch 690/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 104.7444 - val_loss: 155.0843\n",
      "Epoch 691/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 104.6386 - val_loss: 155.7425\n",
      "Epoch 692/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 104.5610 - val_loss: 154.4425\n",
      "Epoch 693/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 104.4038 - val_loss: 155.0886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 694/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 104.2891 - val_loss: 153.7902\n",
      "Epoch 695/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 104.1692 - val_loss: 154.4255\n",
      "Epoch 696/1500\n",
      "87/87 [==============================] - 0s 195us/step - loss: 104.0153 - val_loss: 153.1267\n",
      "Epoch 697/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 103.9347 - val_loss: 153.7521\n",
      "Epoch 698/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.7544 - val_loss: 153.0270\n",
      "Epoch 699/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.6372 - val_loss: 152.9669\n",
      "Epoch 700/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.5059 - val_loss: 152.9009\n",
      "Epoch 701/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.3741 - val_loss: 152.8289\n",
      "Epoch 702/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.2553 - val_loss: 151.9803\n",
      "Epoch 703/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.1248 - val_loss: 151.9064\n",
      "Epoch 704/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.9901 - val_loss: 151.8248\n",
      "Epoch 705/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 102.8552 - val_loss: 151.7354\n",
      "Epoch 706/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.7315 - val_loss: 150.7536\n",
      "Epoch 707/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.5990 - val_loss: 150.6616\n",
      "Epoch 708/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.4613 - val_loss: 150.5598\n",
      "Epoch 709/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 102.3236 - val_loss: 150.4480\n",
      "Epoch 710/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.1856 - val_loss: 150.3254\n",
      "Epoch 711/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.0570 - val_loss: 149.1593\n",
      "Epoch 712/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.9255 - val_loss: 150.0930\n",
      "Epoch 713/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 101.8086 - val_loss: 148.9426\n",
      "Epoch 714/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 101.6456 - val_loss: 148.7942\n",
      "Epoch 715/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.5069 - val_loss: 148.6337\n",
      "Epoch 716/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.3679 - val_loss: 148.4602\n",
      "Epoch 717/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 101.2284 - val_loss: 148.2732\n",
      "Epoch 718/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.0926 - val_loss: 146.9433\n",
      "Epoch 719/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.9673 - val_loss: 147.9158\n",
      "Epoch 720/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 100.8319 - val_loss: 146.6350\n",
      "Epoch 721/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.6785 - val_loss: 146.4282\n",
      "Epoch 722/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 100.5381 - val_loss: 146.2067\n",
      "Epoch 723/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.3973 - val_loss: 145.9697\n",
      "Epoch 724/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.2560 - val_loss: 145.7166\n",
      "Epoch 725/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.1140 - val_loss: 145.4465\n",
      "Epoch 726/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.9716 - val_loss: 145.1587\n",
      "Epoch 727/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.8285 - val_loss: 144.8525\n",
      "Epoch 728/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.6847 - val_loss: 144.5271\n",
      "Epoch 729/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.5403 - val_loss: 144.1817\n",
      "Epoch 730/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.3951 - val_loss: 143.8157\n",
      "Epoch 731/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.2490 - val_loss: 143.4286\n",
      "Epoch 732/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.1022 - val_loss: 143.0196\n",
      "Epoch 733/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.9545 - val_loss: 142.5884\n",
      "Epoch 734/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 98.8079 - val_loss: 143.9694\n",
      "Epoch 735/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.7172 - val_loss: 141.9542\n",
      "Epoch 736/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.5289 - val_loss: 142.9517\n",
      "Epoch 737/1500\n",
      "87/87 [==============================] - 0s 39us/step - loss: 98.4098 - val_loss: 141.2668\n",
      "Epoch 738/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.2556 - val_loss: 142.0879\n",
      "Epoch 739/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.1088 - val_loss: 140.5578\n",
      "Epoch 740/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 97.9852 - val_loss: 141.3028\n",
      "Epoch 741/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 97.8227 - val_loss: 140.8851\n",
      "Epoch 742/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 97.6795 - val_loss: 140.4497\n",
      "Epoch 743/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 97.5351 - val_loss: 139.9959\n",
      "Epoch 744/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 97.3895 - val_loss: 139.5233\n",
      "Epoch 745/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 97.2425 - val_loss: 139.0313\n",
      "Epoch 746/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 97.1089 - val_loss: 140.0179\n",
      "Epoch 747/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.9806 - val_loss: 138.1626\n",
      "Epoch 748/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.8597 - val_loss: 140.5046\n",
      "Epoch 749/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 96.7705 - val_loss: 137.7985\n",
      "Epoch 750/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 96.5715 - val_loss: 138.2775\n",
      "Epoch 751/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.4165 - val_loss: 137.8645\n",
      "Epoch 752/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.2763 - val_loss: 137.4354\n",
      "Epoch 753/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.1346 - val_loss: 136.9893\n",
      "Epoch 754/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 95.9940 - val_loss: 137.7616\n",
      "Epoch 755/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 95.8744 - val_loss: 136.0917\n",
      "Epoch 756/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 95.7508 - val_loss: 138.1943\n",
      "Epoch 757/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 95.6670 - val_loss: 135.5651\n",
      "Epoch 758/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 95.4885 - val_loss: 137.2972\n",
      "Epoch 759/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 95.3756 - val_loss: 135.6098\n",
      "Epoch 760/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 95.1907 - val_loss: 135.1955\n",
      "Epoch 761/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 95.0520 - val_loss: 134.7653\n",
      "Epoch 762/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.9202 - val_loss: 135.4159\n",
      "Epoch 763/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 94.7853 - val_loss: 133.8660\n",
      "Epoch 764/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.6936 - val_loss: 135.7392\n",
      "Epoch 765/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 94.5666 - val_loss: 133.2392\n",
      "Epoch 766/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.4480 - val_loss: 134.8314\n",
      "Epoch 767/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 94.2838 - val_loss: 133.1917\n",
      "Epoch 768/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.1212 - val_loss: 133.8309\n",
      "Epoch 769/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 93.9891 - val_loss: 133.2735\n",
      "Epoch 770/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 93.8493 - val_loss: 132.6915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 771/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 93.7077 - val_loss: 132.0838\n",
      "Epoch 772/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 93.5802 - val_loss: 134.2216\n",
      "Epoch 773/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 93.5698 - val_loss: 130.8665\n",
      "Epoch 774/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 93.4274 - val_loss: 132.2492\n",
      "Epoch 775/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 93.1941 - val_loss: 131.7182\n",
      "Epoch 776/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 93.0571 - val_loss: 131.1633\n",
      "Epoch 777/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.9182 - val_loss: 130.5836\n",
      "Epoch 778/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.8027 - val_loss: 132.4572\n",
      "Epoch 779/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.7677 - val_loss: 129.2191\n",
      "Epoch 780/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.6559 - val_loss: 130.5489\n",
      "Epoch 781/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.4074 - val_loss: 130.0087\n",
      "Epoch 782/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.2706 - val_loss: 129.4443\n",
      "Epoch 783/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 92.1550 - val_loss: 131.1324\n",
      "Epoch 784/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.1286 - val_loss: 127.9381\n",
      "Epoch 785/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.0114 - val_loss: 129.2732\n",
      "Epoch 786/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 91.7633 - val_loss: 128.7266\n",
      "Epoch 787/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 91.6302 - val_loss: 129.1575\n",
      "Epoch 788/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 91.5150 - val_loss: 127.5374\n",
      "Epoch 789/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 91.4278 - val_loss: 129.1384\n",
      "Epoch 790/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 91.3005 - val_loss: 126.5791\n",
      "Epoch 791/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 91.2115 - val_loss: 128.1083\n",
      "Epoch 792/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 90.9945 - val_loss: 126.5085\n",
      "Epoch 793/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.9161 - val_loss: 128.0408\n",
      "Epoch 794/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 90.7782 - val_loss: 124.9615\n",
      "Epoch 795/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.7744 - val_loss: 126.1592\n",
      "Epoch 796/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 90.5092 - val_loss: 127.5348\n",
      "Epoch 797/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.4487 - val_loss: 124.4462\n",
      "Epoch 798/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.3692 - val_loss: 125.7358\n",
      "Epoch 799/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 90.1162 - val_loss: 127.0171\n",
      "Epoch 800/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.1273 - val_loss: 123.5085\n",
      "Epoch 801/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 90.0364 - val_loss: 124.6016\n",
      "Epoch 802/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 89.7965 - val_loss: 125.7582\n",
      "Epoch 803/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.6817 - val_loss: 122.9833\n",
      "Epoch 804/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 89.6548 - val_loss: 124.1131\n",
      "Epoch 805/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.4191 - val_loss: 125.2447\n",
      "Epoch 806/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 89.3596 - val_loss: 121.9248\n",
      "Epoch 807/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 89.3353 - val_loss: 122.9495\n",
      "Epoch 808/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.1065 - val_loss: 124.0124\n",
      "Epoch 809/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 88.9266 - val_loss: 121.3520\n",
      "Epoch 810/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 88.9638 - val_loss: 122.4049\n",
      "Epoch 811/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.7368 - val_loss: 123.4628\n",
      "Epoch 812/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.5998 - val_loss: 120.7659\n",
      "Epoch 813/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.5954 - val_loss: 121.8120\n",
      "Epoch 814/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 88.3700 - val_loss: 122.8632\n",
      "Epoch 815/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 88.2688 - val_loss: 119.5655\n",
      "Epoch 816/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.2839 - val_loss: 120.5600\n",
      "Epoch 817/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 88.0631 - val_loss: 121.5612\n",
      "Epoch 818/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 87.8506 - val_loss: 121.0286\n",
      "Epoch 819/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 87.7482 - val_loss: 122.0791\n",
      "Epoch 820/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 87.7774 - val_loss: 118.6982\n",
      "Epoch 821/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 87.6629 - val_loss: 119.6853\n",
      "Epoch 822/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 87.4441 - val_loss: 120.6788\n",
      "Epoch 823/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 87.2888 - val_loss: 118.6353\n",
      "Epoch 824/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 87.2505 - val_loss: 119.6529\n",
      "Epoch 825/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 87.0355 - val_loss: 119.8709\n",
      "Epoch 826/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.9304 - val_loss: 117.6886\n",
      "Epoch 827/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.8997 - val_loss: 118.7539\n",
      "Epoch 828/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.6766 - val_loss: 118.9706\n",
      "Epoch 829/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.5574 - val_loss: 117.4223\n",
      "Epoch 830/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.4854 - val_loss: 118.5540\n",
      "Epoch 831/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 86.3215 - val_loss: 116.1558\n",
      "Epoch 832/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 86.2888 - val_loss: 117.2840\n",
      "Epoch 833/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 86.0583 - val_loss: 117.4899\n",
      "Epoch 834/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 85.9305 - val_loss: 115.7963\n",
      "Epoch 835/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 85.8690 - val_loss: 116.9819\n",
      "Epoch 836/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 85.6810 - val_loss: 115.2257\n",
      "Epoch 837/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 85.6101 - val_loss: 116.4299\n",
      "Epoch 838/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 85.4264 - val_loss: 114.6169\n",
      "Epoch 839/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 85.3512 - val_loss: 115.8357\n",
      "Epoch 840/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 85.1669 - val_loss: 113.9711\n",
      "Epoch 841/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 85.0923 - val_loss: 115.2007\n",
      "Epoch 842/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.9026 - val_loss: 113.2898\n",
      "Epoch 843/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.8336 - val_loss: 114.5267\n",
      "Epoch 844/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.6338 - val_loss: 112.5743\n",
      "Epoch 845/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 84.5750 - val_loss: 113.8152\n",
      "Epoch 846/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.3666 - val_loss: 112.8304\n",
      "Epoch 847/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.2475 - val_loss: 114.0986\n",
      "Epoch 848/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 84.1628 - val_loss: 111.1330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 849/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.0550 - val_loss: 112.3374\n",
      "Epoch 850/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 83.8317 - val_loss: 112.4478\n",
      "Epoch 851/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 83.7073 - val_loss: 111.3941\n",
      "Epoch 852/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 83.5693 - val_loss: 112.6578\n",
      "Epoch 853/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 83.4988 - val_loss: 110.4692\n",
      "Epoch 854/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 83.3169 - val_loss: 111.7206\n",
      "Epoch 855/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 83.2028 - val_loss: 109.5275\n",
      "Epoch 856/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 83.0637 - val_loss: 110.7666\n",
      "Epoch 857/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 82.9044 - val_loss: 108.5674\n",
      "Epoch 858/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 82.8101 - val_loss: 109.7942\n",
      "Epoch 859/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 82.6181 - val_loss: 108.6372\n",
      "Epoch 860/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 82.4884 - val_loss: 109.8837\n",
      "Epoch 861/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 82.3844 - val_loss: 107.5699\n",
      "Epoch 862/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 82.2379 - val_loss: 108.7961\n",
      "Epoch 863/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 82.0769 - val_loss: 107.5629\n",
      "Epoch 864/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.9194 - val_loss: 108.8062\n",
      "Epoch 865/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.8451 - val_loss: 106.4014\n",
      "Epoch 866/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.6709 - val_loss: 107.6180\n",
      "Epoch 867/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 81.5252 - val_loss: 106.3215\n",
      "Epoch 868/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 81.3549 - val_loss: 107.5529\n",
      "Epoch 869/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.2877 - val_loss: 105.0760\n",
      "Epoch 870/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.1077 - val_loss: 106.2762\n",
      "Epoch 871/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.9632 - val_loss: 104.9264\n",
      "Epoch 872/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.7938 - val_loss: 106.1394\n",
      "Epoch 873/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.7131 - val_loss: 103.6046\n",
      "Epoch 874/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.5474 - val_loss: 104.7837\n",
      "Epoch 875/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 80.3911 - val_loss: 103.3883\n",
      "Epoch 876/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.2352 - val_loss: 104.5783\n",
      "Epoch 877/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.1238 - val_loss: 103.1068\n",
      "Epoch 878/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 79.9345 - val_loss: 101.6286\n",
      "Epoch 879/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.8391 - val_loss: 102.8566\n",
      "Epoch 880/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.6602 - val_loss: 101.3060\n",
      "Epoch 881/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.5251 - val_loss: 102.5326\n",
      "Epoch 882/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 79.3825 - val_loss: 100.9154\n",
      "Epoch 883/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.2142 - val_loss: 102.1386\n",
      "Epoch 884/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 79.1013 - val_loss: 100.4607\n",
      "Epoch 885/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 78.9056 - val_loss: 101.6782\n",
      "Epoch 886/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 78.8165 - val_loss: 99.9453\n",
      "Epoch 887/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 78.6102 - val_loss: 99.5856\n",
      "Epoch 888/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 78.4696 - val_loss: 100.7429\n",
      "Epoch 889/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 78.3844 - val_loss: 98.8862\n",
      "Epoch 890/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 78.1865 - val_loss: 100.0358\n",
      "Epoch 891/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 78.0888 - val_loss: 98.1425\n",
      "Epoch 892/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 77.9037 - val_loss: 99.2827\n",
      "Epoch 893/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.7894 - val_loss: 97.3561\n",
      "Epoch 894/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.6209 - val_loss: 98.4851\n",
      "Epoch 895/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.4863 - val_loss: 96.5284\n",
      "Epoch 896/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.3453 - val_loss: 100.0824\n",
      "Epoch 897/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 77.4222 - val_loss: 95.7260\n",
      "Epoch 898/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 77.1339 - val_loss: 98.3305\n",
      "Epoch 899/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 77.0234 - val_loss: 96.1784\n",
      "Epoch 900/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 76.8290 - val_loss: 95.8694\n",
      "Epoch 901/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 76.6996 - val_loss: 95.5388\n",
      "Epoch 902/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 76.5685 - val_loss: 95.1859\n",
      "Epoch 903/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 76.4356 - val_loss: 94.8097\n",
      "Epoch 904/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 76.3009 - val_loss: 94.4094\n",
      "Epoch 905/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 76.1689 - val_loss: 95.1528\n",
      "Epoch 906/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 76.0357 - val_loss: 94.6825\n",
      "Epoch 907/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 75.8971 - val_loss: 94.1862\n",
      "Epoch 908/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 75.7564 - val_loss: 93.6632\n",
      "Epoch 909/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 75.6138 - val_loss: 93.1127\n",
      "Epoch 910/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 75.4746 - val_loss: 93.9388\n",
      "Epoch 911/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 75.3386 - val_loss: 93.3038\n",
      "Epoch 912/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 75.1916 - val_loss: 92.6406\n",
      "Epoch 913/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 75.0426 - val_loss: 91.9486\n",
      "Epoch 914/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 74.9008 - val_loss: 92.8275\n",
      "Epoch 915/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.7638 - val_loss: 90.5154\n",
      "Epoch 916/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.6397 - val_loss: 91.3820\n",
      "Epoch 917/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.4595 - val_loss: 90.5943\n",
      "Epoch 918/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 74.3251 - val_loss: 91.4584\n",
      "Epoch 919/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.1749 - val_loss: 89.0031\n",
      "Epoch 920/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.0632 - val_loss: 89.8487\n",
      "Epoch 921/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 73.8824 - val_loss: 90.6683\n",
      "Epoch 922/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 73.7452 - val_loss: 88.1595\n",
      "Epoch 923/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 73.6214 - val_loss: 88.9628\n",
      "Epoch 924/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 73.4427 - val_loss: 89.7408\n",
      "Epoch 925/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 73.3064 - val_loss: 87.1863\n",
      "Epoch 926/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 73.1817 - val_loss: 87.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 927/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 73.0047 - val_loss: 88.6886\n",
      "Epoch 928/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 72.8587 - val_loss: 86.0945\n",
      "Epoch 929/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 72.7434 - val_loss: 86.8205\n",
      "Epoch 930/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 72.5676 - val_loss: 87.5216\n",
      "Epoch 931/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 72.4024 - val_loss: 84.8927\n",
      "Epoch 932/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 72.3056 - val_loss: 85.5826\n",
      "Epoch 933/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 72.1309 - val_loss: 86.2473\n",
      "Epoch 934/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 71.9588 - val_loss: 86.8881\n",
      "Epoch 935/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 71.8483 - val_loss: 84.1372\n",
      "Epoch 936/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 71.6978 - val_loss: 84.7697\n",
      "Epoch 937/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 71.5259 - val_loss: 85.3773\n",
      "Epoch 938/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 71.3744 - val_loss: 84.1268\n",
      "Epoch 939/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 71.2262 - val_loss: 84.7291\n",
      "Epoch 940/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 71.0761 - val_loss: 83.3877\n",
      "Epoch 941/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 70.9251 - val_loss: 83.9820\n",
      "Epoch 942/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.7725 - val_loss: 82.5535\n",
      "Epoch 943/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.6225 - val_loss: 83.1368\n",
      "Epoch 944/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.4636 - val_loss: 81.6261\n",
      "Epoch 945/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.3184 - val_loss: 82.1959\n",
      "Epoch 946/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.1496 - val_loss: 80.6085\n",
      "Epoch 947/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.0128 - val_loss: 81.1623\n",
      "Epoch 948/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 69.8402 - val_loss: 81.6788\n",
      "Epoch 949/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 69.6974 - val_loss: 79.9323\n",
      "Epoch 950/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 69.5356 - val_loss: 80.4334\n",
      "Epoch 951/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 69.3701 - val_loss: 78.6359\n",
      "Epoch 952/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 69.2288 - val_loss: 79.1193\n",
      "Epoch 953/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 69.0568 - val_loss: 79.5615\n",
      "Epoch 954/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 68.9025 - val_loss: 77.6268\n",
      "Epoch 955/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 68.7500 - val_loss: 78.0533\n",
      "Epoch 956/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 68.5789 - val_loss: 78.4370\n",
      "Epoch 957/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 68.4248 - val_loss: 76.3856\n",
      "Epoch 958/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 68.2711 - val_loss: 76.7561\n",
      "Epoch 959/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 68.1005 - val_loss: 77.0825\n",
      "Epoch 960/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 67.9370 - val_loss: 74.9340\n",
      "Epoch 961/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 67.7911 - val_loss: 75.2500\n",
      "Epoch 962/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 67.6206 - val_loss: 75.5204\n",
      "Epoch 963/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 67.4515 - val_loss: 75.7458\n",
      "Epoch 964/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 67.2965 - val_loss: 73.4264\n",
      "Epoch 965/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 67.1406 - val_loss: 73.6510\n",
      "Epoch 966/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 66.9709 - val_loss: 73.8289\n",
      "Epoch 967/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 66.8023 - val_loss: 73.9605\n",
      "Epoch 968/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 66.6366 - val_loss: 71.5105\n",
      "Epoch 969/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 66.4885 - val_loss: 71.6514\n",
      "Epoch 970/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 66.3188 - val_loss: 71.7446\n",
      "Epoch 971/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 66.1500 - val_loss: 71.7899\n",
      "Epoch 972/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.9817 - val_loss: 71.7876\n",
      "Epoch 973/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 65.8139 - val_loss: 71.7373\n",
      "Epoch 974/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 65.6523 - val_loss: 69.0210\n",
      "Epoch 975/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.5224 - val_loss: 71.4618\n",
      "Epoch 976/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.3483 - val_loss: 69.0342\n",
      "Epoch 977/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 65.1765 - val_loss: 71.1992\n",
      "Epoch 978/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.0496 - val_loss: 68.9223\n",
      "Epoch 979/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 64.8548 - val_loss: 68.8250\n",
      "Epoch 980/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 64.6914 - val_loss: 68.6912\n",
      "Epoch 981/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 64.5278 - val_loss: 68.5199\n",
      "Epoch 982/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 64.3638 - val_loss: 68.3104\n",
      "Epoch 983/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 64.2036 - val_loss: 65.8117\n",
      "Epoch 984/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 64.0960 - val_loss: 67.8562\n",
      "Epoch 985/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.8950 - val_loss: 65.5053\n",
      "Epoch 986/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.7657 - val_loss: 67.3882\n",
      "Epoch 987/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.5890 - val_loss: 65.1265\n",
      "Epoch 988/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 63.4415 - val_loss: 66.8945\n",
      "Epoch 989/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 63.2840 - val_loss: 64.6894\n",
      "Epoch 990/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.1217 - val_loss: 66.3700\n",
      "Epoch 991/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 62.9793 - val_loss: 64.2012\n",
      "Epoch 992/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 62.8054 - val_loss: 65.8121\n",
      "Epoch 993/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 62.6773 - val_loss: 62.2675\n",
      "Epoch 994/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 62.8035 - val_loss: 68.4966\n",
      "Epoch 995/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 62.7835 - val_loss: 65.3873\n",
      "Epoch 996/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 62.3402 - val_loss: 63.8111\n",
      "Epoch 997/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 62.1586 - val_loss: 62.9864\n",
      "Epoch 998/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 62.0466 - val_loss: 63.7448\n",
      "Epoch 999/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.9212 - val_loss: 62.8333\n",
      "Epoch 1000/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.7823 - val_loss: 62.7455\n",
      "Epoch 1001/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.6524 - val_loss: 62.6414\n",
      "Epoch 1002/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.5221 - val_loss: 61.5894\n",
      "Epoch 1003/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 61.3932 - val_loss: 62.4598\n",
      "Epoch 1004/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.2808 - val_loss: 60.3399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1005/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 61.3451 - val_loss: 65.7152\n",
      "Epoch 1006/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.4594 - val_loss: 62.9122\n",
      "Epoch 1007/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.0361 - val_loss: 61.4701\n",
      "Epoch 1008/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 60.8336 - val_loss: 59.9935\n",
      "Epoch 1009/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 60.7624 - val_loss: 64.4814\n",
      "Epoch 1010/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 60.9716 - val_loss: 62.5905\n",
      "Epoch 1011/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 60.6620 - val_loss: 60.6801\n",
      "Epoch 1012/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 60.4228 - val_loss: 59.3931\n",
      "Epoch 1013/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 60.3138 - val_loss: 61.6981\n",
      "Epoch 1014/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 60.3205 - val_loss: 60.3628\n",
      "Epoch 1015/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 60.1277 - val_loss: 58.9892\n",
      "Epoch 1016/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 59.9791 - val_loss: 59.5088\n",
      "Epoch 1017/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 59.8818 - val_loss: 58.6881\n",
      "Epoch 1018/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 59.7591 - val_loss: 59.2391\n",
      "Epoch 1019/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.6633 - val_loss: 57.6159\n",
      "Epoch 1020/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.6350 - val_loss: 61.4234\n",
      "Epoch 1021/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 59.6969 - val_loss: 59.2160\n",
      "Epoch 1022/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.3857 - val_loss: 57.7268\n",
      "Epoch 1023/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.2229 - val_loss: 58.2513\n",
      "Epoch 1024/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 59.1198 - val_loss: 56.6500\n",
      "Epoch 1025/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.0893 - val_loss: 60.8310\n",
      "Epoch 1026/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.2114 - val_loss: 58.1829\n",
      "Epoch 1027/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 58.8486 - val_loss: 56.7989\n",
      "Epoch 1028/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 58.7014 - val_loss: 57.2739\n",
      "Epoch 1029/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 58.5924 - val_loss: 56.4064\n",
      "Epoch 1030/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 58.4850 - val_loss: 56.9097\n",
      "Epoch 1031/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 58.3749 - val_loss: 55.9507\n",
      "Epoch 1032/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 58.2713 - val_loss: 57.2259\n",
      "Epoch 1033/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 58.2040 - val_loss: 55.4591\n",
      "Epoch 1034/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 58.0617 - val_loss: 56.7822\n",
      "Epoch 1035/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.9706 - val_loss: 54.9279\n",
      "Epoch 1036/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 57.8513 - val_loss: 57.7737\n",
      "Epoch 1037/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.8862 - val_loss: 55.1503\n",
      "Epoch 1038/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.5990 - val_loss: 55.6566\n",
      "Epoch 1039/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.4963 - val_loss: 54.5404\n",
      "Epoch 1040/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 57.3863 - val_loss: 55.9046\n",
      "Epoch 1041/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 57.3094 - val_loss: 53.9137\n",
      "Epoch 1042/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.1767 - val_loss: 55.3134\n",
      "Epoch 1043/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.0560 - val_loss: 53.2513\n",
      "Epoch 1044/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.9716 - val_loss: 56.2606\n",
      "Epoch 1045/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.9957 - val_loss: 52.8029\n",
      "Epoch 1046/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 56.7673 - val_loss: 55.4980\n",
      "Epoch 1047/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 56.7150 - val_loss: 52.3224\n",
      "Epoch 1048/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 56.5723 - val_loss: 54.8249\n",
      "Epoch 1049/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.4532 - val_loss: 52.4140\n",
      "Epoch 1050/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.3150 - val_loss: 53.5366\n",
      "Epoch 1051/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.1922 - val_loss: 53.2072\n",
      "Epoch 1052/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.0844 - val_loss: 52.8556\n",
      "Epoch 1053/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.9743 - val_loss: 52.4804\n",
      "Epoch 1054/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 55.8621 - val_loss: 52.0805\n",
      "Epoch 1055/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.7475 - val_loss: 51.6550\n",
      "Epoch 1056/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 55.6307 - val_loss: 51.2026\n",
      "Epoch 1057/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 55.5219 - val_loss: 53.5143\n",
      "Epoch 1058/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.5485 - val_loss: 49.4310\n",
      "Epoch 1059/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 55.7006 - val_loss: 54.5070\n",
      "Epoch 1060/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 55.6426 - val_loss: 51.0846\n",
      "Epoch 1061/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.1381 - val_loss: 50.8357\n",
      "Epoch 1062/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.0433 - val_loss: 51.1335\n",
      "Epoch 1063/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.9466 - val_loss: 50.8440\n",
      "Epoch 1064/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.8439 - val_loss: 50.5344\n",
      "Epoch 1065/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.7473 - val_loss: 50.8553\n",
      "Epoch 1066/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.6521 - val_loss: 49.8246\n",
      "Epoch 1067/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.5686 - val_loss: 50.8458\n",
      "Epoch 1068/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.4940 - val_loss: 48.6955\n",
      "Epoch 1069/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 54.5424 - val_loss: 51.7798\n",
      "Epoch 1070/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.5065 - val_loss: 49.1259\n",
      "Epoch 1071/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.2299 - val_loss: 49.8044\n",
      "Epoch 1072/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.1122 - val_loss: 49.0054\n",
      "Epoch 1073/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.0365 - val_loss: 49.7166\n",
      "Epoch 1074/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 53.9560 - val_loss: 48.3068\n",
      "Epoch 1075/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.9146 - val_loss: 50.7485\n",
      "Epoch 1076/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.9643 - val_loss: 48.2412\n",
      "Epoch 1077/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 53.7051 - val_loss: 49.8946\n",
      "Epoch 1078/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.7019 - val_loss: 48.4879\n",
      "Epoch 1079/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.5171 - val_loss: 48.1569\n",
      "Epoch 1080/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 53.4302 - val_loss: 48.3071\n",
      "Epoch 1081/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.3546 - val_loss: 47.9332\n",
      "Epoch 1082/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.2710 - val_loss: 48.0890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1083/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 53.1852 - val_loss: 47.6676\n",
      "Epoch 1084/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.1068 - val_loss: 47.8286\n",
      "Epoch 1085/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 53.0174 - val_loss: 47.9898\n",
      "Epoch 1086/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 52.9407 - val_loss: 46.7432\n",
      "Epoch 1087/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.8787 - val_loss: 48.2556\n",
      "Epoch 1088/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.8100 - val_loss: 46.9723\n",
      "Epoch 1089/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.6834 - val_loss: 47.1319\n",
      "Epoch 1090/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 52.5903 - val_loss: 47.2900\n",
      "Epoch 1091/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.5136 - val_loss: 46.5653\n",
      "Epoch 1092/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.4227 - val_loss: 46.7275\n",
      "Epoch 1093/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 52.3286 - val_loss: 46.0838\n",
      "Epoch 1094/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.2862 - val_loss: 47.7140\n",
      "Epoch 1095/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 52.2268 - val_loss: 46.1428\n",
      "Epoch 1096/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 52.0829 - val_loss: 47.1150\n",
      "Epoch 1097/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 52.0293 - val_loss: 46.2609\n",
      "Epoch 1098/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.9104 - val_loss: 46.2822\n",
      "Epoch 1099/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 51.8295 - val_loss: 46.2957\n",
      "Epoch 1100/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.7487 - val_loss: 45.5707\n",
      "Epoch 1101/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.7044 - val_loss: 47.3312\n",
      "Epoch 1102/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.6864 - val_loss: 44.8946\n",
      "Epoch 1103/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.6994 - val_loss: 47.5544\n",
      "Epoch 1104/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.6196 - val_loss: 45.1731\n",
      "Epoch 1105/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.4251 - val_loss: 46.0171\n",
      "Epoch 1106/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 51.2831 - val_loss: 46.0338\n",
      "Epoch 1107/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.2122 - val_loss: 45.4789\n",
      "Epoch 1108/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 51.1551 - val_loss: 46.0516\n",
      "Epoch 1109/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.0809 - val_loss: 45.4190\n",
      "Epoch 1110/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 51.0047 - val_loss: 46.0480\n",
      "Epoch 1111/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.9482 - val_loss: 44.5958\n",
      "Epoch 1112/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.9322 - val_loss: 45.3931\n",
      "Epoch 1113/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 50.7887 - val_loss: 45.3473\n",
      "Epoch 1114/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.7207 - val_loss: 45.2935\n",
      "Epoch 1115/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.6516 - val_loss: 45.2310\n",
      "Epoch 1116/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.5815 - val_loss: 45.1655\n",
      "Epoch 1117/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 50.5104 - val_loss: 45.1006\n",
      "Epoch 1118/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 50.4384 - val_loss: 45.0290\n",
      "Epoch 1119/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.3653 - val_loss: 44.9504\n",
      "Epoch 1120/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.2913 - val_loss: 44.8643\n",
      "Epoch 1121/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 50.2163 - val_loss: 44.7702\n",
      "Epoch 1122/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.1404 - val_loss: 44.6677\n",
      "Epoch 1123/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.0634 - val_loss: 44.5563\n",
      "Epoch 1124/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.9855 - val_loss: 44.4354\n",
      "Epoch 1125/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.9067 - val_loss: 44.3044\n",
      "Epoch 1126/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.8268 - val_loss: 44.1630\n",
      "Epoch 1127/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 49.7460 - val_loss: 44.0105\n",
      "Epoch 1128/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 49.6642 - val_loss: 43.8464\n",
      "Epoch 1129/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 49.5813 - val_loss: 43.6701\n",
      "Epoch 1130/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.4993 - val_loss: 44.4029\n",
      "Epoch 1131/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.4875 - val_loss: 42.9166\n",
      "Epoch 1132/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.4461 - val_loss: 44.6985\n",
      "Epoch 1133/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 49.4061 - val_loss: 43.2315\n",
      "Epoch 1134/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 49.2151 - val_loss: 43.7361\n",
      "Epoch 1135/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.1495 - val_loss: 42.7092\n",
      "Epoch 1136/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 49.1380 - val_loss: 43.5072\n",
      "Epoch 1137/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.0018 - val_loss: 42.6042\n",
      "Epoch 1138/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.9969 - val_loss: 43.2948\n",
      "Epoch 1139/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 48.8576 - val_loss: 42.4991\n",
      "Epoch 1140/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.8580 - val_loss: 43.0917\n",
      "Epoch 1141/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.7152 - val_loss: 42.3937\n",
      "Epoch 1142/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 48.7206 - val_loss: 42.8933\n",
      "Epoch 1143/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.5738 - val_loss: 42.2880\n",
      "Epoch 1144/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.5847 - val_loss: 42.6970\n",
      "Epoch 1145/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.4327 - val_loss: 42.1817\n",
      "Epoch 1146/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.4499 - val_loss: 42.5006\n",
      "Epoch 1147/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.2972 - val_loss: 42.1971\n",
      "Epoch 1148/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.2496 - val_loss: 42.3068\n",
      "Epoch 1149/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.1628 - val_loss: 42.0600\n",
      "Epoch 1150/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 48.1120 - val_loss: 42.1540\n",
      "Epoch 1151/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.0250 - val_loss: 42.0873\n",
      "Epoch 1152/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 47.9570 - val_loss: 42.0169\n",
      "Epoch 1153/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.8879 - val_loss: 41.9428\n",
      "Epoch 1154/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.8179 - val_loss: 41.8648\n",
      "Epoch 1155/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 47.7468 - val_loss: 41.7827\n",
      "Epoch 1156/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 47.6746 - val_loss: 41.6964\n",
      "Epoch 1157/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.6014 - val_loss: 41.6057\n",
      "Epoch 1158/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.5271 - val_loss: 41.5104\n",
      "Epoch 1159/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.4590 - val_loss: 41.6576\n",
      "Epoch 1160/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 47.3975 - val_loss: 41.3110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1161/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 47.3426 - val_loss: 41.6655\n",
      "Epoch 1162/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.3258 - val_loss: 41.0782\n",
      "Epoch 1163/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 47.3703 - val_loss: 41.5917\n",
      "Epoch 1164/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.2097 - val_loss: 41.1495\n",
      "Epoch 1165/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 47.0908 - val_loss: 41.3620\n",
      "Epoch 1166/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.0036 - val_loss: 41.1541\n",
      "Epoch 1167/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 46.9295 - val_loss: 41.2366\n",
      "Epoch 1168/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.8669 - val_loss: 41.0135\n",
      "Epoch 1169/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 46.8084 - val_loss: 41.1012\n",
      "Epoch 1170/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.7314 - val_loss: 41.0276\n",
      "Epoch 1171/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 46.6619 - val_loss: 40.9499\n",
      "Epoch 1172/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 46.5914 - val_loss: 40.8680\n",
      "Epoch 1173/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.5208 - val_loss: 40.9707\n",
      "Epoch 1174/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.5056 - val_loss: 40.5494\n",
      "Epoch 1175/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 46.5556 - val_loss: 40.9523\n",
      "Epoch 1176/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 46.4406 - val_loss: 40.5791\n",
      "Epoch 1177/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 46.3433 - val_loss: 40.9439\n",
      "Epoch 1178/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.3812 - val_loss: 40.5994\n",
      "Epoch 1179/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.1652 - val_loss: 40.8132\n",
      "Epoch 1180/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.2383 - val_loss: 40.4717\n",
      "Epoch 1181/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 46.0768 - val_loss: 40.6829\n",
      "Epoch 1182/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.0947 - val_loss: 40.3435\n",
      "Epoch 1183/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 45.9980 - val_loss: 40.6783\n",
      "Epoch 1184/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.0467 - val_loss: 40.2634\n",
      "Epoch 1185/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.9155 - val_loss: 40.5676\n",
      "Epoch 1186/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 45.9003 - val_loss: 40.1826\n",
      "Epoch 1187/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.8362 - val_loss: 40.4673\n",
      "Epoch 1188/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.7643 - val_loss: 40.1861\n",
      "Epoch 1189/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.6731 - val_loss: 40.3719\n",
      "Epoch 1190/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.6382 - val_loss: 40.1853\n",
      "Epoch 1191/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.5573 - val_loss: 40.2556\n",
      "Epoch 1192/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 45.5211 - val_loss: 40.0529\n",
      "Epoch 1193/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 45.4764 - val_loss: 40.2617\n",
      "Epoch 1194/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.4743 - val_loss: 39.9394\n",
      "Epoch 1195/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.4164 - val_loss: 40.1479\n",
      "Epoch 1196/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.3431 - val_loss: 39.8234\n",
      "Epoch 1197/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.3651 - val_loss: 40.1406\n",
      "Epoch 1198/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.2996 - val_loss: 39.8277\n",
      "Epoch 1199/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.2189 - val_loss: 40.0292\n",
      "Epoch 1200/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.1693 - val_loss: 39.7121\n",
      "Epoch 1201/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 45.1586 - val_loss: 39.9143\n",
      "Epoch 1202/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.0555 - val_loss: 39.7162\n",
      "Epoch 1203/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.0318 - val_loss: 39.9267\n",
      "Epoch 1204/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.9968 - val_loss: 39.7177\n",
      "Epoch 1205/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 44.9051 - val_loss: 39.7826\n",
      "Epoch 1206/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.8813 - val_loss: 39.5577\n",
      "Epoch 1207/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.8542 - val_loss: 39.7895\n",
      "Epoch 1208/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.8192 - val_loss: 39.5558\n",
      "Epoch 1209/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.7253 - val_loss: 39.6515\n",
      "Epoch 1210/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.6934 - val_loss: 39.4016\n",
      "Epoch 1211/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.6852 - val_loss: 39.6501\n",
      "Epoch 1212/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.6282 - val_loss: 39.3942\n",
      "Epoch 1213/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.5528 - val_loss: 39.4937\n",
      "Epoch 1214/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.5088 - val_loss: 39.3989\n",
      "Epoch 1215/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.4583 - val_loss: 39.2989\n",
      "Epoch 1216/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.4188 - val_loss: 39.4089\n",
      "Epoch 1217/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.3752 - val_loss: 39.2973\n",
      "Epoch 1218/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.3214 - val_loss: 39.1801\n",
      "Epoch 1219/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.2838 - val_loss: 39.3013\n",
      "Epoch 1220/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.2357 - val_loss: 39.1715\n",
      "Epoch 1221/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.1785 - val_loss: 39.2999\n",
      "Epoch 1222/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.1491 - val_loss: 39.1570\n",
      "Epoch 1223/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.0894 - val_loss: 39.0080\n",
      "Epoch 1224/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.0439 - val_loss: 39.1483\n",
      "Epoch 1225/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.0000 - val_loss: 38.9855\n",
      "Epoch 1226/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.9370 - val_loss: 39.1331\n",
      "Epoch 1227/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 43.9095 - val_loss: 38.9563\n",
      "Epoch 1228/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 43.8434 - val_loss: 38.7736\n",
      "Epoch 1229/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.8033 - val_loss: 38.9327\n",
      "Epoch 1230/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 43.7499 - val_loss: 38.7359\n",
      "Epoch 1231/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.6954 - val_loss: 38.9014\n",
      "Epoch 1232/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.6552 - val_loss: 38.6906\n",
      "Epoch 1233/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.5895 - val_loss: 39.1618\n",
      "Epoch 1234/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.6647 - val_loss: 38.5211\n",
      "Epoch 1235/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 43.5582 - val_loss: 38.8668\n",
      "Epoch 1236/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.4912 - val_loss: 38.7235\n",
      "Epoch 1237/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.4298 - val_loss: 38.5739\n",
      "Epoch 1238/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 43.3723 - val_loss: 38.6802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1239/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.3426 - val_loss: 38.5176\n",
      "Epoch 1240/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 43.2826 - val_loss: 38.6321\n",
      "Epoch 1241/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.2535 - val_loss: 38.4560\n",
      "Epoch 1242/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.1926 - val_loss: 38.5783\n",
      "Epoch 1243/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.1624 - val_loss: 38.3884\n",
      "Epoch 1244/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.1025 - val_loss: 38.5183\n",
      "Epoch 1245/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 43.0692 - val_loss: 38.3143\n",
      "Epoch 1246/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.0124 - val_loss: 38.4513\n",
      "Epoch 1247/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.9740 - val_loss: 38.2335\n",
      "Epoch 1248/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.9222 - val_loss: 38.3771\n",
      "Epoch 1249/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 42.8768 - val_loss: 38.1459\n",
      "Epoch 1250/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.8322 - val_loss: 38.2954\n",
      "Epoch 1251/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.7807 - val_loss: 37.7484\n",
      "Epoch 1252/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.8568 - val_loss: 38.3754\n",
      "Epoch 1253/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.8034 - val_loss: 37.8103\n",
      "Epoch 1254/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.6961 - val_loss: 37.8967\n",
      "Epoch 1255/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.6360 - val_loss: 37.9864\n",
      "Epoch 1256/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.5761 - val_loss: 38.0794\n",
      "Epoch 1257/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.5422 - val_loss: 37.9105\n",
      "Epoch 1258/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 42.4963 - val_loss: 38.0110\n",
      "Epoch 1259/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.4705 - val_loss: 37.8283\n",
      "Epoch 1260/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.4152 - val_loss: 37.9363\n",
      "Epoch 1261/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 42.3963 - val_loss: 37.7392\n",
      "Epoch 1262/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.3379 - val_loss: 37.5364\n",
      "Epoch 1263/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.3137 - val_loss: 37.6559\n",
      "Epoch 1264/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.2594 - val_loss: 37.4383\n",
      "Epoch 1265/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.2289 - val_loss: 37.5646\n",
      "Epoch 1266/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.1787 - val_loss: 37.3324\n",
      "Epoch 1267/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.1437 - val_loss: 37.4651\n",
      "Epoch 1268/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.0957 - val_loss: 37.2182\n",
      "Epoch 1269/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.0633 - val_loss: 37.6815\n",
      "Epoch 1270/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.0926 - val_loss: 37.1765\n",
      "Epoch 1271/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.9710 - val_loss: 37.2910\n",
      "Epoch 1272/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 41.9409 - val_loss: 37.0754\n",
      "Epoch 1273/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.8870 - val_loss: 37.1958\n",
      "Epoch 1274/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.8583 - val_loss: 36.9647\n",
      "Epoch 1275/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.8173 - val_loss: 37.4006\n",
      "Epoch 1276/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.8404 - val_loss: 36.9134\n",
      "Epoch 1277/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.7252 - val_loss: 37.0132\n",
      "Epoch 1278/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.7117 - val_loss: 36.8051\n",
      "Epoch 1279/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.6496 - val_loss: 36.9131\n",
      "Epoch 1280/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.6374 - val_loss: 36.6910\n",
      "Epoch 1281/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.5739 - val_loss: 36.8069\n",
      "Epoch 1282/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.5606 - val_loss: 36.5709\n",
      "Epoch 1283/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.4982 - val_loss: 36.6942\n",
      "Epoch 1284/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 41.4813 - val_loss: 36.4444\n",
      "Epoch 1285/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 41.4225 - val_loss: 36.5744\n",
      "Epoch 1286/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.3998 - val_loss: 36.3114\n",
      "Epoch 1287/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.3469 - val_loss: 36.4475\n",
      "Epoch 1288/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.3159 - val_loss: 36.1719\n",
      "Epoch 1289/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 41.2713 - val_loss: 36.3134\n",
      "Epoch 1290/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 41.2299 - val_loss: 36.0258\n",
      "Epoch 1291/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.1966 - val_loss: 36.5392\n",
      "Epoch 1292/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.2741 - val_loss: 36.0257\n",
      "Epoch 1293/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.1266 - val_loss: 36.1354\n",
      "Epoch 1294/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.0699 - val_loss: 35.8952\n",
      "Epoch 1295/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 41.0540 - val_loss: 36.0114\n",
      "Epoch 1296/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.9914 - val_loss: 36.1314\n",
      "Epoch 1297/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.0324 - val_loss: 35.6132\n",
      "Epoch 1298/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.0348 - val_loss: 36.3696\n",
      "Epoch 1299/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 41.0801 - val_loss: 36.0369\n",
      "Epoch 1300/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.8999 - val_loss: 35.7094\n",
      "Epoch 1301/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.8750 - val_loss: 35.7773\n",
      "Epoch 1302/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.8229 - val_loss: 35.8487\n",
      "Epoch 1303/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.7934 - val_loss: 35.7113\n",
      "Epoch 1304/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.7607 - val_loss: 35.7884\n",
      "Epoch 1305/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.7398 - val_loss: 35.6369\n",
      "Epoch 1306/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 40.6973 - val_loss: 35.7199\n",
      "Epoch 1307/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.6839 - val_loss: 35.5535\n",
      "Epoch 1308/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 40.6340 - val_loss: 35.3800\n",
      "Epoch 1309/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.6687 - val_loss: 35.9663\n",
      "Epoch 1310/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.7095 - val_loss: 35.5923\n",
      "Epoch 1311/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 40.5640 - val_loss: 35.4490\n",
      "Epoch 1312/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 40.5398 - val_loss: 35.7643\n",
      "Epoch 1313/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.5609 - val_loss: 35.3935\n",
      "Epoch 1314/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.4905 - val_loss: 35.4947\n",
      "Epoch 1315/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.4576 - val_loss: 35.3451\n",
      "Epoch 1316/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.4478 - val_loss: 35.6707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1317/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.4413 - val_loss: 35.2887\n",
      "Epoch 1318/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.4014 - val_loss: 35.3921\n",
      "Epoch 1319/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.3636 - val_loss: 35.4996\n",
      "Epoch 1320/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 40.3417 - val_loss: 35.3369\n",
      "Epoch 1321/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.3200 - val_loss: 35.4514\n",
      "Epoch 1322/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.2857 - val_loss: 35.2730\n",
      "Epoch 1323/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 40.2757 - val_loss: 35.3947\n",
      "Epoch 1324/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 40.2347 - val_loss: 35.5207\n",
      "Epoch 1325/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.2243 - val_loss: 35.3170\n",
      "Epoch 1326/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.1905 - val_loss: 35.4500\n",
      "Epoch 1327/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.1630 - val_loss: 35.2301\n",
      "Epoch 1328/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.1459 - val_loss: 35.3698\n",
      "Epoch 1329/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 40.1021 - val_loss: 35.5140\n",
      "Epoch 1330/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.0992 - val_loss: 35.2682\n",
      "Epoch 1331/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.0579 - val_loss: 35.4186\n",
      "Epoch 1332/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.0324 - val_loss: 35.1578\n",
      "Epoch 1333/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.0134 - val_loss: 35.3138\n",
      "Epoch 1334/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 39.9672 - val_loss: 35.4743\n",
      "Epoch 1335/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.9830 - val_loss: 34.7566\n",
      "Epoch 1336/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.0527 - val_loss: 35.4677\n",
      "Epoch 1337/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.9077 - val_loss: 35.1644\n",
      "Epoch 1338/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.8945 - val_loss: 35.2499\n",
      "Epoch 1339/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.8605 - val_loss: 35.3388\n",
      "Epoch 1340/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.8261 - val_loss: 35.4661\n",
      "Epoch 1341/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.8112 - val_loss: 35.0863\n",
      "Epoch 1342/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.8152 - val_loss: 35.1799\n",
      "Epoch 1343/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.7792 - val_loss: 35.2771\n",
      "Epoch 1344/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.7430 - val_loss: 35.6621\n",
      "Epoch 1345/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 39.7066 - val_loss: 35.2374\n",
      "Epoch 1346/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.7054 - val_loss: 35.6864\n",
      "Epoch 1347/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.6675 - val_loss: 36.1754\n",
      "Epoch 1348/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 39.7161 - val_loss: 34.9198\n",
      "Epoch 1349/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.6943 - val_loss: 35.3531\n",
      "Epoch 1350/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.6282 - val_loss: 35.7823\n",
      "Epoch 1351/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.5929 - val_loss: 36.2213\n",
      "Epoch 1352/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.6095 - val_loss: 35.1504\n",
      "Epoch 1353/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.5938 - val_loss: 35.3720\n",
      "Epoch 1354/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.5574 - val_loss: 35.8287\n",
      "Epoch 1355/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.5208 - val_loss: 36.2952\n",
      "Epoch 1356/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.5248 - val_loss: 35.1185\n",
      "Epoch 1357/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.5232 - val_loss: 35.3639\n",
      "Epoch 1358/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.4859 - val_loss: 35.8426\n",
      "Epoch 1359/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 39.4483 - val_loss: 36.3310\n",
      "Epoch 1360/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 39.4493 - val_loss: 35.7308\n",
      "Epoch 1361/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.4179 - val_loss: 36.2475\n",
      "Epoch 1362/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.4057 - val_loss: 35.5991\n",
      "Epoch 1363/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.3873 - val_loss: 36.1437\n",
      "Epoch 1364/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.3606 - val_loss: 35.4471\n",
      "Epoch 1365/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.3566 - val_loss: 36.0187\n",
      "Epoch 1366/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.3148 - val_loss: 36.5987\n",
      "Epoch 1367/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.3252 - val_loss: 35.8251\n",
      "Epoch 1368/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.2848 - val_loss: 36.4306\n",
      "Epoch 1369/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.2756 - val_loss: 35.6120\n",
      "Epoch 1370/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.2547 - val_loss: 36.2413\n",
      "Epoch 1371/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.2245 - val_loss: 35.3798\n",
      "Epoch 1372/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 39.2521 - val_loss: 37.2517\n",
      "Epoch 1373/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.2499 - val_loss: 35.4899\n",
      "Epoch 1374/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.2164 - val_loss: 37.0479\n",
      "Epoch 1375/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.1871 - val_loss: 36.4173\n",
      "Epoch 1376/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.1332 - val_loss: 35.7662\n",
      "Epoch 1377/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.1464 - val_loss: 37.3141\n",
      "Epoch 1378/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.1421 - val_loss: 36.6833\n",
      "Epoch 1379/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.0882 - val_loss: 36.0319\n",
      "Epoch 1380/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.0779 - val_loss: 37.5717\n",
      "Epoch 1381/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.1027 - val_loss: 36.0511\n",
      "Epoch 1382/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.0540 - val_loss: 37.4376\n",
      "Epoch 1383/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.0510 - val_loss: 36.8744\n",
      "Epoch 1384/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.0010 - val_loss: 36.2898\n",
      "Epoch 1385/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.9908 - val_loss: 36.7707\n",
      "Epoch 1386/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.9566 - val_loss: 36.1374\n",
      "Epoch 1387/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.9947 - val_loss: 38.4494\n",
      "Epoch 1388/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.1447 - val_loss: 35.7556\n",
      "Epoch 1389/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.0736 - val_loss: 37.1293\n",
      "Epoch 1390/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.9208 - val_loss: 36.8162\n",
      "Epoch 1391/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.8876 - val_loss: 37.1339\n",
      "Epoch 1392/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.8847 - val_loss: 36.7848\n",
      "Epoch 1393/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.8573 - val_loss: 37.1226\n",
      "Epoch 1394/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.8470 - val_loss: 36.7346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1395/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.8312 - val_loss: 37.7705\n",
      "Epoch 1396/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.8603 - val_loss: 36.7170\n",
      "Epoch 1397/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.8122 - val_loss: 37.7559\n",
      "Epoch 1398/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.8276 - val_loss: 36.7005\n",
      "Epoch 1399/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.8009 - val_loss: 38.3266\n",
      "Epoch 1400/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.8987 - val_loss: 36.3594\n",
      "Epoch 1401/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.8628 - val_loss: 37.7230\n",
      "Epoch 1402/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.7691 - val_loss: 36.9024\n",
      "Epoch 1403/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.7331 - val_loss: 38.2475\n",
      "Epoch 1404/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.8143 - val_loss: 36.9210\n",
      "Epoch 1405/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.7165 - val_loss: 38.2013\n",
      "Epoch 1406/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.7798 - val_loss: 37.4304\n",
      "Epoch 1407/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.6790 - val_loss: 37.1436\n",
      "Epoch 1408/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.6560 - val_loss: 37.9793\n",
      "Epoch 1409/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.7021 - val_loss: 37.1343\n",
      "Epoch 1410/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.6403 - val_loss: 38.5076\n",
      "Epoch 1411/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.7667 - val_loss: 37.1547\n",
      "Epoch 1412/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.6231 - val_loss: 38.4534\n",
      "Epoch 1413/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.7254 - val_loss: 37.1667\n",
      "Epoch 1414/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.6081 - val_loss: 38.4138\n",
      "Epoch 1415/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.6877 - val_loss: 37.1740\n",
      "Epoch 1416/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.5944 - val_loss: 38.3838\n",
      "Epoch 1417/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.6524 - val_loss: 37.1783\n",
      "Epoch 1418/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.5890 - val_loss: 38.7360\n",
      "Epoch 1419/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.7161 - val_loss: 37.2564\n",
      "Epoch 1420/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.5667 - val_loss: 38.2890\n",
      "Epoch 1421/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.5974 - val_loss: 37.6479\n",
      "Epoch 1422/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.5102 - val_loss: 37.4169\n",
      "Epoch 1423/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.5122 - val_loss: 38.5263\n",
      "Epoch 1424/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.6081 - val_loss: 37.3731\n",
      "Epoch 1425/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.5184 - val_loss: 38.8841\n",
      "Epoch 1426/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.6739 - val_loss: 37.4451\n",
      "Epoch 1427/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4981 - val_loss: 38.8418\n",
      "Epoch 1428/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.6306 - val_loss: 37.4971\n",
      "Epoch 1429/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4864 - val_loss: 38.4835\n",
      "Epoch 1430/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.5302 - val_loss: 37.4921\n",
      "Epoch 1431/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4821 - val_loss: 38.4949\n",
      "Epoch 1432/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.5138 - val_loss: 37.4898\n",
      "Epoch 1433/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4781 - val_loss: 38.8516\n",
      "Epoch 1434/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.5682 - val_loss: 37.8692\n",
      "Epoch 1435/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.4264 - val_loss: 38.0632\n",
      "Epoch 1436/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4151 - val_loss: 37.8570\n",
      "Epoch 1437/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.4139 - val_loss: 38.5048\n",
      "Epoch 1438/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.4557 - val_loss: 37.3885\n",
      "Epoch 1439/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.4943 - val_loss: 38.8735\n",
      "Epoch 1440/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.5109 - val_loss: 37.8096\n",
      "Epoch 1441/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.4071 - val_loss: 38.4469\n",
      "Epoch 1442/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.3992 - val_loss: 37.7984\n",
      "Epoch 1443/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.3995 - val_loss: 38.9301\n",
      "Epoch 1444/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4828 - val_loss: 37.8084\n",
      "Epoch 1445/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3918 - val_loss: 38.9303\n",
      "Epoch 1446/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 38.4608 - val_loss: 37.8189\n",
      "Epoch 1447/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3855 - val_loss: 38.4874\n",
      "Epoch 1448/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3451 - val_loss: 37.7471\n",
      "Epoch 1449/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3943 - val_loss: 38.9233\n",
      "Epoch 1450/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4223 - val_loss: 37.7659\n",
      "Epoch 1451/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3853 - val_loss: 38.9233\n",
      "Epoch 1452/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.3983 - val_loss: 37.7838\n",
      "Epoch 1453/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.3770 - val_loss: 38.9269\n",
      "Epoch 1454/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3748 - val_loss: 37.8012\n",
      "Epoch 1455/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.3730 - val_loss: 39.2883\n",
      "Epoch 1456/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.4523 - val_loss: 37.8740\n",
      "Epoch 1457/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3560 - val_loss: 39.2553\n",
      "Epoch 1458/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.4184 - val_loss: 37.9291\n",
      "Epoch 1459/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 38.3473 - val_loss: 38.8746\n",
      "Epoch 1460/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3178 - val_loss: 38.2433\n",
      "Epoch 1461/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2936 - val_loss: 38.8624\n",
      "Epoch 1462/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3043 - val_loss: 38.1868\n",
      "Epoch 1463/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.2909 - val_loss: 38.8453\n",
      "Epoch 1464/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.2897 - val_loss: 38.1258\n",
      "Epoch 1465/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2881 - val_loss: 38.8228\n",
      "Epoch 1466/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2740 - val_loss: 38.0608\n",
      "Epoch 1467/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.3029 - val_loss: 39.7144\n",
      "Epoch 1468/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4529 - val_loss: 38.1594\n",
      "Epoch 1469/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.2792 - val_loss: 39.6171\n",
      "Epoch 1470/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.4139 - val_loss: 38.2292\n",
      "Epoch 1471/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.2652 - val_loss: 39.2060\n",
      "Epoch 1472/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2885 - val_loss: 38.5544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1473/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2117 - val_loss: 38.7167\n",
      "Epoch 1474/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.2156 - val_loss: 38.4408\n",
      "Epoch 1475/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2119 - val_loss: 39.1422\n",
      "Epoch 1476/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2569 - val_loss: 38.3813\n",
      "Epoch 1477/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.2139 - val_loss: 40.1357\n",
      "Epoch 1478/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4609 - val_loss: 38.1615\n",
      "Epoch 1479/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.2860 - val_loss: 39.5426\n",
      "Epoch 1480/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.3062 - val_loss: 38.2229\n",
      "Epoch 1481/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2750 - val_loss: 39.5348\n",
      "Epoch 1482/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2817 - val_loss: 38.5813\n",
      "Epoch 1483/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.1807 - val_loss: 39.0959\n",
      "Epoch 1484/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2042 - val_loss: 38.4578\n",
      "Epoch 1485/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.2030 - val_loss: 39.8391\n",
      "Epoch 1486/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.3477 - val_loss: 38.5210\n",
      "Epoch 1487/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.1921 - val_loss: 39.8325\n",
      "Epoch 1488/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3196 - val_loss: 38.5724\n",
      "Epoch 1489/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.1850 - val_loss: 39.4556\n",
      "Epoch 1490/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.2174 - val_loss: 38.8513\n",
      "Epoch 1491/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.1500 - val_loss: 39.0046\n",
      "Epoch 1492/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.1496 - val_loss: 38.7528\n",
      "Epoch 1493/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.1472 - val_loss: 39.3312\n",
      "Epoch 1494/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 38.1830 - val_loss: 38.6184\n",
      "Epoch 1495/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.1619 - val_loss: 39.6964\n",
      "Epoch 1496/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.2425 - val_loss: 38.5551\n",
      "Epoch 1497/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.1782 - val_loss: 40.1387\n",
      "Epoch 1498/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3240 - val_loss: 38.6510\n",
      "Epoch 1499/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.1598 - val_loss: 40.0751\n",
      "Epoch 1500/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2892 - val_loss: 38.7219\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=1500,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
