{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Interpolation/InterpolatedDenMonth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATES</th>\n",
       "      <th>D REVENUE</th>\n",
       "      <th>U CR</th>\n",
       "      <th>D OE</th>\n",
       "      <th>D NOI</th>\n",
       "      <th>U CAPEX</th>\n",
       "      <th>U CWK</th>\n",
       "      <th>D FCF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>1884.372544</td>\n",
       "      <td>976.202014</td>\n",
       "      <td>475.249997</td>\n",
       "      <td>757.519678</td>\n",
       "      <td>207.477947</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>856.600959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1884.566826</td>\n",
       "      <td>983.762225</td>\n",
       "      <td>485.004015</td>\n",
       "      <td>734.017979</td>\n",
       "      <td>207.303532</td>\n",
       "      <td>3638.472896</td>\n",
       "      <td>810.859727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1884.761107</td>\n",
       "      <td>991.322435</td>\n",
       "      <td>494.758033</td>\n",
       "      <td>710.516281</td>\n",
       "      <td>207.129117</td>\n",
       "      <td>3676.945791</td>\n",
       "      <td>765.118495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1884.955389</td>\n",
       "      <td>998.882646</td>\n",
       "      <td>504.512051</td>\n",
       "      <td>687.014582</td>\n",
       "      <td>206.954702</td>\n",
       "      <td>3715.418687</td>\n",
       "      <td>719.377263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1880.767673</td>\n",
       "      <td>1006.377690</td>\n",
       "      <td>481.542613</td>\n",
       "      <td>511.922217</td>\n",
       "      <td>207.283705</td>\n",
       "      <td>3792.839197</td>\n",
       "      <td>732.414605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>1785.193900</td>\n",
       "      <td>972.274049</td>\n",
       "      <td>461.469501</td>\n",
       "      <td>414.377788</td>\n",
       "      <td>98.125058</td>\n",
       "      <td>2795.231076</td>\n",
       "      <td>733.164793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>1730.706270</td>\n",
       "      <td>953.165883</td>\n",
       "      <td>488.056175</td>\n",
       "      <td>424.307526</td>\n",
       "      <td>95.323030</td>\n",
       "      <td>2858.452805</td>\n",
       "      <td>729.764211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>1667.921494</td>\n",
       "      <td>856.900384</td>\n",
       "      <td>471.261535</td>\n",
       "      <td>434.874583</td>\n",
       "      <td>92.144220</td>\n",
       "      <td>2849.606546</td>\n",
       "      <td>737.713784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1605.136718</td>\n",
       "      <td>760.634885</td>\n",
       "      <td>454.466895</td>\n",
       "      <td>445.441639</td>\n",
       "      <td>88.965411</td>\n",
       "      <td>2840.760287</td>\n",
       "      <td>745.663358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1542.351942</td>\n",
       "      <td>664.369385</td>\n",
       "      <td>437.672255</td>\n",
       "      <td>456.008696</td>\n",
       "      <td>85.786601</td>\n",
       "      <td>2831.914028</td>\n",
       "      <td>753.612931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DATES    D REVENUE         U CR        D OE       D NOI     U CAPEX  \\\n",
       "0    2009-12-31  1884.372544   976.202014  475.249997  757.519678  207.477947   \n",
       "1    2010-01-31  1884.566826   983.762225  485.004015  734.017979  207.303532   \n",
       "2    2010-02-28  1884.761107   991.322435  494.758033  710.516281  207.129117   \n",
       "3    2010-03-31  1884.955389   998.882646  504.512051  687.014582  206.954702   \n",
       "4    2010-04-30  1880.767673  1006.377690  481.542613  511.922217  207.283705   \n",
       "..          ...          ...          ...         ...         ...         ...   \n",
       "104  2018-08-31  1785.193900   972.274049  461.469501  414.377788   98.125058   \n",
       "105  2018-09-30  1730.706270   953.165883  488.056175  424.307526   95.323030   \n",
       "106  2018-10-31  1667.921494   856.900384  471.261535  434.874583   92.144220   \n",
       "107  2018-11-30  1605.136718   760.634885  454.466895  445.441639   88.965411   \n",
       "108  2018-12-31  1542.351942   664.369385  437.672255  456.008696   85.786601   \n",
       "\n",
       "           U CWK       D FCF  \n",
       "0    3600.000000  856.600959  \n",
       "1    3638.472896  810.859727  \n",
       "2    3676.945791  765.118495  \n",
       "3    3715.418687  719.377263  \n",
       "4    3792.839197  732.414605  \n",
       "..           ...         ...  \n",
       "104  2795.231076  733.164793  \n",
       "105  2858.452805  729.764211  \n",
       "106  2849.606546  737.713784  \n",
       "107  2840.760287  745.663358  \n",
       "108  2831.914028  753.612931  \n",
       "\n",
       "[109 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2009-12-31', 1884.372544, 976.2020142, 475.24999739999987,\n",
       "        757.519678, 207.4779469, 3600.0, 856.6009594000002],\n",
       "       ['2010-01-31', 1884.5668256666668, 983.7622247333335, 485.0040154,\n",
       "        734.0179793333333, 207.30353190000002, 3638.472895666667,\n",
       "        810.8597272333334],\n",
       "       ['2010-02-28', 1884.761107333333, 991.3224352666666,\n",
       "        494.75803339999993, 710.5162806666667, 207.12911689999999,\n",
       "        3676.945791333333, 765.1184950666667],\n",
       "       ['2010-03-31', 1884.955389, 998.8826458, 504.5120514, 687.014582,\n",
       "        206.9547019, 3715.418687, 719.3772629],\n",
       "       ['2010-04-30', 1880.767673333333, 1006.3776901999997,\n",
       "        481.5426128333333, 511.92221730000006, 207.28370463333331,\n",
       "        3792.8391966666654, 732.4146046],\n",
       "       ['2010-05-31', 1876.5799576666668, 1013.8727346000001,\n",
       "        458.5731742666666, 336.8298526000001, 207.61270736666665,\n",
       "        3870.2597063333333, 745.4519462999999],\n",
       "       ['2010-06-30', 1872.392242, 1021.367779, 435.6037357, 161.7374879,\n",
       "        207.9417101, 3947.680216, 758.489288],\n",
       "       ['2010-07-31', 1853.240114, 1015.1698886666666, 421.8261523,\n",
       "        187.6358415, 207.7144804333333, 3965.748594666666,\n",
       "        756.6899263333332],\n",
       "       ['2010-08-31', 1834.087986, 1008.9719983333333, 408.0485689,\n",
       "        213.5341951, 207.48725076666665, 3983.816973333333,\n",
       "        754.8905646666667],\n",
       "       ['2010-09-30', 1814.935858, 1002.774108, 394.2709855, 239.4325487,\n",
       "        207.2600211, 4001.885352, 753.0912030000002],\n",
       "       ['2010-10-31', 1851.320774, 1008.5464486666666, 411.1460836666666,\n",
       "        246.73378476666667, 212.21753106666665, 4093.646761333333,\n",
       "        715.3372166666667],\n",
       "       ['2010-11-30', 1887.70569, 1014.3187893333335, 428.0211818333333,\n",
       "        254.03502083333333, 217.1750410333333, 4185.408170666667,\n",
       "        677.5832303333334],\n",
       "       ['2010-12-31', 1924.090606, 1020.09113, 444.89628, 261.3362569,\n",
       "        222.132551, 4277.16958, 639.829244],\n",
       "       ['2011-01-31', 1840.631611, 1004.3123142000001, 439.1857453333333,\n",
       "        253.3520595, 221.3439679, 4255.687593333333, 625.4113584666667],\n",
       "       ['2011-02-28', 1757.172616, 988.5334983999999, 433.47521066666667,\n",
       "        245.36786209999997, 220.5553848, 4234.205606666666,\n",
       "        610.9934729333334],\n",
       "       ['2011-03-31', 1673.713621, 972.7546826, 427.764676, 237.3836647,\n",
       "        219.7668017, 4212.723620000002, 596.5755874],\n",
       "       ['2011-04-30', 1697.5040236666666, 943.8024453, 430.2420961,\n",
       "        237.04147333333333, 223.6841742333333, 4253.569522333334,\n",
       "        596.6860897999999],\n",
       "       ['2011-05-31', 1721.2944263333334, 914.850208, 432.71951619999993,\n",
       "        236.69928196666663, 227.60154676666667, 4294.415424666667,\n",
       "        596.7965922000002],\n",
       "       ['2011-06-30', 1745.084829, 885.8979707000002, 435.1969363,\n",
       "        236.3570906, 231.5189193, 4335.261327, 596.9070946],\n",
       "       ['2011-07-31', 1775.998593, 875.2949231666668, 444.6121419,\n",
       "        236.2388491, 227.72962546666668, 4329.278552666667,\n",
       "        628.3878588333333],\n",
       "       ['2011-08-31', 1806.912357, 864.6918756333333, 454.02734749999996,\n",
       "        236.1206076, 223.94033163333333, 4323.295778333333,\n",
       "        659.8686230666667],\n",
       "       ['2011-09-30', 1837.826121, 854.0888281, 463.44255310000005,\n",
       "        236.0023661, 220.1510378, 4317.313004, 691.3493873],\n",
       "       ['2011-10-31', 1885.0705423333332, 857.2014145333335,\n",
       "        470.0062231666667, 244.5463731, 212.89141523333333,\n",
       "        4309.167693666666, 690.0065109333333],\n",
       "       ['2011-11-30', 1932.314963666667, 860.3140009666665,\n",
       "        476.56989323333335, 253.0903801, 205.63179266666666,\n",
       "        4301.022383333333, 688.6636345666667],\n",
       "       ['2011-12-31', 1979.559385, 863.4265874, 483.13356330000005,\n",
       "        261.6343871, 198.3721701, 4292.877073, 687.3207582],\n",
       "       ['2012-01-31', 1910.3834399999998, 858.4811889666668,\n",
       "        483.72593293333335, 263.6296255, 198.3738698333333, 4291.272806,\n",
       "        711.6253066666667],\n",
       "       ['2012-02-29', 1841.2074949999999, 853.5357905333334,\n",
       "        484.3183025666667, 265.6248639, 198.37556956666668,\n",
       "        4289.668538999998, 735.9298551333335],\n",
       "       ['2012-03-31', 1772.03155, 848.5903921, 484.9106722, 267.6201023,\n",
       "        198.3772693, 4288.064272, 760.2344036000002],\n",
       "       ['2012-04-30', 1805.3226533333332, 852.7747259666667,\n",
       "        500.10971653333337, 262.53434903333334, 203.80507153333332,\n",
       "        4248.180273333333, 760.4925157666668],\n",
       "       ['2012-05-31', 1838.6137566666666, 856.9590598333334,\n",
       "        515.3087608666667, 257.44859576666664, 209.23287376666664,\n",
       "        4208.296274666666, 760.7506279333335],\n",
       "       ['2012-06-30', 1871.90486, 861.1433937, 530.5078052, 252.3628425,\n",
       "        214.660676, 4168.412276, 761.0087401000002],\n",
       "       ['2012-07-31', 1870.0189613333332, 888.6962120999999,\n",
       "        512.9147551333333, 274.75087963333334, 210.84279596666664,\n",
       "        4100.139316, 761.8369197333334],\n",
       "       ['2012-08-31', 1868.1330626666668, 916.2490305,\n",
       "        495.32170506666665, 297.1389167666666, 207.02491593333332,\n",
       "        4031.8663560000005, 762.6650993666667],\n",
       "       ['2012-09-30', 1866.247164, 943.8018489, 477.728655, 319.5269539,\n",
       "        203.2070359, 3963.593396, 763.493279],\n",
       "       ['2012-10-31', 1872.4550559999998, 871.4747433666665,\n",
       "        485.99178150000006, 323.89898873333334, 198.78250549999998,\n",
       "        3581.357899333333, 776.451154],\n",
       "       ['2012-11-30', 1878.6629480000001, 799.1476378333333, 494.254908,\n",
       "        328.27102356666666, 194.3579751, 3199.1224026666664, 789.409029],\n",
       "       ['2012-12-31', 1884.87084, 726.8205323, 502.5180345, 332.6430584,\n",
       "        189.9334447, 2816.886906, 802.366904],\n",
       "       ['2013-01-31', 1896.30791, 753.0427803333333, 514.5447965666667,\n",
       "        333.5149285333333, 192.01279616666667, 2893.339288,\n",
       "        793.3964250333332],\n",
       "       ['2013-02-28', 1907.7449800000002, 779.2650283666666,\n",
       "        526.5715586333333, 334.38679866666666, 194.09214763333333,\n",
       "        2969.79167, 784.4259460666666],\n",
       "       ['2013-03-31', 1919.18205, 805.4872763999998, 538.5983207,\n",
       "        335.2586688, 196.1714991, 3046.244052, 775.4554671],\n",
       "       ['2013-04-30', 1867.7861953333331, 783.4079046999999,\n",
       "        530.8571651000001, 335.86746453333336, 195.6670658333333,\n",
       "        3048.921046333333, 766.0225778666667],\n",
       "       ['2013-05-31', 1816.3903406666666, 761.328533, 523.1160095,\n",
       "        336.47626026666666, 195.16263256666667, 3051.5980406666667,\n",
       "        756.5896886333333],\n",
       "       ['2013-06-30', 1764.994486, 739.2491613, 515.3748539, 337.085056,\n",
       "        194.6581993, 3054.275035, 747.1567994],\n",
       "       ['2013-07-31', 1768.6169343333331, 757.7170030999998, 520.4034373,\n",
       "        329.2451542333333, 198.2425908, 2952.306149666667, 757.3171348],\n",
       "       ['2013-08-31', 1772.2393826666666, 776.1848449, 525.4320207000002,\n",
       "        321.40525246666664, 201.8269823, 2850.337264333333,\n",
       "        767.4774702000001],\n",
       "       ['2013-09-30', 1775.861831, 794.6526867, 530.4606041000002,\n",
       "        313.5653506999999, 205.4113738, 2748.368379, 777.6378056],\n",
       "       ['2013-10-31', 1781.312525333333, 813.7964820333333,\n",
       "        531.6057520333334, 320.4643399333333, 203.1795522,\n",
       "        2446.6776800000002, 776.7867944],\n",
       "       ['2013-11-30', 1786.7632196666668, 832.9402773666667,\n",
       "        532.7508999666667, 327.36332916666663, 200.9477306, 2144.986981,\n",
       "        775.9357832000001],\n",
       "       ['2013-12-31', 1792.2139140000004, 852.0840727, 533.8960479,\n",
       "        334.2623184, 198.715909, 1843.296282, 775.0847719999998],\n",
       "       ['2014-01-31', 1772.1792736666669, 861.7296213999998, 528.3798523,\n",
       "        342.6429393, 191.0202412333333, 1839.5208699999998,\n",
       "        774.5279383333333],\n",
       "       ['2014-02-28', 1752.144633333333, 871.3751701, 522.8636567,\n",
       "        351.0235602, 183.32457346666664, 1835.7454579999999,\n",
       "        773.9711046666665],\n",
       "       ['2014-03-31', 1732.109993, 881.0207187999998, 517.3474611,\n",
       "        359.4041811, 175.6289057, 1831.970046, 773.414271],\n",
       "       ['2014-04-30', 1721.8949903333335, 890.6855609999999,\n",
       "        507.92859023333335, 349.91219946666666, 168.82089786666666,\n",
       "        1848.738663666667, 760.1008118333333],\n",
       "       ['2014-05-31', 1711.6799876666666, 900.3504032,\n",
       "        498.50971936666673, 340.4202178333333, 162.01289003333332,\n",
       "        1865.507281333333, 746.7873526666666],\n",
       "       ['2014-06-30', 1701.464985, 910.0152454, 489.09084850000005,\n",
       "        330.9282362, 155.2048822, 1882.275899, 733.4738934999998],\n",
       "       ['2014-07-31', 1728.4231946666666, 914.2662125, 481.6856930666667,\n",
       "        331.95640223333334, 150.8917052333333, 1896.6301780000001,\n",
       "        727.0572944999999],\n",
       "       ['2014-08-31', 1755.3814043333334, 918.5171796,\n",
       "        474.28053763333327, 332.98456826666666, 146.57852826666667,\n",
       "        1910.984457, 720.6406955],\n",
       "       ['2014-09-30', 1782.339614, 922.7681467, 466.8753822,\n",
       "        334.0127343000001, 142.2653513, 1925.338736, 714.2240965],\n",
       "       ['2014-10-31', 1725.137324333333, 965.4992654666668,\n",
       "        478.28002593333326, 415.6834258, 141.41725303333334,\n",
       "        1946.0452103333328, 728.1806636666665],\n",
       "       ['2014-11-30', 1667.935034666667, 1008.2303842333333,\n",
       "        489.68466966666665, 497.35411730000004, 140.56915476666666,\n",
       "        1966.751684666667, 742.1372308333333],\n",
       "       ['2014-12-31', 1610.732745, 1050.961503, 501.0893134, 579.0248088,\n",
       "        139.7210565, 1987.458159, 756.093798],\n",
       "       ['2015-01-31', 1746.422448666667, 1054.429111, 508.5803035333333,\n",
       "        579.2034107000001, 137.07026573333334, 2005.468421,\n",
       "        771.8714318333333],\n",
       "       ['2015-02-28', 1882.112152333333, 1057.8967189999998,\n",
       "        516.0712936666666, 579.3820125999998, 134.41947496666666,\n",
       "        2023.478683, 787.6490656666667],\n",
       "       ['2015-03-31', 2017.801856, 1061.364327, 523.5622838,\n",
       "        579.5606144999998, 131.7686842, 2041.488945, 803.4266995],\n",
       "       ['2015-04-30', 2107.344116, 1096.44801, 526.1169451000002,\n",
       "        577.4851386666667, 131.0318083, 2038.723757, 790.2704643666667],\n",
       "       ['2015-05-31', 2196.8863760000004, 1131.531693, 528.6716064,\n",
       "        575.4096628333333, 130.2949324, 2035.9585690000001,\n",
       "        777.1142292333334],\n",
       "       ['2015-06-30', 2286.428636, 1166.615376, 531.2262677, 573.334187,\n",
       "        129.5580565, 2033.193381, 763.9579941000002],\n",
       "       ['2015-07-31', 2528.4880129999997, 1194.063472, 562.655131,\n",
       "        551.8155191666667, 128.9129271, 2038.7990923333327, 725.2341621],\n",
       "       ['2015-08-31', 2770.5473899999997, 1221.511568, 594.0839943000002,\n",
       "        530.2968513333334, 128.2677977, 2044.4048036666666, 686.5103301],\n",
       "       ['2015-09-30', 3012.606767, 1248.959664, 625.5128576000002,\n",
       "        508.7781835, 127.6226683, 2050.010515, 647.7864981],\n",
       "       ['2015-10-31', 2903.7366663333332, 1269.6701176666666,\n",
       "        641.4096041, 508.9514627, 117.39104201, 2031.2492326666666,\n",
       "        646.7970369666667],\n",
       "       ['2015-11-30', 2794.8665656666667, 1290.3805713333334,\n",
       "        657.3063506000002, 509.1247419, 107.15941572, 2012.4879503333332,\n",
       "        645.8075758333333],\n",
       "       ['2015-12-31', 2685.996465, 1311.091025, 673.2030971, 509.2980211,\n",
       "        96.92778943, 1993.726668, 644.8181147000001],\n",
       "       ['2016-01-31', 2834.140780666666, 1366.4444733333332,\n",
       "        674.0792667333334, 500.8613563, 382.7185262866667, 2146.965891,\n",
       "        645.8685558666667],\n",
       "       ['2016-02-29', 2982.285096333333, 1421.7979216666665,\n",
       "        674.9554363666666, 492.4246915, 668.5092631433333, 2300.205114,\n",
       "        646.9189970333333],\n",
       "       ['2016-03-31', 3130.429412, 1477.1513699999996, 675.831606,\n",
       "        483.9880267, 954.3, 2453.444337, 647.9694382],\n",
       "       ['2016-04-30', 3060.8571920000004, 1508.7480799999996,\n",
       "        653.6836567666667, 473.8452801, 669.9162089666665,\n",
       "        2448.972803333333, 637.6159774666667],\n",
       "       ['2016-05-31', 2991.284972, 1540.34479, 631.5357075333333,\n",
       "        463.7025335, 385.5324179333333, 2444.5012696666668,\n",
       "        627.2625167333333],\n",
       "       ['2016-06-30', 2921.712752, 1571.9415, 609.3877583, 453.5597869,\n",
       "        101.1486269, 2440.029736, 616.909056],\n",
       "       ['2016-07-31', 2979.422658, 1553.679082333333, 612.0762081333334,\n",
       "        473.88566743333337, 103.19915363333334, 2475.9393219999997,\n",
       "        608.7162381333333],\n",
       "       ['2016-08-31', 3037.132564, 1535.416664666667, 614.7646579666666,\n",
       "        494.2115479666666, 105.24968036666668, 2511.848908,\n",
       "        600.5234202666667],\n",
       "       ['2016-09-30', 3094.84247, 1517.1542470000004, 617.4531078,\n",
       "        514.5374285, 107.3002071, 2547.758494, 592.3306024],\n",
       "       ['2016-10-31', 2941.914112333333, 1493.7506856666669,\n",
       "        601.3888980666667, 497.1393087333333, 106.27214706666666,\n",
       "        2492.278597666667, 605.3801967666667],\n",
       "       ['2016-11-30', 2788.9857546666667, 1470.347124333333,\n",
       "        585.3246883333334, 479.7411889666667, 105.24408703333332,\n",
       "        2436.798701333333, 618.4297911333333],\n",
       "       ['2016-12-31', 2636.057397, 1446.943563, 569.2604786, 462.3430692,\n",
       "        104.216027, 2381.318805, 631.4793855],\n",
       "       ['2017-01-31', 2629.386708333333, 1421.7832626666666,\n",
       "        553.3237412999999, 463.41115033333335, 105.35134376666666,\n",
       "        2394.287123333333, 669.1150419666667],\n",
       "       ['2017-02-28', 2622.7160196666664, 1396.6229623333334,\n",
       "        537.3870039999998, 464.47923146666665, 106.48666053333334,\n",
       "        2407.2554416666667, 706.7506984333335],\n",
       "       ['2017-03-31', 2616.045331, 1371.462662, 521.4502666999998,\n",
       "        465.5473126, 107.6219773, 2420.2237600000008, 744.3863549],\n",
       "       ['2017-04-30', 2582.999953333333, 1385.6997396666666,\n",
       "        514.2855105000001, 456.63566276666666, 110.51347520000002,\n",
       "        2409.258488666667, 756.4180495333335],\n",
       "       ['2017-05-31', 2549.9545756666666, 1399.9368173333332,\n",
       "        507.12075430000004, 447.72401293333337, 113.4049731,\n",
       "        2398.293217333333, 768.4497441666666],\n",
       "       ['2017-06-30', 2516.909198, 1414.173895, 499.9559981, 438.8123631,\n",
       "        116.296471, 2387.327946, 780.4814388],\n",
       "       ['2017-07-31', 2529.782464, 1411.1019353333334,\n",
       "        495.39090156666674, 419.9090564333334, 113.3286994,\n",
       "        2482.2845416666664, 773.9692328666665],\n",
       "       ['2017-08-31', 2542.65573, 1408.0299756666666, 490.8258050333334,\n",
       "        401.00574976666667, 110.3609278, 2577.241137333333,\n",
       "        767.4570269333334],\n",
       "       ['2017-09-30', 2555.528996, 1404.958016, 486.2607085, 382.1024431,\n",
       "        107.3931562, 2672.197733, 760.944821],\n",
       "       ['2017-10-31', 2493.5604476666667, 1340.4287853333333,\n",
       "        485.59986143333333, 386.4377233, 106.82599553333333,\n",
       "        2640.667416333333, 748.7807167999999],\n",
       "       ['2017-11-30', 2431.591899333333, 1275.8995546666667,\n",
       "        484.9390143666667, 390.7730035, 106.25883486666667,\n",
       "        2609.1370996666665, 736.6166125999998],\n",
       "       ['2017-12-31', 2369.623351, 1211.370324, 484.27816730000006,\n",
       "        395.1082837, 105.6916742, 2577.606783, 724.4525083999998],\n",
       "       ['2018-01-31', 2307.4944116666666, 1212.1657916666666,\n",
       "        479.0572197666667, 392.9066318, 102.59903074333334,\n",
       "        2577.6159756666666, 729.9813198666666],\n",
       "       ['2018-02-28', 2245.365472333333, 1212.9612593333334,\n",
       "        473.83627223333326, 390.7049799, 99.50638728666668,\n",
       "        2577.625168333333, 735.5101313333333],\n",
       "       ['2018-03-31', 2183.236533, 1213.756727, 468.6153247, 388.503328,\n",
       "        96.41374383, 2577.634361, 741.0389428],\n",
       "       ['2018-04-30', 2086.8807423333333, 1146.0012783333334,\n",
       "        448.50893473333326, 390.50832306666666, 98.85220068666668,\n",
       "        2608.0187796666664, 740.6812809333335],\n",
       "       ['2018-05-31', 1990.5249516666665, 1078.2458296666666,\n",
       "        428.40254476666667, 392.51331813333326, 101.29065754333334,\n",
       "        2638.403198333333, 740.3236190666668],\n",
       "       ['2018-06-30', 1894.169161, 1010.490381, 408.2961548, 394.5183132,\n",
       "        103.7291144, 2668.787617, 739.9659572],\n",
       "       ['2018-07-31', 1839.681530666667, 991.3822151, 434.8828281333333,\n",
       "        404.4480508333333, 100.92708618333334, 2732.0093463333333,\n",
       "        736.5653751333334],\n",
       "       ['2018-08-31', 1785.1939003333332, 972.2740492,\n",
       "        461.46950146666666, 414.37778846666674, 98.12505796666667,\n",
       "        2795.231075666666, 733.1647930666667],\n",
       "       ['2018-09-30', 1730.70627, 953.1658833, 488.0561748, 424.3075261,\n",
       "        95.32302975, 2858.452805, 729.764211],\n",
       "       ['2018-10-31', 1667.921494, 856.9003839999998, 471.2615347666666,\n",
       "        434.87458276666666, 92.14422016333332, 2849.606546,\n",
       "        737.7137843333335],\n",
       "       ['2018-11-30', 1605.136718, 760.6348846999998, 454.4668947333333,\n",
       "        445.44163943333336, 88.96541057666668, 2840.760287,\n",
       "        745.6633576666667],\n",
       "       ['2018-12-31', 1542.351942, 664.3693853999998, 437.67225470000005,\n",
       "        456.0086961, 85.78660099, 2831.914028, 753.612931]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,1:7]\n",
    "Y = dataset[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21536771, 0.34358992, 0.28760773, 1.        , 0.14011453,\n",
       "        0.70628215],\n",
       "       [0.21549004, 0.35192007, 0.32225043, 0.9605532 , 0.13991371,\n",
       "        0.72165108],\n",
       "       [0.21561238, 0.36025022, 0.35689312, 0.92110641, 0.13971289,\n",
       "        0.73702   ],\n",
       "       [0.21573472, 0.36858036, 0.39153581, 0.88165961, 0.13951207,\n",
       "        0.75238893],\n",
       "       [0.21309775, 0.37683871, 0.30995679, 0.58777307, 0.13989088,\n",
       "        0.78331641],\n",
       "       [0.21046078, 0.38509706, 0.22837778, 0.29388654, 0.14026969,\n",
       "        0.8142439 ],\n",
       "       [0.2078238 , 0.3933554 , 0.14679876, 0.        , 0.1406485 ,\n",
       "        0.84517139],\n",
       "       [0.19576386, 0.38652631, 0.09786584, 0.0434695 , 0.14038687,\n",
       "        0.85238924],\n",
       "       [0.18370391, 0.37969722, 0.04893292, 0.086939  , 0.14012524,\n",
       "        0.85960709],\n",
       "       [0.17164397, 0.37286814, 0.        , 0.1304085 , 0.13986361,\n",
       "        0.86682494],\n",
       "       [0.19455526, 0.37922834, 0.05993416, 0.14266337, 0.14557165,\n",
       "        0.90348124],\n",
       "       [0.21746656, 0.38558854, 0.11986831, 0.15491825, 0.15127969,\n",
       "        0.94013755],\n",
       "       [0.24037786, 0.39194874, 0.17980247, 0.16717312, 0.15698773,\n",
       "        0.97679385],\n",
       "       [0.18782438, 0.37456299, 0.15952074, 0.15377192, 0.15607976,\n",
       "        0.96821236],\n",
       "       [0.1352709 , 0.35717725, 0.13923902, 0.14037072, 0.1551718 ,\n",
       "        0.95963086],\n",
       "       [0.08271742, 0.33979151, 0.1189573 , 0.12696952, 0.15426383,\n",
       "        0.95104936],\n",
       "       [0.09769806, 0.30789075, 0.12775618, 0.12639516, 0.15877426,\n",
       "        0.96736624],\n",
       "       [0.11267869, 0.27598999, 0.13655507, 0.1258208 , 0.1632847 ,\n",
       "        0.98368312],\n",
       "       [0.12765932, 0.24408924, 0.14535396, 0.12524645, 0.16779513,\n",
       "        1.        ],\n",
       "       [0.14712547, 0.23240637, 0.17879331, 0.12504798, 0.16343216,\n",
       "        0.99761004],\n",
       "       [0.16659163, 0.2207235 , 0.21223267, 0.12484952, 0.1590692 ,\n",
       "        0.99522007],\n",
       "       [0.18605779, 0.20904063, 0.24567202, 0.12465105, 0.15470623,\n",
       "        0.99283011],\n",
       "       [0.21580723, 0.2124702 , 0.26898377, 0.13899188, 0.14634756,\n",
       "        0.98957627],\n",
       "       [0.24555667, 0.21589978, 0.29229552, 0.1533327 , 0.13798888,\n",
       "        0.98632243],\n",
       "       [0.27530612, 0.21932935, 0.31560727, 0.16767352, 0.1296302 ,\n",
       "        0.98306859],\n",
       "       [0.23174656, 0.21388031, 0.31771115, 0.17102246, 0.12963216,\n",
       "        0.98242773],\n",
       "       [0.18818701, 0.20843127, 0.31981503, 0.1743714 , 0.12963412,\n",
       "        0.98178686],\n",
       "       [0.14462746, 0.20298222, 0.32191891, 0.17772034, 0.12963608,\n",
       "        0.981146  ],\n",
       "       [0.16559061, 0.20759269, 0.37590033, 0.16918408, 0.13588561,\n",
       "        0.96521338],\n",
       "       [0.18655375, 0.21220316, 0.42988176, 0.16064782, 0.14213514,\n",
       "        0.94928075],\n",
       "       [0.2075169 , 0.21681363, 0.48386319, 0.15211155, 0.14838467,\n",
       "        0.93334813],\n",
       "       [0.20632937, 0.24717245, 0.42137913, 0.18968911, 0.14398879,\n",
       "        0.90607485],\n",
       "       [0.20514183, 0.27753127, 0.35889507, 0.22726666, 0.13959291,\n",
       "        0.87880157],\n",
       "       [0.2039543 , 0.30789009, 0.29641102, 0.26484421, 0.13519703,\n",
       "        0.85152829],\n",
       "       [0.20786336, 0.22819714, 0.32575861, 0.27218252, 0.13010266,\n",
       "        0.69883512],\n",
       "       [0.21177242, 0.14850418, 0.3551062 , 0.27952084, 0.12500829,\n",
       "        0.54614194],\n",
       "       [0.21568148, 0.06881122, 0.38445379, 0.28685915, 0.11991392,\n",
       "        0.39344876],\n",
       "       [0.22288331, 0.09770397, 0.42716844, 0.28832255, 0.12230807,\n",
       "        0.42398951],\n",
       "       [0.23008515, 0.12659671, 0.46988309, 0.28978595, 0.12470222,\n",
       "        0.45453025],\n",
       "       [0.23728698, 0.15548945, 0.51259773, 0.29124936, 0.12709637,\n",
       "        0.485071  ],\n",
       "       [0.20492341, 0.1311615 , 0.48510399, 0.2922712 , 0.12651557,\n",
       "        0.48614039],\n",
       "       [0.17255984, 0.10683355, 0.45761024, 0.29329304, 0.12593477,\n",
       "        0.48720978],\n",
       "       [0.14019627, 0.08250559, 0.4301165 , 0.29431489, 0.12535397,\n",
       "        0.48827917],\n",
       "       [0.1424773 , 0.10285422, 0.44797618, 0.28115588, 0.12948101,\n",
       "        0.44754524],\n",
       "       [0.14475833, 0.12320284, 0.46583586, 0.26799687, 0.13360805,\n",
       "        0.40681132],\n",
       "       [0.14703936, 0.14355146, 0.48369555, 0.25483787, 0.13773509,\n",
       "        0.36607739],\n",
       "       [0.15047162, 0.16464487, 0.48776269, 0.26641758, 0.13516539,\n",
       "        0.24555977],\n",
       "       [0.15390388, 0.18573829, 0.49182984, 0.2779973 , 0.13259569,\n",
       "        0.12504215],\n",
       "       [0.15733614, 0.2068317 , 0.49589698, 0.28957702, 0.13002598,\n",
       "        0.00452454],\n",
       "       [0.14472048, 0.21745956, 0.47630548, 0.3036436 , 0.12116525,\n",
       "        0.00301636],\n",
       "       [0.13210482, 0.22808742, 0.45671398, 0.31771019, 0.11230451,\n",
       "        0.00150818],\n",
       "       [0.11948917, 0.23871528, 0.43712248, 0.33177677, 0.10344378,\n",
       "        0.        ],\n",
       "       [0.11305686, 0.2493644 , 0.4036701 , 0.31584481, 0.09560508,\n",
       "        0.00669863],\n",
       "       [0.10662455, 0.26001352, 0.37021773, 0.29991284, 0.08776639,\n",
       "        0.01339726],\n",
       "       [0.10019224, 0.27066264, 0.33676536, 0.28398088, 0.0799277 ,\n",
       "        0.02009588],\n",
       "       [0.11716762, 0.27534652, 0.31046496, 0.28570662, 0.07496154,\n",
       "        0.02583005],\n",
       "       [0.13414299, 0.28003041, 0.28416457, 0.28743236, 0.06999538,\n",
       "        0.03156421],\n",
       "       [0.15111837, 0.2847143 , 0.25786417, 0.2891581 , 0.06502922,\n",
       "        0.03729837],\n",
       "       [0.11509853, 0.33179719, 0.29836928, 0.42623956, 0.06405273,\n",
       "        0.04557007],\n",
       "       [0.07907869, 0.37888008, 0.33887439, 0.56332102, 0.06307623,\n",
       "        0.05384177],\n",
       "       [0.04305886, 0.42596297, 0.3793795 , 0.70040248, 0.06209974,\n",
       "        0.06211347],\n",
       "       [0.12850161, 0.42978373, 0.40598475, 0.70070225, 0.05904764,\n",
       "        0.06930811],\n",
       "       [0.21394436, 0.43360448, 0.43259   , 0.70100203, 0.05599554,\n",
       "        0.07650274],\n",
       "       [0.2993871 , 0.43742523, 0.45919525, 0.70130181, 0.05294344,\n",
       "        0.08369737],\n",
       "       [0.35577117, 0.47608186, 0.46826847, 0.69781819, 0.052095  ,\n",
       "        0.08259275],\n",
       "       [0.41215523, 0.5147385 , 0.47734168, 0.69433458, 0.05124657,\n",
       "        0.08148813],\n",
       "       [0.46853929, 0.55339513, 0.4864149 , 0.69085096, 0.05039813,\n",
       "        0.08038351],\n",
       "       [0.62096219, 0.58363857, 0.59803869, 0.65473261, 0.04965534,\n",
       "        0.08262284],\n",
       "       [0.7733851 , 0.613882  , 0.70966248, 0.61861427, 0.04891254,\n",
       "        0.08486218],\n",
       "       [0.925808  , 0.64412543, 0.82128627, 0.58249592, 0.04816974,\n",
       "        0.08710152],\n",
       "       [0.85725335, 0.66694505, 0.87774568, 0.58278676, 0.03638912,\n",
       "        0.07960687],\n",
       "       [0.78869869, 0.68976468, 0.93420509, 0.58307761, 0.0246085 ,\n",
       "        0.07211223],\n",
       "       [0.72014404, 0.7125843 , 0.9906645 , 0.58336845, 0.01282788,\n",
       "        0.06461758],\n",
       "       [0.81342936, 0.77357499, 0.99377633, 0.5692078 , 0.34188526,\n",
       "        0.12583268],\n",
       "       [0.90671468, 0.83456568, 0.99688817, 0.55504714, 0.67094263,\n",
       "        0.18704778],\n",
       "       [1.        , 0.89555637, 1.        , 0.54088649, 1.        ,\n",
       "        0.24826288],\n",
       "       [0.95619092, 0.93037091, 0.92133861, 0.52386224, 0.67256257,\n",
       "        0.24647661],\n",
       "       [0.91238183, 0.96518546, 0.84267722, 0.50683799, 0.34512515,\n",
       "        0.24469035],\n",
       "       [0.86857275, 1.        , 0.76401584, 0.48981373, 0.01768772,\n",
       "        0.24290409],\n",
       "       [0.90491222, 0.97987772, 0.77356422, 0.52393003, 0.02004869,\n",
       "        0.25724904],\n",
       "       [0.9412517 , 0.95975545, 0.78311261, 0.55804632, 0.02240965,\n",
       "        0.27159399],\n",
       "       [0.97759118, 0.93963317, 0.792661  , 0.59216262, 0.02477061,\n",
       "        0.28593894],\n",
       "       [0.88129339, 0.91384617, 0.73560682, 0.56296047, 0.02358691,\n",
       "        0.26377616],\n",
       "       [0.78499559, 0.88805917, 0.67855264, 0.53375832, 0.02240321,\n",
       "        0.24161338],\n",
       "       [0.6886978 , 0.86227217, 0.62149846, 0.50455617, 0.02121951,\n",
       "        0.21945059],\n",
       "       [0.68449732, 0.83454953, 0.56489702, 0.50634891, 0.0225267 ,\n",
       "        0.2246311 ],\n",
       "       [0.68029684, 0.80682688, 0.50829558, 0.50814165, 0.0238339 ,\n",
       "        0.22981161],\n",
       "       [0.67609636, 0.77910423, 0.45169414, 0.50993438, 0.02514109,\n",
       "        0.23499212],\n",
       "       [0.65528794, 0.79479123, 0.42624755, 0.49497649, 0.02847034,\n",
       "        0.23061177],\n",
       "       [0.63447952, 0.81047822, 0.40080097, 0.48001859, 0.03179959,\n",
       "        0.22623143],\n",
       "       [0.61367111, 0.82616521, 0.37535438, 0.46506069, 0.03512884,\n",
       "        0.22185109],\n",
       "       [0.6217773 , 0.8227804 , 0.35914083, 0.43333214, 0.03171177,\n",
       "        0.25978379],\n",
       "       [0.6298835 , 0.81939559, 0.34292729, 0.40160358, 0.0282947 ,\n",
       "        0.29771649],\n",
       "       [0.63798969, 0.81601078, 0.32671374, 0.36987503, 0.02487763,\n",
       "        0.33564919],\n",
       "       [0.59896858, 0.74490984, 0.32436665, 0.37715165, 0.02422461,\n",
       "        0.32305364],\n",
       "       [0.55994747, 0.6738089 , 0.32201957, 0.38442827, 0.02357158,\n",
       "        0.3104581 ],\n",
       "       [0.52092636, 0.60270796, 0.31967248, 0.39170489, 0.02291856,\n",
       "        0.29786256],\n",
       "       [0.48180425, 0.60358444, 0.30112959, 0.38800949, 0.01935771,\n",
       "        0.29786623],\n",
       "       [0.44268214, 0.60446092, 0.2825867 , 0.38431409, 0.01579686,\n",
       "        0.2978699 ],\n",
       "       [0.40356003, 0.6053374 , 0.26404381, 0.3806187 , 0.01223601,\n",
       "        0.29787357],\n",
       "       [0.34288554, 0.53068168, 0.19263329, 0.38398401, 0.01504364,\n",
       "        0.31001136],\n",
       "       [0.28221105, 0.45602596, 0.12122277, 0.38734933, 0.01785126,\n",
       "        0.32214915],\n",
       "       [0.22153656, 0.38137024, 0.04981225, 0.39071464, 0.02065888,\n",
       "        0.33428694],\n",
       "       [0.18722612, 0.36031608, 0.14423836, 0.40738137, 0.01743264,\n",
       "        0.35954238],\n",
       "       [0.15291569, 0.33926193, 0.23866447, 0.42404809, 0.01420641,\n",
       "        0.38479782],\n",
       "       [0.11860525, 0.31820777, 0.33309058, 0.44071482, 0.01098017,\n",
       "        0.41005326],\n",
       "       [0.07907017, 0.21213851, 0.27344218, 0.45845126, 0.00732012,\n",
       "        0.40651941],\n",
       "       [0.03953508, 0.10606926, 0.21379378, 0.4761877 , 0.00366006,\n",
       "        0.40298556],\n",
       "       [0.        , 0.        , 0.15414538, 0.49392414, 0.        ,\n",
       "        0.39945171]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 6) (11, 6) (11, 6) (87,) (11,) (11,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(6, activation='softplus', input_shape=(6,)),\n",
    "    Dense(32, activation='softplus'),\n",
    "    Dense(12, activation='softplus'),\n",
    "    Dense(1, activation='softplus'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87 samples, validate on 11 samples\n",
      "Epoch 1/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 729.1453 - val_loss: 686.9617\n",
      "Epoch 2/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 729.1331 - val_loss: 686.9492\n",
      "Epoch 3/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 729.1203 - val_loss: 686.9351\n",
      "Epoch 4/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 729.1063 - val_loss: 686.9195\n",
      "Epoch 5/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 729.0905 - val_loss: 686.9019\n",
      "Epoch 6/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 729.0729 - val_loss: 686.8822\n",
      "Epoch 7/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 729.0530 - val_loss: 686.8600\n",
      "Epoch 8/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 729.0307 - val_loss: 686.8353\n",
      "Epoch 9/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 729.0060 - val_loss: 686.8080\n",
      "Epoch 10/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.9786 - val_loss: 686.7778\n",
      "Epoch 11/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 728.9484 - val_loss: 686.7447\n",
      "Epoch 12/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 728.9154 - val_loss: 686.7087\n",
      "Epoch 13/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 728.8795 - val_loss: 686.6699\n",
      "Epoch 14/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.8408 - val_loss: 686.6283\n",
      "Epoch 15/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 728.7992 - val_loss: 686.5838\n",
      "Epoch 16/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.7549 - val_loss: 686.5367\n",
      "Epoch 17/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 728.7079 - val_loss: 686.4869\n",
      "Epoch 18/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.6586 - val_loss: 686.4348\n",
      "Epoch 19/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 728.6067 - val_loss: 686.3803\n",
      "Epoch 20/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.5526 - val_loss: 686.3237\n",
      "Epoch 21/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 728.4963 - val_loss: 686.2650\n",
      "Epoch 22/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.4382 - val_loss: 686.2046\n",
      "Epoch 23/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.3782 - val_loss: 686.1425\n",
      "Epoch 24/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 728.3167 - val_loss: 686.0789\n",
      "Epoch 25/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.2536 - val_loss: 686.0138\n",
      "Epoch 26/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.1892 - val_loss: 685.9476\n",
      "Epoch 27/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.1235 - val_loss: 685.8801\n",
      "Epoch 28/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 728.0569 - val_loss: 685.8118\n",
      "Epoch 29/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 727.9892 - val_loss: 685.7424\n",
      "Epoch 30/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.9206 - val_loss: 685.6724\n",
      "Epoch 31/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.8513 - val_loss: 685.6016\n",
      "Epoch 32/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.7813 - val_loss: 685.5300\n",
      "Epoch 33/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.7108 - val_loss: 685.4580\n",
      "Epoch 34/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.6395 - val_loss: 685.3854\n",
      "Epoch 35/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 727.5678 - val_loss: 685.3123\n",
      "Epoch 36/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.4957 - val_loss: 685.2386\n",
      "Epoch 37/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.4231 - val_loss: 685.1646\n",
      "Epoch 38/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.3502 - val_loss: 685.0903\n",
      "Epoch 39/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.2768 - val_loss: 685.0156\n",
      "Epoch 40/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.2032 - val_loss: 684.9406\n",
      "Epoch 41/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 727.1292 - val_loss: 684.8652\n",
      "Epoch 42/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 727.0551 - val_loss: 684.7896\n",
      "Epoch 43/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 726.9806 - val_loss: 684.7137\n",
      "Epoch 44/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 726.9059 - val_loss: 684.6375\n",
      "Epoch 45/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 726.8309 - val_loss: 684.5610\n",
      "Epoch 46/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 726.7557 - val_loss: 684.4843\n",
      "Epoch 47/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 726.6803 - val_loss: 684.4072\n",
      "Epoch 48/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 726.6046 - val_loss: 684.3300\n",
      "Epoch 49/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 726.5287 - val_loss: 684.2524\n",
      "Epoch 50/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 726.4526 - val_loss: 684.1746\n",
      "Epoch 51/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 726.3761 - val_loss: 684.0965\n",
      "Epoch 52/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 726.2996 - val_loss: 684.0181\n",
      "Epoch 53/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 726.2227 - val_loss: 683.9395\n",
      "Epoch 54/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 726.1456 - val_loss: 683.8606\n",
      "Epoch 55/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 726.0683 - val_loss: 683.7815\n",
      "Epoch 56/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 725.9907 - val_loss: 683.7020\n",
      "Epoch 57/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 725.9130 - val_loss: 683.6223\n",
      "Epoch 58/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 725.8350 - val_loss: 683.5423\n",
      "Epoch 59/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 725.7567 - val_loss: 683.4619\n",
      "Epoch 60/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 725.6782 - val_loss: 683.3814\n",
      "Epoch 61/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 725.5994 - val_loss: 683.3005\n",
      "Epoch 62/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 725.5203 - val_loss: 683.2193\n",
      "Epoch 63/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 725.4412 - val_loss: 683.1378\n",
      "Epoch 64/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 725.3615 - val_loss: 683.0560\n",
      "Epoch 65/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 725.2817 - val_loss: 682.9739\n",
      "Epoch 66/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 725.2016 - val_loss: 682.8914\n",
      "Epoch 67/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 725.1212 - val_loss: 682.8085\n",
      "Epoch 68/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 725.0405 - val_loss: 682.7254\n",
      "Epoch 69/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 724.9595 - val_loss: 682.6418\n",
      "Epoch 70/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 724.8782 - val_loss: 682.5580\n",
      "Epoch 71/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 724.7966 - val_loss: 682.4737\n",
      "Epoch 72/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 724.7145 - val_loss: 682.3890\n",
      "Epoch 73/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 724.6323 - val_loss: 682.3039\n",
      "Epoch 74/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 724.5496 - val_loss: 682.2184\n",
      "Epoch 75/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 724.4666 - val_loss: 682.1324\n",
      "Epoch 76/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 724.3832 - val_loss: 682.0461\n",
      "Epoch 77/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 724.2995 - val_loss: 681.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 724.2153 - val_loss: 681.8720\n",
      "Epoch 79/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 724.1307 - val_loss: 681.7842\n",
      "Epoch 80/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 724.0457 - val_loss: 681.6960\n",
      "Epoch 81/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 723.9603 - val_loss: 681.6074\n",
      "Epoch 82/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 723.8746 - val_loss: 681.5181\n",
      "Epoch 83/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 723.7883 - val_loss: 681.4283\n",
      "Epoch 84/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 723.7015 - val_loss: 681.3381\n",
      "Epoch 85/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 723.6142 - val_loss: 681.2473\n",
      "Epoch 86/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 723.5266 - val_loss: 681.1558\n",
      "Epoch 87/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 723.4384 - val_loss: 681.0639\n",
      "Epoch 88/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 723.3498 - val_loss: 680.9713\n",
      "Epoch 89/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 723.2606 - val_loss: 680.8782\n",
      "Epoch 90/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 723.1708 - val_loss: 680.7844\n",
      "Epoch 91/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 723.0806 - val_loss: 680.6900\n",
      "Epoch 92/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 722.9897 - val_loss: 680.5950\n",
      "Epoch 93/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 722.8984 - val_loss: 680.4993\n",
      "Epoch 94/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 722.8065 - val_loss: 680.4030\n",
      "Epoch 95/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 722.7139 - val_loss: 680.3059\n",
      "Epoch 96/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 722.6207 - val_loss: 680.2082\n",
      "Epoch 97/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 722.5270 - val_loss: 680.1097\n",
      "Epoch 98/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 722.4327 - val_loss: 680.0105\n",
      "Epoch 99/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 722.3375 - val_loss: 679.9106\n",
      "Epoch 100/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 722.2418 - val_loss: 679.8099\n",
      "Epoch 101/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 722.1455 - val_loss: 679.7084\n",
      "Epoch 102/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 722.0484 - val_loss: 679.6061\n",
      "Epoch 103/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 721.9507 - val_loss: 679.5031\n",
      "Epoch 104/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 721.8522 - val_loss: 679.3992\n",
      "Epoch 105/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 721.7530 - val_loss: 679.2944\n",
      "Epoch 106/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 721.6530 - val_loss: 679.1887\n",
      "Epoch 107/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 721.5522 - val_loss: 679.0823\n",
      "Epoch 108/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 721.4509 - val_loss: 678.9749\n",
      "Epoch 109/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 721.3486 - val_loss: 678.8666\n",
      "Epoch 110/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 721.2455 - val_loss: 678.7574\n",
      "Epoch 111/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 721.1417 - val_loss: 678.6474\n",
      "Epoch 112/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 721.0371 - val_loss: 678.5364\n",
      "Epoch 113/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 720.9316 - val_loss: 678.4244\n",
      "Epoch 114/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 720.8253 - val_loss: 678.3116\n",
      "Epoch 115/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 720.7183 - val_loss: 678.1977\n",
      "Epoch 116/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 720.6103 - val_loss: 678.0829\n",
      "Epoch 117/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 720.5015 - val_loss: 677.9670\n",
      "Epoch 118/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 720.3918 - val_loss: 677.8503\n",
      "Epoch 119/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 720.2812 - val_loss: 677.7324\n",
      "Epoch 120/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 720.1698 - val_loss: 677.6136\n",
      "Epoch 121/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 720.0575 - val_loss: 677.4937\n",
      "Epoch 122/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 719.9442 - val_loss: 677.3728\n",
      "Epoch 123/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 719.8300 - val_loss: 677.2507\n",
      "Epoch 124/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 719.7149 - val_loss: 677.1277\n",
      "Epoch 125/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 719.5989 - val_loss: 677.0035\n",
      "Epoch 126/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 719.4819 - val_loss: 676.8783\n",
      "Epoch 127/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 719.3640 - val_loss: 676.7519\n",
      "Epoch 128/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 719.2450 - val_loss: 676.6244\n",
      "Epoch 129/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 719.1251 - val_loss: 676.4957\n",
      "Epoch 130/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 719.0043 - val_loss: 676.3660\n",
      "Epoch 131/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 718.8824 - val_loss: 676.2350\n",
      "Epoch 132/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 718.7594 - val_loss: 676.1030\n",
      "Epoch 133/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 718.6354 - val_loss: 675.9697\n",
      "Epoch 134/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 718.5106 - val_loss: 675.8351\n",
      "Epoch 135/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 718.3845 - val_loss: 675.6993\n",
      "Epoch 136/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 718.2574 - val_loss: 675.5624\n",
      "Epoch 137/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 718.1293 - val_loss: 675.4243\n",
      "Epoch 138/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 718.0001 - val_loss: 675.2847\n",
      "Epoch 139/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.8697 - val_loss: 675.1440\n",
      "Epoch 140/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.7383 - val_loss: 675.0020\n",
      "Epoch 141/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.6058 - val_loss: 674.8588\n",
      "Epoch 142/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 717.4721 - val_loss: 674.7142\n",
      "Epoch 143/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 717.3373 - val_loss: 674.5683\n",
      "Epoch 144/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.2014 - val_loss: 674.4210\n",
      "Epoch 145/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 717.0643 - val_loss: 674.2725\n",
      "Epoch 146/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 716.9261 - val_loss: 674.1225\n",
      "Epoch 147/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 716.7866 - val_loss: 673.9711\n",
      "Epoch 148/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 716.6459 - val_loss: 673.8184\n",
      "Epoch 149/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 716.5042 - val_loss: 673.6643\n",
      "Epoch 150/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 716.3611 - val_loss: 673.5087\n",
      "Epoch 151/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 716.2168 - val_loss: 673.3518\n",
      "Epoch 152/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 716.0713 - val_loss: 673.1934\n",
      "Epoch 153/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 715.9245 - val_loss: 673.0336\n",
      "Epoch 154/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 715.7765 - val_loss: 672.8722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 715.6271 - val_loss: 672.7094\n",
      "Epoch 156/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 715.4765 - val_loss: 672.5450\n",
      "Epoch 157/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 715.3246 - val_loss: 672.3792\n",
      "Epoch 158/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 715.1714 - val_loss: 672.2117\n",
      "Epoch 159/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 715.0169 - val_loss: 672.0428\n",
      "Epoch 160/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 714.8610 - val_loss: 671.8723\n",
      "Epoch 161/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 714.7037 - val_loss: 671.7003\n",
      "Epoch 162/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 714.5451 - val_loss: 671.5266\n",
      "Epoch 163/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 714.3851 - val_loss: 671.3513\n",
      "Epoch 164/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 714.2237 - val_loss: 671.1744\n",
      "Epoch 165/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 714.0609 - val_loss: 670.9959\n",
      "Epoch 166/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 713.8967 - val_loss: 670.8157\n",
      "Epoch 167/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 713.7311 - val_loss: 670.6339\n",
      "Epoch 168/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 713.5640 - val_loss: 670.4504\n",
      "Epoch 169/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 713.3956 - val_loss: 670.2652\n",
      "Epoch 170/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 713.2256 - val_loss: 670.0782\n",
      "Epoch 171/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 713.0541 - val_loss: 669.8895\n",
      "Epoch 172/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 712.8812 - val_loss: 669.6992\n",
      "Epoch 173/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.7067 - val_loss: 669.5070\n",
      "Epoch 174/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.5306 - val_loss: 669.3130\n",
      "Epoch 175/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 712.3531 - val_loss: 669.1172\n",
      "Epoch 176/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 712.1740 - val_loss: 668.9197\n",
      "Epoch 177/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 711.9933 - val_loss: 668.7202\n",
      "Epoch 178/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 711.8111 - val_loss: 668.5189\n",
      "Epoch 179/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 711.6271 - val_loss: 668.3158\n",
      "Epoch 180/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 711.4417 - val_loss: 668.1106\n",
      "Epoch 181/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 711.2546 - val_loss: 667.9036\n",
      "Epoch 182/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 711.0658 - val_loss: 667.6947\n",
      "Epoch 183/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 710.8753 - val_loss: 667.4836\n",
      "Epoch 184/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 710.6831 - val_loss: 667.2709\n",
      "Epoch 185/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 710.4892 - val_loss: 667.0558\n",
      "Epoch 186/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 710.2936 - val_loss: 666.8389\n",
      "Epoch 187/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 710.0962 - val_loss: 666.6199\n",
      "Epoch 188/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 709.8970 - val_loss: 666.3987\n",
      "Epoch 189/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 709.6959 - val_loss: 666.1754\n",
      "Epoch 190/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 709.4931 - val_loss: 665.9500\n",
      "Epoch 191/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 709.2885 - val_loss: 665.7224\n",
      "Epoch 192/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 709.0819 - val_loss: 665.4926\n",
      "Epoch 193/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 708.8735 - val_loss: 665.2606\n",
      "Epoch 194/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 708.6631 - val_loss: 665.0263\n",
      "Epoch 195/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 708.4508 - val_loss: 664.7898\n",
      "Epoch 196/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 708.2366 - val_loss: 664.5509\n",
      "Epoch 197/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 708.0204 - val_loss: 664.3098\n",
      "Epoch 198/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 707.8022 - val_loss: 664.0662\n",
      "Epoch 199/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 707.5819 - val_loss: 663.8203\n",
      "Epoch 200/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 707.3596 - val_loss: 663.5718\n",
      "Epoch 201/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 707.1352 - val_loss: 663.3211\n",
      "Epoch 202/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 706.9087 - val_loss: 663.0679\n",
      "Epoch 203/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 706.6801 - val_loss: 662.8121\n",
      "Epoch 204/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 706.4495 - val_loss: 662.5539\n",
      "Epoch 205/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 706.2166 - val_loss: 662.2932\n",
      "Epoch 206/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 705.9817 - val_loss: 662.0299\n",
      "Epoch 207/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 705.7445 - val_loss: 661.7641\n",
      "Epoch 208/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 705.5052 - val_loss: 661.4957\n",
      "Epoch 209/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 705.2635 - val_loss: 661.2246\n",
      "Epoch 210/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 705.0198 - val_loss: 660.9510\n",
      "Epoch 211/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 704.7737 - val_loss: 660.6747\n",
      "Epoch 212/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 704.5255 - val_loss: 660.3957\n",
      "Epoch 213/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 704.2749 - val_loss: 660.1142\n",
      "Epoch 214/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 704.0221 - val_loss: 659.8299\n",
      "Epoch 215/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 703.7670 - val_loss: 659.5430\n",
      "Epoch 216/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 703.5096 - val_loss: 659.2533\n",
      "Epoch 217/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 703.2499 - val_loss: 658.9609\n",
      "Epoch 218/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 702.9879 - val_loss: 658.6658\n",
      "Epoch 219/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 702.7235 - val_loss: 658.3680\n",
      "Epoch 220/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 702.4568 - val_loss: 658.0674\n",
      "Epoch 221/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 702.1879 - val_loss: 657.7640\n",
      "Epoch 222/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 701.9164 - val_loss: 657.4578\n",
      "Epoch 223/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 701.6426 - val_loss: 657.1489\n",
      "Epoch 224/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 701.3664 - val_loss: 656.8372\n",
      "Epoch 225/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 701.0879 - val_loss: 656.5227\n",
      "Epoch 226/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 700.8070 - val_loss: 656.2053\n",
      "Epoch 227/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 700.5236 - val_loss: 655.8851\n",
      "Epoch 228/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 700.2379 - val_loss: 655.5621\n",
      "Epoch 229/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 699.9496 - val_loss: 655.2362\n",
      "Epoch 230/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 699.6591 - val_loss: 654.9074\n",
      "Epoch 231/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 699.3659 - val_loss: 654.5757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 699.0704 - val_loss: 654.2412\n",
      "Epoch 233/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 698.7724 - val_loss: 653.9037\n",
      "Epoch 234/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 698.4719 - val_loss: 653.5633\n",
      "Epoch 235/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 698.1689 - val_loss: 653.2200\n",
      "Epoch 236/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 697.8634 - val_loss: 652.8737\n",
      "Epoch 237/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 697.5554 - val_loss: 652.5244\n",
      "Epoch 238/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 697.2449 - val_loss: 652.1721\n",
      "Epoch 239/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 696.9318 - val_loss: 651.8169\n",
      "Epoch 240/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 696.6161 - val_loss: 651.4586\n",
      "Epoch 241/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 696.2980 - val_loss: 651.0973\n",
      "Epoch 242/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 695.9772 - val_loss: 650.7329\n",
      "Epoch 243/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 695.6538 - val_loss: 650.3655\n",
      "Epoch 244/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 695.3279 - val_loss: 649.9949\n",
      "Epoch 245/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 694.9993 - val_loss: 649.6213\n",
      "Epoch 246/1500\n",
      "87/87 [==============================] - 0s 230us/step - loss: 694.6680 - val_loss: 649.2446\n",
      "Epoch 247/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 694.3342 - val_loss: 648.8647\n",
      "Epoch 248/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 693.9977 - val_loss: 648.4817\n",
      "Epoch 249/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 693.6585 - val_loss: 648.0955\n",
      "Epoch 250/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 693.3167 - val_loss: 647.7061\n",
      "Epoch 251/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 692.9720 - val_loss: 647.3135\n",
      "Epoch 252/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 692.6247 - val_loss: 646.9176\n",
      "Epoch 253/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 692.2747 - val_loss: 646.5186\n",
      "Epoch 254/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 691.9219 - val_loss: 646.1161\n",
      "Epoch 255/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 691.5663 - val_loss: 645.7106\n",
      "Epoch 256/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 691.2079 - val_loss: 645.3016\n",
      "Epoch 257/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 690.8469 - val_loss: 644.8893\n",
      "Epoch 258/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 690.4829 - val_loss: 644.4737\n",
      "Epoch 259/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 690.1161 - val_loss: 644.0547\n",
      "Epoch 260/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 689.7465 - val_loss: 643.6324\n",
      "Epoch 261/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 689.3740 - val_loss: 643.2067\n",
      "Epoch 262/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 688.9987 - val_loss: 642.7774\n",
      "Epoch 263/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 688.6205 - val_loss: 642.3449\n",
      "Epoch 264/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 688.2393 - val_loss: 641.9088\n",
      "Epoch 265/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 687.8552 - val_loss: 641.4693\n",
      "Epoch 266/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 687.4683 - val_loss: 641.0263\n",
      "Epoch 267/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 687.0784 - val_loss: 640.5797\n",
      "Epoch 268/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 686.6854 - val_loss: 640.1296\n",
      "Epoch 269/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 686.2895 - val_loss: 639.6759\n",
      "Epoch 270/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 685.8906 - val_loss: 639.2187\n",
      "Epoch 271/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 685.4887 - val_loss: 638.7578\n",
      "Epoch 272/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 685.0838 - val_loss: 638.2934\n",
      "Epoch 273/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 684.6758 - val_loss: 637.8253\n",
      "Epoch 274/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 684.2648 - val_loss: 637.3535\n",
      "Epoch 275/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 683.8506 - val_loss: 636.8781\n",
      "Epoch 276/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 683.4333 - val_loss: 636.3989\n",
      "Epoch 277/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 683.0130 - val_loss: 635.9161\n",
      "Epoch 278/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 682.5896 - val_loss: 635.4295\n",
      "Epoch 279/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 682.1628 - val_loss: 634.9391\n",
      "Epoch 280/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 681.7331 - val_loss: 634.4449\n",
      "Epoch 281/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 681.3001 - val_loss: 633.9470\n",
      "Epoch 282/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 680.8640 - val_loss: 633.4453\n",
      "Epoch 283/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 680.4246 - val_loss: 632.9395\n",
      "Epoch 284/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 679.9819 - val_loss: 632.4301\n",
      "Epoch 285/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 679.5360 - val_loss: 631.9166\n",
      "Epoch 286/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 679.0869 - val_loss: 631.3994\n",
      "Epoch 287/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 678.6344 - val_loss: 630.8781\n",
      "Epoch 288/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 678.1788 - val_loss: 630.3529\n",
      "Epoch 289/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 677.7196 - val_loss: 629.8238\n",
      "Epoch 290/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 677.2573 - val_loss: 629.2906\n",
      "Epoch 291/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 676.7915 - val_loss: 628.7534\n",
      "Epoch 292/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 676.3224 - val_loss: 628.2122\n",
      "Epoch 293/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 675.8498 - val_loss: 627.6668\n",
      "Epoch 294/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 675.3740 - val_loss: 627.1175\n",
      "Epoch 295/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 674.8946 - val_loss: 626.5640\n",
      "Epoch 296/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 674.4117 - val_loss: 626.0064\n",
      "Epoch 297/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 673.9255 - val_loss: 625.4446\n",
      "Epoch 298/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 673.4358 - val_loss: 624.8787\n",
      "Epoch 299/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 672.9426 - val_loss: 624.3086\n",
      "Epoch 300/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 672.4458 - val_loss: 623.7342\n",
      "Epoch 301/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 671.9456 - val_loss: 623.1556\n",
      "Epoch 302/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 671.4417 - val_loss: 622.5727\n",
      "Epoch 303/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 670.9343 - val_loss: 621.9856\n",
      "Epoch 304/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 670.4233 - val_loss: 621.3942\n",
      "Epoch 305/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 669.9088 - val_loss: 620.7985\n",
      "Epoch 306/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 669.3906 - val_loss: 620.1984\n",
      "Epoch 307/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 668.8688 - val_loss: 619.5939\n",
      "Epoch 308/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 668.3432 - val_loss: 618.9850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 667.8141 - val_loss: 618.3717\n",
      "Epoch 310/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 667.2812 - val_loss: 617.7540\n",
      "Epoch 311/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 666.7446 - val_loss: 617.1318\n",
      "Epoch 312/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 666.2043 - val_loss: 616.5051\n",
      "Epoch 313/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 665.6603 - val_loss: 615.8740\n",
      "Epoch 314/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 665.1124 - val_loss: 615.2383\n",
      "Epoch 315/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 664.5609 - val_loss: 614.5980\n",
      "Epoch 316/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 664.0054 - val_loss: 613.9532\n",
      "Epoch 317/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 663.4461 - val_loss: 613.3037\n",
      "Epoch 318/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 662.8831 - val_loss: 612.6496\n",
      "Epoch 319/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 662.3161 - val_loss: 611.9910\n",
      "Epoch 320/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 661.7453 - val_loss: 611.3276\n",
      "Epoch 321/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 661.1705 - val_loss: 610.6595\n",
      "Epoch 322/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 660.5919 - val_loss: 609.9868\n",
      "Epoch 323/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 660.0093 - val_loss: 609.3091\n",
      "Epoch 324/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 659.4229 - val_loss: 608.6269\n",
      "Epoch 325/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 658.8323 - val_loss: 607.9398\n",
      "Epoch 326/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 658.2379 - val_loss: 607.2480\n",
      "Epoch 327/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 657.6393 - val_loss: 606.5512\n",
      "Epoch 328/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 657.0368 - val_loss: 605.8496\n",
      "Epoch 329/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 656.4302 - val_loss: 605.1432\n",
      "Epoch 330/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 655.8195 - val_loss: 604.4319\n",
      "Epoch 331/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 655.2048 - val_loss: 603.7156\n",
      "Epoch 332/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 654.5860 - val_loss: 602.9943\n",
      "Epoch 333/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 653.9630 - val_loss: 602.2681\n",
      "Epoch 334/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 653.3359 - val_loss: 601.5369\n",
      "Epoch 335/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 652.7045 - val_loss: 600.8007\n",
      "Epoch 336/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 652.0690 - val_loss: 600.0594\n",
      "Epoch 337/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 651.4294 - val_loss: 599.3130\n",
      "Epoch 338/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 650.7855 - val_loss: 598.5616\n",
      "Epoch 339/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 650.1374 - val_loss: 597.8051\n",
      "Epoch 340/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 649.4850 - val_loss: 597.0433\n",
      "Epoch 341/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 648.8283 - val_loss: 596.2764\n",
      "Epoch 342/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 648.1674 - val_loss: 595.5044\n",
      "Epoch 343/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 647.5020 - val_loss: 594.7271\n",
      "Epoch 344/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 646.8323 - val_loss: 593.9446\n",
      "Epoch 345/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 646.1584 - val_loss: 593.1567\n",
      "Epoch 346/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 645.4799 - val_loss: 592.3637\n",
      "Epoch 347/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 644.7971 - val_loss: 591.5654\n",
      "Epoch 348/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 644.1099 - val_loss: 590.7615\n",
      "Epoch 349/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 643.4182 - val_loss: 589.9524\n",
      "Epoch 350/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 642.7222 - val_loss: 589.1379\n",
      "Epoch 351/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 642.0215 - val_loss: 588.3180\n",
      "Epoch 352/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 641.3165 - val_loss: 587.4926\n",
      "Epoch 353/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 640.6068 - val_loss: 586.6619\n",
      "Epoch 354/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 639.8926 - val_loss: 585.8255\n",
      "Epoch 355/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 639.1740 - val_loss: 584.9836\n",
      "Epoch 356/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 638.4506 - val_loss: 584.1363\n",
      "Epoch 357/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 637.7227 - val_loss: 583.2833\n",
      "Epoch 358/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 636.9901 - val_loss: 582.4248\n",
      "Epoch 359/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 636.2530 - val_loss: 581.5607\n",
      "Epoch 360/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 635.5111 - val_loss: 580.6909\n",
      "Epoch 361/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 634.7646 - val_loss: 579.8154\n",
      "Epoch 362/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 634.0133 - val_loss: 578.9343\n",
      "Epoch 363/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 633.2573 - val_loss: 578.0474\n",
      "Epoch 364/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 632.4966 - val_loss: 577.1548\n",
      "Epoch 365/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 631.7312 - val_loss: 576.2564\n",
      "Epoch 366/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 630.9609 - val_loss: 575.3522\n",
      "Epoch 367/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 630.1858 - val_loss: 574.4423\n",
      "Epoch 368/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 629.4059 - val_loss: 573.5264\n",
      "Epoch 369/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 628.6212 - val_loss: 572.6047\n",
      "Epoch 370/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 627.8315 - val_loss: 571.6770\n",
      "Epoch 371/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 627.0370 - val_loss: 570.7435\n",
      "Epoch 372/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 626.2375 - val_loss: 569.8040\n",
      "Epoch 373/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 625.4332 - val_loss: 568.8585\n",
      "Epoch 374/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 624.6238 - val_loss: 567.9070\n",
      "Epoch 375/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 623.8096 - val_loss: 566.9495\n",
      "Epoch 376/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 622.9902 - val_loss: 565.9859\n",
      "Epoch 377/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 622.1658 - val_loss: 565.0162\n",
      "Epoch 378/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 621.3365 - val_loss: 564.0405\n",
      "Epoch 379/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 620.5021 - val_loss: 563.0586\n",
      "Epoch 380/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 619.6626 - val_loss: 562.0706\n",
      "Epoch 381/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 618.8179 - val_loss: 561.0763\n",
      "Epoch 382/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 617.9683 - val_loss: 560.0758\n",
      "Epoch 383/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 617.1133 - val_loss: 559.0691\n",
      "Epoch 384/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 616.2532 - val_loss: 558.0562\n",
      "Epoch 385/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 615.3880 - val_loss: 557.0369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 386/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 614.5176 - val_loss: 556.0113\n",
      "Epoch 387/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 613.6418 - val_loss: 554.9793\n",
      "Epoch 388/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 612.7608 - val_loss: 553.9410\n",
      "Epoch 389/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 611.8746 - val_loss: 552.8962\n",
      "Epoch 390/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 610.9831 - val_loss: 551.8451\n",
      "Epoch 391/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 610.0862 - val_loss: 550.7874\n",
      "Epoch 392/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 609.1839 - val_loss: 549.7233\n",
      "Epoch 393/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 608.2764 - val_loss: 548.6527\n",
      "Epoch 394/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 607.3634 - val_loss: 547.5756\n",
      "Epoch 395/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 606.4451 - val_loss: 546.4918\n",
      "Epoch 396/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 605.5212 - val_loss: 545.4015\n",
      "Epoch 397/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 604.5920 - val_loss: 544.3046\n",
      "Epoch 398/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 603.6572 - val_loss: 543.2009\n",
      "Epoch 399/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 602.7169 - val_loss: 542.0906\n",
      "Epoch 400/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 601.7712 - val_loss: 540.9736\n",
      "Epoch 401/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 600.8198 - val_loss: 539.8499\n",
      "Epoch 402/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 599.8630 - val_loss: 538.7195\n",
      "Epoch 403/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 598.9005 - val_loss: 537.5822\n",
      "Epoch 404/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 597.9324 - val_loss: 536.4380\n",
      "Epoch 405/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 596.9587 - val_loss: 535.2872\n",
      "Epoch 406/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 595.9793 - val_loss: 534.1293\n",
      "Epoch 407/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 594.9942 - val_loss: 532.9646\n",
      "Epoch 408/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 594.0035 - val_loss: 531.7930\n",
      "Epoch 409/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 593.0070 - val_loss: 530.6143\n",
      "Epoch 410/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 592.0048 - val_loss: 529.4288\n",
      "Epoch 411/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 590.9967 - val_loss: 528.2361\n",
      "Epoch 412/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 589.9829 - val_loss: 527.0365\n",
      "Epoch 413/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 588.9633 - val_loss: 525.8298\n",
      "Epoch 414/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 587.9378 - val_loss: 524.6160\n",
      "Epoch 415/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 586.9065 - val_loss: 523.3950\n",
      "Epoch 416/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 585.8693 - val_loss: 522.1669\n",
      "Epoch 417/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 584.8262 - val_loss: 520.9316\n",
      "Epoch 418/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 583.7771 - val_loss: 519.6890\n",
      "Epoch 419/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 582.7220 - val_loss: 518.4393\n",
      "Epoch 420/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 581.6611 - val_loss: 517.1823\n",
      "Epoch 421/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 580.5941 - val_loss: 515.9179\n",
      "Epoch 422/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 579.5211 - val_loss: 514.6462\n",
      "Epoch 423/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 578.4420 - val_loss: 513.3672\n",
      "Epoch 424/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 577.3568 - val_loss: 512.0807\n",
      "Epoch 425/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 576.2656 - val_loss: 510.7869\n",
      "Epoch 426/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 575.1682 - val_loss: 509.4857\n",
      "Epoch 427/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 574.0646 - val_loss: 508.1768\n",
      "Epoch 428/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 572.9550 - val_loss: 506.8605\n",
      "Epoch 429/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 571.8391 - val_loss: 505.5367\n",
      "Epoch 430/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 570.7170 - val_loss: 504.2054\n",
      "Epoch 431/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 569.5886 - val_loss: 502.8664\n",
      "Epoch 432/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 568.4540 - val_loss: 501.5198\n",
      "Epoch 433/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 567.3131 - val_loss: 500.1655\n",
      "Epoch 434/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 566.1659 - val_loss: 498.8035\n",
      "Epoch 435/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 565.0124 - val_loss: 497.4339\n",
      "Epoch 436/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 563.8525 - val_loss: 496.0565\n",
      "Epoch 437/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 562.6862 - val_loss: 494.6712\n",
      "Epoch 438/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 561.5135 - val_loss: 493.2782\n",
      "Epoch 439/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 560.3344 - val_loss: 491.8774\n",
      "Epoch 440/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 559.1487 - val_loss: 490.4687\n",
      "Epoch 441/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 557.9567 - val_loss: 489.0521\n",
      "Epoch 442/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 556.7581 - val_loss: 487.6276\n",
      "Epoch 443/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 555.5529 - val_loss: 486.1950\n",
      "Epoch 444/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 554.3413 - val_loss: 484.7546\n",
      "Epoch 445/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 553.1231 - val_loss: 483.3061\n",
      "Epoch 446/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 551.8982 - val_loss: 481.8495\n",
      "Epoch 447/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 550.6667 - val_loss: 480.3849\n",
      "Epoch 448/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 549.4286 - val_loss: 478.9121\n",
      "Epoch 449/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 548.1837 - val_loss: 477.4312\n",
      "Epoch 450/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 546.9323 - val_loss: 475.9422\n",
      "Epoch 451/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 545.6740 - val_loss: 474.4449\n",
      "Epoch 452/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 544.4089 - val_loss: 472.9394\n",
      "Epoch 453/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 543.1371 - val_loss: 471.4257\n",
      "Epoch 454/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 541.8586 - val_loss: 469.9036\n",
      "Epoch 455/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 540.5731 - val_loss: 468.3731\n",
      "Epoch 456/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 539.2808 - val_loss: 466.8344\n",
      "Epoch 457/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 537.9818 - val_loss: 465.2872\n",
      "Epoch 458/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 536.6757 - val_loss: 463.7317\n",
      "Epoch 459/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 535.3627 - val_loss: 462.1676\n",
      "Epoch 460/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 534.0427 - val_loss: 460.5951\n",
      "Epoch 461/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 532.7158 - val_loss: 459.0140\n",
      "Epoch 462/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 531.3818 - val_loss: 457.4244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 463/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 530.0408 - val_loss: 455.8263\n",
      "Epoch 464/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 528.6927 - val_loss: 454.2195\n",
      "Epoch 465/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 527.3376 - val_loss: 452.6040\n",
      "Epoch 466/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 525.9754 - val_loss: 450.9799\n",
      "Epoch 467/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 524.6061 - val_loss: 449.3470\n",
      "Epoch 468/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 523.2294 - val_loss: 447.7054\n",
      "Epoch 469/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 521.8457 - val_loss: 446.0551\n",
      "Epoch 470/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 520.4548 - val_loss: 444.3960\n",
      "Epoch 471/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 519.0566 - val_loss: 442.7279\n",
      "Epoch 472/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 517.6511 - val_loss: 441.0511\n",
      "Epoch 473/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 516.2383 - val_loss: 439.3652\n",
      "Epoch 474/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 514.8184 - val_loss: 437.6705\n",
      "Epoch 475/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 513.3909 - val_loss: 435.9668\n",
      "Epoch 476/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 511.9561 - val_loss: 434.2540\n",
      "Epoch 477/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 510.5139 - val_loss: 432.5323\n",
      "Epoch 478/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 509.0643 - val_loss: 430.8015\n",
      "Epoch 479/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 507.6073 - val_loss: 429.0616\n",
      "Epoch 480/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 506.1427 - val_loss: 427.3125\n",
      "Epoch 481/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 504.6707 - val_loss: 425.5542\n",
      "Epoch 482/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 503.1911 - val_loss: 423.7868\n",
      "Epoch 483/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 501.7039 - val_loss: 422.0101\n",
      "Epoch 484/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 500.2092 - val_loss: 420.2242\n",
      "Epoch 485/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 498.7068 - val_loss: 418.4289\n",
      "Epoch 486/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 497.1969 - val_loss: 416.6243\n",
      "Epoch 487/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 495.6792 - val_loss: 414.8103\n",
      "Epoch 488/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 494.1539 - val_loss: 412.9869\n",
      "Epoch 489/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 492.6209 - val_loss: 411.1541\n",
      "Epoch 490/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 491.0800 - val_loss: 409.3118\n",
      "Epoch 491/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 489.5315 - val_loss: 407.4600\n",
      "Epoch 492/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 487.9751 - val_loss: 405.5986\n",
      "Epoch 493/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 486.4108 - val_loss: 403.7277\n",
      "Epoch 494/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 484.8387 - val_loss: 401.8472\n",
      "Epoch 495/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 483.2588 - val_loss: 399.9570\n",
      "Epoch 496/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 481.6709 - val_loss: 398.0571\n",
      "Epoch 497/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 480.0751 - val_loss: 396.1475\n",
      "Epoch 498/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 478.4713 - val_loss: 394.2282\n",
      "Epoch 499/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 476.8595 - val_loss: 392.2990\n",
      "Epoch 500/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 475.2397 - val_loss: 390.3601\n",
      "Epoch 501/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 473.6118 - val_loss: 388.4113\n",
      "Epoch 502/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 471.9759 - val_loss: 386.4526\n",
      "Epoch 503/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 470.3319 - val_loss: 384.4840\n",
      "Epoch 504/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 468.6797 - val_loss: 382.5055\n",
      "Epoch 505/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 467.0193 - val_loss: 380.5169\n",
      "Epoch 506/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 465.3508 - val_loss: 378.5183\n",
      "Epoch 507/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 463.6740 - val_loss: 376.5097\n",
      "Epoch 508/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 461.9890 - val_loss: 374.4909\n",
      "Epoch 509/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 460.2957 - val_loss: 372.4619\n",
      "Epoch 510/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 458.5942 - val_loss: 370.4229\n",
      "Epoch 511/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 456.8842 - val_loss: 368.3736\n",
      "Epoch 512/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 455.1660 - val_loss: 366.3141\n",
      "Epoch 513/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 453.4393 - val_loss: 364.2444\n",
      "Epoch 514/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 451.7042 - val_loss: 362.1642\n",
      "Epoch 515/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 449.9606 - val_loss: 360.0737\n",
      "Epoch 516/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 448.2086 - val_loss: 357.9728\n",
      "Epoch 517/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 446.4482 - val_loss: 355.8616\n",
      "Epoch 518/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 444.6791 - val_loss: 353.7398\n",
      "Epoch 519/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 442.9015 - val_loss: 351.6076\n",
      "Epoch 520/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 441.1153 - val_loss: 349.4648\n",
      "Epoch 521/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 439.3205 - val_loss: 347.3115\n",
      "Epoch 522/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 437.5169 - val_loss: 345.1476\n",
      "Epoch 523/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 435.7048 - val_loss: 342.9730\n",
      "Epoch 524/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 433.8839 - val_loss: 340.7877\n",
      "Epoch 525/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 432.0543 - val_loss: 338.5917\n",
      "Epoch 526/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 430.2159 - val_loss: 336.3850\n",
      "Epoch 527/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 428.3687 - val_loss: 334.1674\n",
      "Epoch 528/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 426.5128 - val_loss: 331.9391\n",
      "Epoch 529/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 424.6480 - val_loss: 329.6999\n",
      "Epoch 530/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 422.7743 - val_loss: 327.4498\n",
      "Epoch 531/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 420.8916 - val_loss: 325.1888\n",
      "Epoch 532/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 419.0000 - val_loss: 322.9167\n",
      "Epoch 533/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 417.0995 - val_loss: 320.6338\n",
      "Epoch 534/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 415.1899 - val_loss: 318.3396\n",
      "Epoch 535/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 413.2713 - val_loss: 316.0345\n",
      "Epoch 536/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 411.3436 - val_loss: 313.7181\n",
      "Epoch 537/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 409.4069 - val_loss: 311.3907\n",
      "Epoch 538/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 407.4610 - val_loss: 309.0520\n",
      "Epoch 539/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 405.5060 - val_loss: 306.7021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 540/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 403.5418 - val_loss: 304.3409\n",
      "Epoch 541/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 401.5683 - val_loss: 301.9684\n",
      "Epoch 542/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 399.5858 - val_loss: 299.5847\n",
      "Epoch 543/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 397.5938 - val_loss: 297.1894\n",
      "Epoch 544/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 395.5926 - val_loss: 294.7827\n",
      "Epoch 545/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 393.5819 - val_loss: 292.3646\n",
      "Epoch 546/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 391.5620 - val_loss: 289.9350\n",
      "Epoch 547/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 389.5327 - val_loss: 287.4938\n",
      "Epoch 548/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 387.4940 - val_loss: 285.0410\n",
      "Epoch 549/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 385.4458 - val_loss: 282.5767\n",
      "Epoch 550/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 383.3881 - val_loss: 280.1007\n",
      "Epoch 551/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 381.3209 - val_loss: 277.6130\n",
      "Epoch 552/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 379.2441 - val_loss: 275.1135\n",
      "Epoch 553/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 377.1578 - val_loss: 272.6023\n",
      "Epoch 554/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 375.0619 - val_loss: 270.0792\n",
      "Epoch 555/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 372.9563 - val_loss: 267.5443\n",
      "Epoch 556/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 370.8411 - val_loss: 264.9976\n",
      "Epoch 557/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 368.7161 - val_loss: 262.4389\n",
      "Epoch 558/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 366.5814 - val_loss: 259.8682\n",
      "Epoch 559/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 364.4370 - val_loss: 257.2855\n",
      "Epoch 560/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 362.2828 - val_loss: 254.6908\n",
      "Epoch 561/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 360.1187 - val_loss: 252.0840\n",
      "Epoch 562/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 357.9448 - val_loss: 249.4651\n",
      "Epoch 563/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 355.7610 - val_loss: 246.8340\n",
      "Epoch 564/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 353.5674 - val_loss: 244.1907\n",
      "Epoch 565/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 351.3637 - val_loss: 241.5352\n",
      "Epoch 566/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 349.1501 - val_loss: 238.8674\n",
      "Epoch 567/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 346.9264 - val_loss: 236.1873\n",
      "Epoch 568/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 344.6927 - val_loss: 233.4948\n",
      "Epoch 569/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 342.4490 - val_loss: 230.7899\n",
      "Epoch 570/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 340.1951 - val_loss: 228.0726\n",
      "Epoch 571/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 337.9311 - val_loss: 225.3428\n",
      "Epoch 572/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 335.6570 - val_loss: 222.6005\n",
      "Epoch 573/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 333.3726 - val_loss: 219.8456\n",
      "Epoch 574/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 331.0780 - val_loss: 217.0782\n",
      "Epoch 575/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 328.7732 - val_loss: 214.2981\n",
      "Epoch 576/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 326.4580 - val_loss: 211.5054\n",
      "Epoch 577/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 324.1325 - val_loss: 208.6999\n",
      "Epoch 578/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 321.7968 - val_loss: 205.8817\n",
      "Epoch 579/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 319.4505 - val_loss: 203.0506\n",
      "Epoch 580/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 317.0939 - val_loss: 200.2068\n",
      "Epoch 581/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 314.7268 - val_loss: 197.3501\n",
      "Epoch 582/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 312.3492 - val_loss: 194.4805\n",
      "Epoch 583/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 309.9612 - val_loss: 191.5979\n",
      "Epoch 584/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 307.5625 - val_loss: 188.7023\n",
      "Epoch 585/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 305.1533 - val_loss: 185.7937\n",
      "Epoch 586/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 302.7335 - val_loss: 182.8720\n",
      "Epoch 587/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 300.3031 - val_loss: 180.0342\n",
      "Epoch 588/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 297.8619 - val_loss: 177.7842\n",
      "Epoch 589/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 295.4100 - val_loss: 175.5243\n",
      "Epoch 590/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 293.0164 - val_loss: 173.3211\n",
      "Epoch 591/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 290.6941 - val_loss: 171.1019\n",
      "Epoch 592/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 288.3548 - val_loss: 169.1209\n",
      "Epoch 593/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 285.9994 - val_loss: 167.5584\n",
      "Epoch 594/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 283.6281 - val_loss: 165.9859\n",
      "Epoch 595/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 281.2878 - val_loss: 164.4504\n",
      "Epoch 596/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 279.0354 - val_loss: 163.4829\n",
      "Epoch 597/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 276.7631 - val_loss: 162.5695\n",
      "Epoch 598/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 274.4716 - val_loss: 161.6492\n",
      "Epoch 599/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 272.1617 - val_loss: 160.8224\n",
      "Epoch 600/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 269.8340 - val_loss: 160.5144\n",
      "Epoch 601/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 267.4892 - val_loss: 160.7063\n",
      "Epoch 602/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 265.1279 - val_loss: 161.0205\n",
      "Epoch 603/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 262.7507 - val_loss: 161.3375\n",
      "Epoch 604/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 260.3580 - val_loss: 161.6571\n",
      "Epoch 605/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 257.9503 - val_loss: 161.9794\n",
      "Epoch 606/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 255.5897 - val_loss: 162.2894\n",
      "Epoch 607/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 253.3522 - val_loss: 162.5859\n",
      "Epoch 608/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 251.2020 - val_loss: 162.8872\n",
      "Epoch 609/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 249.0280 - val_loss: 163.1931\n",
      "Epoch 610/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 246.8313 - val_loss: 163.5034\n",
      "Epoch 611/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 244.6128 - val_loss: 163.8179\n",
      "Epoch 612/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 242.3737 - val_loss: 164.1364\n",
      "Epoch 613/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 240.1146 - val_loss: 164.4588\n",
      "Epoch 614/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 237.8413 - val_loss: 164.7697\n",
      "Epoch 615/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 235.6908 - val_loss: 165.0852\n",
      "Epoch 616/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 233.5178 - val_loss: 165.4053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 231.3234 - val_loss: 165.7295\n",
      "Epoch 618/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 229.1086 - val_loss: 166.0579\n",
      "Epoch 619/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 226.8741 - val_loss: 166.3902\n",
      "Epoch 620/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 224.6208 - val_loss: 166.7263\n",
      "Epoch 621/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 222.3496 - val_loss: 167.0660\n",
      "Epoch 622/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 220.0610 - val_loss: 167.4091\n",
      "Epoch 623/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 217.8534 - val_loss: 167.7006\n",
      "Epoch 624/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 216.0207 - val_loss: 167.9791\n",
      "Epoch 625/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 214.2500 - val_loss: 168.2650\n",
      "Epoch 626/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 212.4494 - val_loss: 168.5580\n",
      "Epoch 627/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 210.6200 - val_loss: 168.8579\n",
      "Epoch 628/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 208.7784 - val_loss: 169.1457\n",
      "Epoch 629/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 207.0311 - val_loss: 169.4408\n",
      "Epoch 630/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 205.2558 - val_loss: 169.7428\n",
      "Epoch 631/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 203.4538 - val_loss: 170.0516\n",
      "Epoch 632/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 201.6262 - val_loss: 170.3669\n",
      "Epoch 633/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 199.7742 - val_loss: 170.6885\n",
      "Epoch 634/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 197.8990 - val_loss: 171.0162\n",
      "Epoch 635/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 196.0017 - val_loss: 171.3497\n",
      "Epoch 636/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 194.0831 - val_loss: 171.6887\n",
      "Epoch 637/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 192.1444 - val_loss: 172.0332\n",
      "Epoch 638/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 190.1865 - val_loss: 172.3827\n",
      "Epoch 639/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 188.2102 - val_loss: 172.7371\n",
      "Epoch 640/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 186.2163 - val_loss: 173.0963\n",
      "Epoch 641/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 184.2056 - val_loss: 173.4599\n",
      "Epoch 642/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 182.1788 - val_loss: 173.8278\n",
      "Epoch 643/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 180.1478 - val_loss: 174.1765\n",
      "Epoch 644/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 178.2422 - val_loss: 174.5309\n",
      "Epoch 645/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 176.3171 - val_loss: 174.8906\n",
      "Epoch 646/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 174.3734 - val_loss: 175.2553\n",
      "Epoch 647/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 172.4736 - val_loss: 175.5981\n",
      "Epoch 648/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 170.6478 - val_loss: 175.9472\n",
      "Epoch 649/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 168.8012 - val_loss: 176.3024\n",
      "Epoch 650/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 166.9344 - val_loss: 176.6635\n",
      "Epoch 651/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 165.0558 - val_loss: 177.0138\n",
      "Epoch 652/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 163.3080 - val_loss: 177.3705\n",
      "Epoch 653/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 161.5382 - val_loss: 177.7334\n",
      "Epoch 654/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 159.7473 - val_loss: 178.1022\n",
      "Epoch 655/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 157.9363 - val_loss: 178.4767\n",
      "Epoch 656/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 156.1062 - val_loss: 178.8566\n",
      "Epoch 657/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 154.2579 - val_loss: 179.2416\n",
      "Epoch 658/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 152.4400 - val_loss: 179.5923\n",
      "Epoch 659/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 150.8899 - val_loss: 179.9363\n",
      "Epoch 660/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 149.4426 - val_loss: 180.5010\n",
      "Epoch 661/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 148.0329 - val_loss: 181.3381\n",
      "Epoch 662/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 146.6104 - val_loss: 182.1611\n",
      "Epoch 663/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 145.2633 - val_loss: 183.0013\n",
      "Epoch 664/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 143.8899 - val_loss: 183.8579\n",
      "Epoch 665/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 142.4912 - val_loss: 185.2808\n",
      "Epoch 666/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 141.0684 - val_loss: 186.8138\n",
      "Epoch 667/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 139.6226 - val_loss: 188.3710\n",
      "Epoch 668/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 138.1547 - val_loss: 189.9517\n",
      "Epoch 669/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 136.6659 - val_loss: 191.5546\n",
      "Epoch 670/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 135.1572 - val_loss: 193.1787\n",
      "Epoch 671/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 133.6884 - val_loss: 194.7249\n",
      "Epoch 672/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 132.3071 - val_loss: 196.2209\n",
      "Epoch 673/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 131.0801 - val_loss: 197.5513\n",
      "Epoch 674/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 130.0526 - val_loss: 198.9164\n",
      "Epoch 675/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 129.0000 - val_loss: 200.3152\n",
      "Epoch 676/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 127.9231 - val_loss: 201.7468\n",
      "Epoch 677/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 126.8821 - val_loss: 203.1249\n",
      "Epoch 678/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 125.8842 - val_loss: 204.5368\n",
      "Epoch 679/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 124.8632 - val_loss: 205.9817\n",
      "Epoch 680/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 123.8527 - val_loss: 207.3441\n",
      "Epoch 681/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 123.0107 - val_loss: 208.5089\n",
      "Epoch 682/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 122.3085 - val_loss: 209.7101\n",
      "Epoch 683/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 121.5848 - val_loss: 210.9474\n",
      "Epoch 684/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 120.8877 - val_loss: 212.1116\n",
      "Epoch 685/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 120.2348 - val_loss: 213.3116\n",
      "Epoch 686/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 119.5860 - val_loss: 214.4381\n",
      "Epoch 687/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 118.9993 - val_loss: 215.6007\n",
      "Epoch 688/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 118.3940 - val_loss: 216.7987\n",
      "Epoch 689/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 117.8001 - val_loss: 217.8463\n",
      "Epoch 690/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 117.2703 - val_loss: 218.9298\n",
      "Epoch 691/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 116.7731 - val_loss: 219.8607\n",
      "Epoch 692/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 116.3101 - val_loss: 220.8318\n",
      "Epoch 693/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 115.8367 - val_loss: 221.6468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 694/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 115.4574 - val_loss: 222.3177\n",
      "Epoch 695/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 115.1213 - val_loss: 223.0360\n",
      "Epoch 696/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 114.7754 - val_loss: 223.7980\n",
      "Epoch 697/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 114.4590 - val_loss: 224.2131\n",
      "Epoch 698/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 114.2523 - val_loss: 224.5021\n",
      "Epoch 699/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 114.0937 - val_loss: 224.3997\n",
      "Epoch 700/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 113.9817 - val_loss: 224.3303\n",
      "Epoch 701/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 113.8733 - val_loss: 224.2888\n",
      "Epoch 702/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 113.7670 - val_loss: 224.2725\n",
      "Epoch 703/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 113.6619 - val_loss: 224.2793\n",
      "Epoch 704/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 113.5572 - val_loss: 224.3083\n",
      "Epoch 705/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 113.4525 - val_loss: 224.3586\n",
      "Epoch 706/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 113.3472 - val_loss: 224.4299\n",
      "Epoch 707/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 113.2411 - val_loss: 224.5224\n",
      "Epoch 708/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 113.1339 - val_loss: 224.6357\n",
      "Epoch 709/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 113.0253 - val_loss: 224.7703\n",
      "Epoch 710/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 112.9152 - val_loss: 224.9265\n",
      "Epoch 711/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.8033 - val_loss: 225.1047\n",
      "Epoch 712/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.7019 - val_loss: 224.9632\n",
      "Epoch 713/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.6069 - val_loss: 224.8340\n",
      "Epoch 714/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 112.5115 - val_loss: 224.7171\n",
      "Epoch 715/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.4156 - val_loss: 224.6128\n",
      "Epoch 716/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.3190 - val_loss: 224.5212\n",
      "Epoch 717/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 112.2223 - val_loss: 224.0182\n",
      "Epoch 718/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.1283 - val_loss: 223.9582\n",
      "Epoch 719/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 112.0299 - val_loss: 223.9115\n",
      "Epoch 720/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.9304 - val_loss: 223.8783\n",
      "Epoch 721/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.8299 - val_loss: 223.8593\n",
      "Epoch 722/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 111.7324 - val_loss: 223.3237\n",
      "Epoch 723/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 111.6334 - val_loss: 223.3406\n",
      "Epoch 724/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.5301 - val_loss: 223.3727\n",
      "Epoch 725/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.4255 - val_loss: 223.4206\n",
      "Epoch 726/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 111.3287 - val_loss: 222.8506\n",
      "Epoch 727/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.2234 - val_loss: 222.9392\n",
      "Epoch 728/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 111.1156 - val_loss: 223.0452\n",
      "Epoch 729/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 111.0156 - val_loss: 222.4466\n",
      "Epoch 730/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 110.9096 - val_loss: 222.5973\n",
      "Epoch 731/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 110.7982 - val_loss: 222.7670\n",
      "Epoch 732/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 110.6989 - val_loss: 222.1356\n",
      "Epoch 733/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 110.5879 - val_loss: 222.3547\n",
      "Epoch 734/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 110.4773 - val_loss: 221.7043\n",
      "Epoch 735/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 110.3753 - val_loss: 221.9756\n",
      "Epoch 736/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 110.2577 - val_loss: 222.2687\n",
      "Epoch 737/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 110.1563 - val_loss: 221.5798\n",
      "Epoch 738/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 110.0406 - val_loss: 221.9305\n",
      "Epoch 739/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 109.9310 - val_loss: 221.2214\n",
      "Epoch 740/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 109.8211 - val_loss: 221.6325\n",
      "Epoch 741/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 109.7049 - val_loss: 220.9030\n",
      "Epoch 742/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 109.5988 - val_loss: 221.3774\n",
      "Epoch 743/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 109.4781 - val_loss: 220.6281\n",
      "Epoch 744/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 109.3756 - val_loss: 222.3255\n",
      "Epoch 745/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 109.2817 - val_loss: 221.5791\n",
      "Epoch 746/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 109.1513 - val_loss: 220.8357\n",
      "Epoch 747/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 109.0218 - val_loss: 220.0947\n",
      "Epoch 748/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.9291 - val_loss: 221.9234\n",
      "Epoch 749/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 108.8255 - val_loss: 221.1904\n",
      "Epoch 750/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 108.6960 - val_loss: 220.4606\n",
      "Epoch 751/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.5671 - val_loss: 219.7339\n",
      "Epoch 752/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.4670 - val_loss: 221.6696\n",
      "Epoch 753/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.3771 - val_loss: 219.7479\n",
      "Epoch 754/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 108.2305 - val_loss: 220.3888\n",
      "Epoch 755/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 108.1215 - val_loss: 219.7159\n",
      "Epoch 756/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 107.9965 - val_loss: 220.4249\n",
      "Epoch 757/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 107.8965 - val_loss: 219.7356\n",
      "Epoch 758/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 107.7690 - val_loss: 219.0496\n",
      "Epoch 759/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 107.6607 - val_loss: 219.8779\n",
      "Epoch 760/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 107.5434 - val_loss: 219.1779\n",
      "Epoch 761/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 107.4173 - val_loss: 220.0727\n",
      "Epoch 762/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 107.3166 - val_loss: 219.3617\n",
      "Epoch 763/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 107.1877 - val_loss: 218.6579\n",
      "Epoch 764/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 107.0716 - val_loss: 219.6732\n",
      "Epoch 765/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.9605 - val_loss: 218.9619\n",
      "Epoch 766/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 106.8314 - val_loss: 218.2599\n",
      "Epoch 767/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 106.7197 - val_loss: 219.3905\n",
      "Epoch 768/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.6038 - val_loss: 218.6848\n",
      "Epoch 769/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.4745 - val_loss: 217.9904\n",
      "Epoch 770/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 106.3610 - val_loss: 219.2279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 771/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 106.2463 - val_loss: 218.5337\n",
      "Epoch 772/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 106.1170 - val_loss: 217.8526\n",
      "Epoch 773/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 105.9953 - val_loss: 219.1861\n",
      "Epoch 774/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 105.8932 - val_loss: 216.7933\n",
      "Epoch 775/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 105.8366 - val_loss: 220.0922\n",
      "Epoch 776/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 105.8161 - val_loss: 216.2694\n",
      "Epoch 777/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 105.6701 - val_loss: 218.8805\n",
      "Epoch 778/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 105.4776 - val_loss: 217.1263\n",
      "Epoch 779/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 105.3367 - val_loss: 218.2923\n",
      "Epoch 780/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 105.2477 - val_loss: 217.9487\n",
      "Epoch 781/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 105.1360 - val_loss: 217.6057\n",
      "Epoch 782/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 105.0237 - val_loss: 217.2631\n",
      "Epoch 783/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 104.9110 - val_loss: 216.9211\n",
      "Epoch 784/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 104.7976 - val_loss: 216.5799\n",
      "Epoch 785/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 104.6886 - val_loss: 217.2239\n",
      "Epoch 786/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 104.5823 - val_loss: 216.8712\n",
      "Epoch 787/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 104.4665 - val_loss: 216.5207\n",
      "Epoch 788/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 104.3503 - val_loss: 216.1726\n",
      "Epoch 789/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 104.2437 - val_loss: 216.9739\n",
      "Epoch 790/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 104.1301 - val_loss: 216.6151\n",
      "Epoch 791/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 104.0117 - val_loss: 216.2605\n",
      "Epoch 792/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.8932 - val_loss: 215.9106\n",
      "Epoch 793/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 103.7821 - val_loss: 216.8850\n",
      "Epoch 794/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.6750 - val_loss: 214.2757\n",
      "Epoch 795/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.7318 - val_loss: 217.4245\n",
      "Epoch 796/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 103.5872 - val_loss: 214.8053\n",
      "Epoch 797/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 103.3992 - val_loss: 216.1203\n",
      "Epoch 798/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 103.2405 - val_loss: 215.8491\n",
      "Epoch 799/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 103.1280 - val_loss: 215.5814\n",
      "Epoch 800/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 103.0151 - val_loss: 215.3172\n",
      "Epoch 801/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 102.9072 - val_loss: 216.0044\n",
      "Epoch 802/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.8322 - val_loss: 213.3082\n",
      "Epoch 803/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 102.8908 - val_loss: 215.9104\n",
      "Epoch 804/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.6536 - val_loss: 213.6547\n",
      "Epoch 805/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 102.5766 - val_loss: 215.9102\n",
      "Epoch 806/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 102.4930 - val_loss: 213.8743\n",
      "Epoch 807/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 102.3302 - val_loss: 214.9056\n",
      "Epoch 808/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.1883 - val_loss: 213.4292\n",
      "Epoch 809/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 102.1473 - val_loss: 214.4996\n",
      "Epoch 810/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.9846 - val_loss: 214.9663\n",
      "Epoch 811/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 101.9342 - val_loss: 212.8644\n",
      "Epoch 812/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 101.8701 - val_loss: 214.5234\n",
      "Epoch 813/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 101.7055 - val_loss: 213.0500\n",
      "Epoch 814/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.6219 - val_loss: 213.5341\n",
      "Epoch 815/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.4925 - val_loss: 214.0388\n",
      "Epoch 816/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.3912 - val_loss: 213.1494\n",
      "Epoch 817/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 101.2913 - val_loss: 213.7113\n",
      "Epoch 818/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 101.1805 - val_loss: 212.7802\n",
      "Epoch 819/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 101.0862 - val_loss: 213.4009\n",
      "Epoch 820/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.9680 - val_loss: 212.4291\n",
      "Epoch 821/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.8772 - val_loss: 213.1095\n",
      "Epoch 822/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.7540 - val_loss: 212.0983\n",
      "Epoch 823/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.6646 - val_loss: 212.8387\n",
      "Epoch 824/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 100.5387 - val_loss: 211.7903\n",
      "Epoch 825/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 100.4497 - val_loss: 213.3948\n",
      "Epoch 826/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.3868 - val_loss: 210.8642\n",
      "Epoch 827/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 100.3372 - val_loss: 213.8988\n",
      "Epoch 828/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 100.2962 - val_loss: 211.8729\n",
      "Epoch 829/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 100.0145 - val_loss: 211.7148\n",
      "Epoch 830/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.9119 - val_loss: 211.5621\n",
      "Epoch 831/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.8083 - val_loss: 211.4153\n",
      "Epoch 832/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.7036 - val_loss: 211.2748\n",
      "Epoch 833/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 99.5978 - val_loss: 211.1408\n",
      "Epoch 834/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 99.4911 - val_loss: 211.0139\n",
      "Epoch 835/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.3838 - val_loss: 210.0791\n",
      "Epoch 836/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.3348 - val_loss: 212.8091\n",
      "Epoch 837/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 99.3559 - val_loss: 210.6441\n",
      "Epoch 838/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 99.0748 - val_loss: 209.8414\n",
      "Epoch 839/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 99.0051 - val_loss: 211.7650\n",
      "Epoch 840/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.9611 - val_loss: 210.3255\n",
      "Epoch 841/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.7704 - val_loss: 209.5321\n",
      "Epoch 842/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 98.6929 - val_loss: 211.4594\n",
      "Epoch 843/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.6587 - val_loss: 210.0301\n",
      "Epoch 844/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 98.4661 - val_loss: 209.2460\n",
      "Epoch 845/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.3756 - val_loss: 211.1782\n",
      "Epoch 846/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 98.3577 - val_loss: 209.7588\n",
      "Epoch 847/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 98.1620 - val_loss: 208.9842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 848/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 98.0618 - val_loss: 210.2955\n",
      "Epoch 849/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 98.0018 - val_loss: 208.8386\n",
      "Epoch 850/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 97.8477 - val_loss: 210.1670\n",
      "Epoch 851/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 97.8064 - val_loss: 208.7067\n",
      "Epoch 852/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 97.6339 - val_loss: 208.6770\n",
      "Epoch 853/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 97.5305 - val_loss: 208.6556\n",
      "Epoch 854/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 97.4258 - val_loss: 207.8232\n",
      "Epoch 855/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 97.3726 - val_loss: 209.9764\n",
      "Epoch 856/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 97.3225 - val_loss: 208.4647\n",
      "Epoch 857/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 97.1178 - val_loss: 207.6679\n",
      "Epoch 858/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 97.0318 - val_loss: 209.1451\n",
      "Epoch 859/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 96.9648 - val_loss: 207.6215\n",
      "Epoch 860/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.8084 - val_loss: 208.3893\n",
      "Epoch 861/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.7119 - val_loss: 207.5700\n",
      "Epoch 862/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 96.5909 - val_loss: 207.6004\n",
      "Epoch 863/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.4828 - val_loss: 207.6418\n",
      "Epoch 864/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 96.3734 - val_loss: 207.6950\n",
      "Epoch 865/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 96.2686 - val_loss: 206.7890\n",
      "Epoch 866/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 96.1680 - val_loss: 207.8053\n",
      "Epoch 867/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 96.0719 - val_loss: 206.0101\n",
      "Epoch 868/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 95.9960 - val_loss: 208.4824\n",
      "Epoch 869/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 95.9369 - val_loss: 206.8864\n",
      "Epoch 870/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 95.7245 - val_loss: 206.9499\n",
      "Epoch 871/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 95.6144 - val_loss: 207.0257\n",
      "Epoch 872/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 95.5185 - val_loss: 205.3568\n",
      "Epoch 873/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 95.4520 - val_loss: 207.7366\n",
      "Epoch 874/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 95.3876 - val_loss: 206.2180\n",
      "Epoch 875/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 95.1899 - val_loss: 206.2945\n",
      "Epoch 876/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 95.0797 - val_loss: 206.3838\n",
      "Epoch 877/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.9798 - val_loss: 204.7872\n",
      "Epoch 878/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 94.9118 - val_loss: 206.5015\n",
      "Epoch 879/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.7966 - val_loss: 204.9576\n",
      "Epoch 880/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.6710 - val_loss: 205.8841\n",
      "Epoch 881/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.5528 - val_loss: 205.1683\n",
      "Epoch 882/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.4382 - val_loss: 205.3088\n",
      "Epoch 883/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 94.3251 - val_loss: 204.5669\n",
      "Epoch 884/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 94.2270 - val_loss: 205.6554\n",
      "Epoch 885/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 94.1312 - val_loss: 203.9623\n",
      "Epoch 886/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 94.0276 - val_loss: 205.0599\n",
      "Epoch 887/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 93.8945 - val_loss: 204.3151\n",
      "Epoch 888/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 93.7760 - val_loss: 204.5314\n",
      "Epoch 889/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 93.6637 - val_loss: 203.7621\n",
      "Epoch 890/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 93.5596 - val_loss: 205.0176\n",
      "Epoch 891/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 93.4683 - val_loss: 203.2227\n",
      "Epoch 892/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 93.3519 - val_loss: 204.4573\n",
      "Epoch 893/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 93.2290 - val_loss: 203.7043\n",
      "Epoch 894/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 93.0988 - val_loss: 202.9516\n",
      "Epoch 895/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 93.0046 - val_loss: 204.2837\n",
      "Epoch 896/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 92.8965 - val_loss: 203.5146\n",
      "Epoch 897/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.7649 - val_loss: 202.7477\n",
      "Epoch 898/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 92.6479 - val_loss: 204.1668\n",
      "Epoch 899/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.5687 - val_loss: 202.3129\n",
      "Epoch 900/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 92.4295 - val_loss: 203.6773\n",
      "Epoch 901/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 92.3275 - val_loss: 202.9430\n",
      "Epoch 902/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.1973 - val_loss: 202.2109\n",
      "Epoch 903/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 92.0703 - val_loss: 202.6100\n",
      "Epoch 904/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 91.9647 - val_loss: 201.8581\n",
      "Epoch 905/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 91.8387 - val_loss: 202.3140\n",
      "Epoch 906/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 91.7308 - val_loss: 201.5438\n",
      "Epoch 907/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 91.6037 - val_loss: 202.0580\n",
      "Epoch 908/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 91.4955 - val_loss: 201.2717\n",
      "Epoch 909/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 91.3654 - val_loss: 201.8450\n",
      "Epoch 910/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 91.2590 - val_loss: 201.0449\n",
      "Epoch 911/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 91.1252 - val_loss: 200.2549\n",
      "Epoch 912/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 91.0900 - val_loss: 204.1098\n",
      "Epoch 913/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 91.0372 - val_loss: 201.1078\n",
      "Epoch 914/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 90.7996 - val_loss: 200.5358\n",
      "Epoch 915/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.6773 - val_loss: 199.9635\n",
      "Epoch 916/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.6054 - val_loss: 203.0704\n",
      "Epoch 917/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.5492 - val_loss: 200.5360\n",
      "Epoch 918/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.3605 - val_loss: 200.0329\n",
      "Epoch 919/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.2424 - val_loss: 199.5285\n",
      "Epoch 920/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 90.1564 - val_loss: 202.3550\n",
      "Epoch 921/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 90.1000 - val_loss: 200.6994\n",
      "Epoch 922/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.9413 - val_loss: 200.2128\n",
      "Epoch 923/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 89.8234 - val_loss: 199.7258\n",
      "Epoch 924/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.7052 - val_loss: 199.2383\n",
      "Epoch 925/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.5867 - val_loss: 198.7507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 926/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 89.5065 - val_loss: 201.0128\n",
      "Epoch 927/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 89.4034 - val_loss: 200.3768\n",
      "Epoch 928/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.2823 - val_loss: 199.7431\n",
      "Epoch 929/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.1611 - val_loss: 199.2310\n",
      "Epoch 930/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 89.0398 - val_loss: 198.7329\n",
      "Epoch 931/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 88.9183 - val_loss: 198.2367\n",
      "Epoch 932/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.8114 - val_loss: 199.9774\n",
      "Epoch 933/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.7127 - val_loss: 199.3271\n",
      "Epoch 934/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.5888 - val_loss: 198.6887\n",
      "Epoch 935/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.4649 - val_loss: 198.1824\n",
      "Epoch 936/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.3411 - val_loss: 197.6806\n",
      "Epoch 937/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 88.2290 - val_loss: 198.4288\n",
      "Epoch 938/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 88.1120 - val_loss: 197.8465\n",
      "Epoch 939/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 87.9863 - val_loss: 197.3386\n",
      "Epoch 940/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 87.8721 - val_loss: 198.3081\n",
      "Epoch 941/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 87.7540 - val_loss: 197.6534\n",
      "Epoch 942/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 87.6266 - val_loss: 197.0964\n",
      "Epoch 943/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 87.5051 - val_loss: 198.3316\n",
      "Epoch 944/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 87.3913 - val_loss: 197.6779\n",
      "Epoch 945/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 87.2625 - val_loss: 197.0414\n",
      "Epoch 946/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 87.1339 - val_loss: 196.4687\n",
      "Epoch 947/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 87.0181 - val_loss: 197.9673\n",
      "Epoch 948/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.9042 - val_loss: 195.5579\n",
      "Epoch 949/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.8209 - val_loss: 198.2962\n",
      "Epoch 950/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 86.6948 - val_loss: 195.9634\n",
      "Epoch 951/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 86.5353 - val_loss: 197.2131\n",
      "Epoch 952/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.4214 - val_loss: 195.1146\n",
      "Epoch 953/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.3296 - val_loss: 197.6306\n",
      "Epoch 954/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 86.2178 - val_loss: 195.5349\n",
      "Epoch 955/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 86.0575 - val_loss: 196.6815\n",
      "Epoch 956/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 85.9526 - val_loss: 194.6697\n",
      "Epoch 957/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 85.8420 - val_loss: 197.1423\n",
      "Epoch 958/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 85.7513 - val_loss: 195.1789\n",
      "Epoch 959/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 85.5796 - val_loss: 196.2826\n",
      "Epoch 960/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 85.4908 - val_loss: 194.3203\n",
      "Epoch 961/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 85.3598 - val_loss: 195.4757\n",
      "Epoch 962/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 85.2328 - val_loss: 193.5808\n",
      "Epoch 963/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 85.1485 - val_loss: 196.0301\n",
      "Epoch 964/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 85.0344 - val_loss: 194.1618\n",
      "Epoch 965/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.8748 - val_loss: 195.2855\n",
      "Epoch 966/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 84.7791 - val_loss: 193.4107\n",
      "Epoch 967/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.6515 - val_loss: 194.5890\n",
      "Epoch 968/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.5256 - val_loss: 192.7105\n",
      "Epoch 969/1500\n",
      "87/87 [==============================] - 0s 437us/step - loss: 84.4336 - val_loss: 195.2261\n",
      "Epoch 970/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 84.3288 - val_loss: 193.4278\n",
      "Epoch 971/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 84.1580 - val_loss: 194.5788\n",
      "Epoch 972/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 84.0769 - val_loss: 192.7710\n",
      "Epoch 973/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 83.9309 - val_loss: 193.9783\n",
      "Epoch 974/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 83.8264 - val_loss: 192.1642\n",
      "Epoch 975/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 83.7014 - val_loss: 193.4229\n",
      "Epoch 976/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 83.5772 - val_loss: 191.6058\n",
      "Epoch 977/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 83.4741 - val_loss: 194.1876\n",
      "Epoch 978/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 83.3923 - val_loss: 191.3017\n",
      "Epoch 979/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 83.2422 - val_loss: 193.6192\n",
      "Epoch 980/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 83.1446 - val_loss: 190.9632\n",
      "Epoch 981/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 83.0159 - val_loss: 193.1218\n",
      "Epoch 982/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 82.9044 - val_loss: 190.6161\n",
      "Epoch 983/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 82.7923 - val_loss: 192.6706\n",
      "Epoch 984/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 82.6709 - val_loss: 191.2565\n",
      "Epoch 985/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 82.5309 - val_loss: 191.0530\n",
      "Epoch 986/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 82.4153 - val_loss: 190.8593\n",
      "Epoch 987/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 82.2987 - val_loss: 190.6760\n",
      "Epoch 988/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 82.1811 - val_loss: 190.5040\n",
      "Epoch 989/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 82.0625 - val_loss: 190.3443\n",
      "Epoch 990/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.9429 - val_loss: 190.1977\n",
      "Epoch 991/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 81.8224 - val_loss: 190.0652\n",
      "Epoch 992/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.7009 - val_loss: 189.9478\n",
      "Epoch 993/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.5784 - val_loss: 189.8465\n",
      "Epoch 994/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 81.4549 - val_loss: 189.7625\n",
      "Epoch 995/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.3305 - val_loss: 189.6965\n",
      "Epoch 996/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 81.2050 - val_loss: 189.6499\n",
      "Epoch 997/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 81.0786 - val_loss: 189.6235\n",
      "Epoch 998/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 80.9537 - val_loss: 187.7992\n",
      "Epoch 999/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.8642 - val_loss: 191.0309\n",
      "Epoch 1000/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.8301 - val_loss: 186.9626\n",
      "Epoch 1001/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.6647 - val_loss: 189.4099\n",
      "Epoch 1002/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 80.4978 - val_loss: 186.9352\n",
      "Epoch 1003/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.4082 - val_loss: 189.2094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1004/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.2753 - val_loss: 186.8689\n",
      "Epoch 1005/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 80.1568 - val_loss: 189.0294\n",
      "Epoch 1006/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 80.0553 - val_loss: 186.7837\n",
      "Epoch 1007/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.9083 - val_loss: 188.8658\n",
      "Epoch 1008/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 79.8372 - val_loss: 186.6905\n",
      "Epoch 1009/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.6729 - val_loss: 186.6463\n",
      "Epoch 1010/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 79.5560 - val_loss: 186.6154\n",
      "Epoch 1011/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.4377 - val_loss: 186.5986\n",
      "Epoch 1012/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.3192 - val_loss: 185.4381\n",
      "Epoch 1013/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 79.2359 - val_loss: 187.8022\n",
      "Epoch 1014/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 79.1325 - val_loss: 186.6389\n",
      "Epoch 1015/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 78.9859 - val_loss: 185.4792\n",
      "Epoch 1016/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 78.8514 - val_loss: 186.7829\n",
      "Epoch 1017/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 78.7633 - val_loss: 185.5918\n",
      "Epoch 1018/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 78.6158 - val_loss: 184.4061\n",
      "Epoch 1019/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 78.5259 - val_loss: 186.9589\n",
      "Epoch 1020/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 78.4324 - val_loss: 183.5950\n",
      "Epoch 1021/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 78.3288 - val_loss: 185.8047\n",
      "Epoch 1022/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 78.1769 - val_loss: 184.7741\n",
      "Epoch 1023/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 78.0368 - val_loss: 183.7426\n",
      "Epoch 1024/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 77.9269 - val_loss: 184.9958\n",
      "Epoch 1025/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.8174 - val_loss: 183.9291\n",
      "Epoch 1026/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.6755 - val_loss: 182.8640\n",
      "Epoch 1027/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 77.5988 - val_loss: 185.3185\n",
      "Epoch 1028/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.4950 - val_loss: 182.0793\n",
      "Epoch 1029/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.4019 - val_loss: 184.2398\n",
      "Epoch 1030/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 77.2426 - val_loss: 183.2897\n",
      "Epoch 1031/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 77.1052 - val_loss: 182.3382\n",
      "Epoch 1032/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 76.9914 - val_loss: 183.5880\n",
      "Epoch 1033/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 76.8859 - val_loss: 182.6022\n",
      "Epoch 1034/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 76.7466 - val_loss: 181.6173\n",
      "Epoch 1035/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 76.6457 - val_loss: 184.0412\n",
      "Epoch 1036/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 76.6014 - val_loss: 180.9055\n",
      "Epoch 1037/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 76.4416 - val_loss: 183.0631\n",
      "Epoch 1038/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 76.3283 - val_loss: 181.0850\n",
      "Epoch 1039/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 76.1822 - val_loss: 181.3018\n",
      "Epoch 1040/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 76.0601 - val_loss: 181.5360\n",
      "Epoch 1041/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 75.9392 - val_loss: 180.6328\n",
      "Epoch 1042/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 75.8272 - val_loss: 180.9261\n",
      "Epoch 1043/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 75.7005 - val_loss: 181.2383\n",
      "Epoch 1044/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 75.5883 - val_loss: 179.0015\n",
      "Epoch 1045/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 75.5389 - val_loss: 181.4950\n",
      "Epoch 1046/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 75.3959 - val_loss: 179.3948\n",
      "Epoch 1047/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 75.2426 - val_loss: 179.7176\n",
      "Epoch 1048/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 75.1145 - val_loss: 180.0593\n",
      "Epoch 1049/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 74.9934 - val_loss: 179.0701\n",
      "Epoch 1050/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 74.8784 - val_loss: 179.4778\n",
      "Epoch 1051/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.7520 - val_loss: 178.4656\n",
      "Epoch 1052/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 74.6382 - val_loss: 178.9422\n",
      "Epoch 1053/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.5085 - val_loss: 177.9090\n",
      "Epoch 1054/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.3995 - val_loss: 179.7945\n",
      "Epoch 1055/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 74.3224 - val_loss: 177.4251\n",
      "Epoch 1056/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 74.1579 - val_loss: 179.2288\n",
      "Epoch 1057/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 74.0707 - val_loss: 176.9626\n",
      "Epoch 1058/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 73.9169 - val_loss: 178.7109\n",
      "Epoch 1059/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 73.8230 - val_loss: 176.5224\n",
      "Epoch 1060/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 73.6761 - val_loss: 178.2323\n",
      "Epoch 1061/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 73.5783 - val_loss: 176.1044\n",
      "Epoch 1062/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 73.4349 - val_loss: 177.7876\n",
      "Epoch 1063/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 73.3361 - val_loss: 175.7084\n",
      "Epoch 1064/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 73.1929 - val_loss: 177.3731\n",
      "Epoch 1065/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 73.0959 - val_loss: 175.3341\n",
      "Epoch 1066/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 72.9500 - val_loss: 176.9862\n",
      "Epoch 1067/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 72.8573 - val_loss: 174.9812\n",
      "Epoch 1068/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 72.7074 - val_loss: 175.4191\n",
      "Epoch 1069/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 72.5959 - val_loss: 174.6062\n",
      "Epoch 1070/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 72.4668 - val_loss: 175.1119\n",
      "Epoch 1071/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 72.3569 - val_loss: 174.2817\n",
      "Epoch 1072/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 72.2246 - val_loss: 173.4601\n",
      "Epoch 1073/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 72.1194 - val_loss: 175.4435\n",
      "Epoch 1074/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 72.0049 - val_loss: 174.5980\n",
      "Epoch 1075/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 71.8705 - val_loss: 173.7646\n",
      "Epoch 1076/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 71.7365 - val_loss: 172.9430\n",
      "Epoch 1077/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 71.6093 - val_loss: 173.6633\n",
      "Epoch 1078/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 71.4910 - val_loss: 172.8298\n",
      "Epoch 1079/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 71.3555 - val_loss: 172.0112\n",
      "Epoch 1080/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 71.2356 - val_loss: 172.8661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1081/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 71.1073 - val_loss: 172.0378\n",
      "Epoch 1082/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 70.9706 - val_loss: 171.2279\n",
      "Epoch 1083/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 70.8522 - val_loss: 172.2155\n",
      "Epoch 1084/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.7197 - val_loss: 171.3982\n",
      "Epoch 1085/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 70.5819 - val_loss: 170.6025\n",
      "Epoch 1086/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 70.4589 - val_loss: 171.7164\n",
      "Epoch 1087/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 70.3285 - val_loss: 170.9154\n",
      "Epoch 1088/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.1897 - val_loss: 170.1395\n",
      "Epoch 1089/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 70.0556 - val_loss: 171.3686\n",
      "Epoch 1090/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 69.9338 - val_loss: 170.5886\n",
      "Epoch 1091/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 69.7942 - val_loss: 169.8369\n",
      "Epoch 1092/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 69.6551 - val_loss: 169.1130\n",
      "Epoch 1093/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 69.5229 - val_loss: 170.5188\n",
      "Epoch 1094/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 69.3966 - val_loss: 169.7889\n",
      "Epoch 1095/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 69.2565 - val_loss: 169.0900\n",
      "Epoch 1096/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 69.1169 - val_loss: 168.4219\n",
      "Epoch 1097/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 68.9777 - val_loss: 167.7847\n",
      "Epoch 1098/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 68.8623 - val_loss: 170.8813\n",
      "Epoch 1099/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 68.8492 - val_loss: 166.4842\n",
      "Epoch 1100/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 68.6792 - val_loss: 168.4261\n",
      "Epoch 1101/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 68.4777 - val_loss: 167.8895\n",
      "Epoch 1102/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 68.3488 - val_loss: 167.3633\n",
      "Epoch 1103/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 68.2195 - val_loss: 166.8481\n",
      "Epoch 1104/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 68.0967 - val_loss: 167.8038\n",
      "Epoch 1105/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 67.9722 - val_loss: 167.2729\n",
      "Epoch 1106/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 67.8400 - val_loss: 166.7571\n",
      "Epoch 1107/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 67.7076 - val_loss: 166.2574\n",
      "Epoch 1108/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 67.5752 - val_loss: 167.3967\n",
      "Epoch 1109/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 67.4625 - val_loss: 165.3811\n",
      "Epoch 1110/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 67.3339 - val_loss: 167.8011\n",
      "Epoch 1111/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 67.2798 - val_loss: 164.7351\n",
      "Epoch 1112/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 67.1011 - val_loss: 166.8151\n",
      "Epoch 1113/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 66.9897 - val_loss: 164.0471\n",
      "Epoch 1114/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 66.8775 - val_loss: 165.9415\n",
      "Epoch 1115/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 66.7173 - val_loss: 164.3934\n",
      "Epoch 1116/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 66.5936 - val_loss: 165.2051\n",
      "Epoch 1117/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 66.4580 - val_loss: 163.6327\n",
      "Epoch 1118/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 66.3595 - val_loss: 164.5059\n",
      "Epoch 1119/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 66.2058 - val_loss: 164.1005\n",
      "Epoch 1120/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 66.0777 - val_loss: 165.0332\n",
      "Epoch 1121/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.9979 - val_loss: 162.2330\n",
      "Epoch 1122/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 65.9005 - val_loss: 164.2353\n",
      "Epoch 1123/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.7267 - val_loss: 162.7098\n",
      "Epoch 1124/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.6031 - val_loss: 163.5988\n",
      "Epoch 1125/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 65.4689 - val_loss: 162.0534\n",
      "Epoch 1126/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.3622 - val_loss: 163.0019\n",
      "Epoch 1127/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 65.2114 - val_loss: 161.4402\n",
      "Epoch 1128/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 65.1180 - val_loss: 162.4440\n",
      "Epoch 1129/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 64.9567 - val_loss: 162.2087\n",
      "Epoch 1130/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 64.8308 - val_loss: 161.9880\n",
      "Epoch 1131/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 64.7037 - val_loss: 161.7830\n",
      "Epoch 1132/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 64.5756 - val_loss: 161.5944\n",
      "Epoch 1133/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 64.4463 - val_loss: 161.4235\n",
      "Epoch 1134/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 64.3159 - val_loss: 161.2711\n",
      "Epoch 1135/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 64.1844 - val_loss: 161.1379\n",
      "Epoch 1136/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 64.0518 - val_loss: 161.0251\n",
      "Epoch 1137/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.9199 - val_loss: 159.1840\n",
      "Epoch 1138/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.8139 - val_loss: 160.6986\n",
      "Epoch 1139/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.6610 - val_loss: 157.6908\n",
      "Epoch 1140/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.6451 - val_loss: 160.0537\n",
      "Epoch 1141/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 63.4034 - val_loss: 158.6405\n",
      "Epoch 1142/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 63.2865 - val_loss: 159.8295\n",
      "Epoch 1143/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 63.1728 - val_loss: 157.1850\n",
      "Epoch 1144/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 63.1021 - val_loss: 159.4085\n",
      "Epoch 1145/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 62.9382 - val_loss: 156.9904\n",
      "Epoch 1146/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 62.8365 - val_loss: 159.0231\n",
      "Epoch 1147/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 62.7081 - val_loss: 156.7460\n",
      "Epoch 1148/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 62.5774 - val_loss: 158.6568\n",
      "Epoch 1149/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 62.4807 - val_loss: 156.4767\n",
      "Epoch 1150/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 62.3219 - val_loss: 158.3044\n",
      "Epoch 1151/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 62.2670 - val_loss: 155.3571\n",
      "Epoch 1152/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 62.1438 - val_loss: 157.0299\n",
      "Epoch 1153/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 61.9891 - val_loss: 155.9814\n",
      "Epoch 1154/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.8468 - val_loss: 154.9336\n",
      "Epoch 1155/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 61.7678 - val_loss: 156.7070\n",
      "Epoch 1156/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 61.6700 - val_loss: 154.6925\n",
      "Epoch 1157/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 61.5147 - val_loss: 156.4259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1158/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.4685 - val_loss: 153.6951\n",
      "Epoch 1159/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.3323 - val_loss: 155.2543\n",
      "Epoch 1160/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 61.1981 - val_loss: 153.3946\n",
      "Epoch 1161/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 61.0830 - val_loss: 155.0127\n",
      "Epoch 1162/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 60.9967 - val_loss: 153.1606\n",
      "Epoch 1163/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 60.8417 - val_loss: 153.9984\n",
      "Epoch 1164/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 60.7404 - val_loss: 152.2488\n",
      "Epoch 1165/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 60.6514 - val_loss: 154.5273\n",
      "Epoch 1166/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 60.5948 - val_loss: 152.0030\n",
      "Epoch 1167/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 60.4086 - val_loss: 152.7281\n",
      "Epoch 1168/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 60.2856 - val_loss: 151.1947\n",
      "Epoch 1169/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 60.2291 - val_loss: 153.2520\n",
      "Epoch 1170/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 60.1402 - val_loss: 151.5349\n",
      "Epoch 1171/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.9554 - val_loss: 151.4766\n",
      "Epoch 1172/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.8462 - val_loss: 151.4332\n",
      "Epoch 1173/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.7397 - val_loss: 149.9291\n",
      "Epoch 1174/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.6919 - val_loss: 151.3513\n",
      "Epoch 1175/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 59.5467 - val_loss: 149.7471\n",
      "Epoch 1176/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 59.4365 - val_loss: 150.4436\n",
      "Epoch 1177/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 59.2999 - val_loss: 150.4409\n",
      "Epoch 1178/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 59.2029 - val_loss: 148.8550\n",
      "Epoch 1179/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 59.1215 - val_loss: 149.5593\n",
      "Epoch 1180/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 58.9663 - val_loss: 149.6023\n",
      "Epoch 1181/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 58.8579 - val_loss: 148.0179\n",
      "Epoch 1182/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 58.7955 - val_loss: 148.7490\n",
      "Epoch 1183/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 58.6258 - val_loss: 148.8354\n",
      "Epoch 1184/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 58.5126 - val_loss: 147.2362\n",
      "Epoch 1185/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 58.4592 - val_loss: 148.0121\n",
      "Epoch 1186/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 58.2790 - val_loss: 148.1387\n",
      "Epoch 1187/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 58.1673 - val_loss: 146.5089\n",
      "Epoch 1188/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 58.1133 - val_loss: 147.3462\n",
      "Epoch 1189/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 57.9266 - val_loss: 147.5092\n",
      "Epoch 1190/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.8225 - val_loss: 145.8335\n",
      "Epoch 1191/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 57.7585 - val_loss: 146.7484\n",
      "Epoch 1192/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 57.5731 - val_loss: 145.9316\n",
      "Epoch 1193/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 57.4747 - val_loss: 147.1669\n",
      "Epoch 1194/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.3903 - val_loss: 145.2827\n",
      "Epoch 1195/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 57.2512 - val_loss: 146.4300\n",
      "Epoch 1196/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 57.1375 - val_loss: 144.6597\n",
      "Epoch 1197/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 57.0267 - val_loss: 145.7330\n",
      "Epoch 1198/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.8878 - val_loss: 144.0595\n",
      "Epoch 1199/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.8009 - val_loss: 145.0713\n",
      "Epoch 1200/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.6407 - val_loss: 143.4798\n",
      "Epoch 1201/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 56.5738 - val_loss: 144.4410\n",
      "Epoch 1202/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 56.3957 - val_loss: 142.9188\n",
      "Epoch 1203/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 56.3453 - val_loss: 143.8394\n",
      "Epoch 1204/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.1541 - val_loss: 143.1030\n",
      "Epoch 1205/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 56.0486 - val_loss: 144.3857\n",
      "Epoch 1206/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.9757 - val_loss: 142.5509\n",
      "Epoch 1207/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 55.8147 - val_loss: 143.7952\n",
      "Epoch 1208/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.7300 - val_loss: 142.0193\n",
      "Epoch 1209/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 55.5796 - val_loss: 143.2370\n",
      "Epoch 1210/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 55.4879 - val_loss: 140.8562\n",
      "Epoch 1211/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 55.4074 - val_loss: 141.6762\n",
      "Epoch 1212/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 55.2088 - val_loss: 141.9753\n",
      "Epoch 1213/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.1061 - val_loss: 140.4695\n",
      "Epoch 1214/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 55.0233 - val_loss: 141.5019\n",
      "Epoch 1215/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 54.8687 - val_loss: 140.0069\n",
      "Epoch 1216/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.7842 - val_loss: 141.0491\n",
      "Epoch 1217/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.6418 - val_loss: 138.9175\n",
      "Epoch 1218/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.6126 - val_loss: 140.4541\n",
      "Epoch 1219/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.4118 - val_loss: 138.5055\n",
      "Epoch 1220/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.3784 - val_loss: 139.2081\n",
      "Epoch 1221/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 54.1783 - val_loss: 140.2329\n",
      "Epoch 1222/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.1288 - val_loss: 138.1529\n",
      "Epoch 1223/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 54.0030 - val_loss: 138.8929\n",
      "Epoch 1224/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 53.8377 - val_loss: 138.3034\n",
      "Epoch 1225/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.7195 - val_loss: 137.7720\n",
      "Epoch 1226/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.6274 - val_loss: 138.8132\n",
      "Epoch 1227/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 53.5501 - val_loss: 136.7575\n",
      "Epoch 1228/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.4460 - val_loss: 137.5094\n",
      "Epoch 1229/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 53.2762 - val_loss: 136.9883\n",
      "Epoch 1230/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 53.1576 - val_loss: 136.4742\n",
      "Epoch 1231/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 53.0541 - val_loss: 137.5627\n",
      "Epoch 1232/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 53.0142 - val_loss: 134.6219\n",
      "Epoch 1233/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 53.0806 - val_loss: 136.7426\n",
      "Epoch 1234/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.7814 - val_loss: 134.8002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1235/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 52.7351 - val_loss: 136.2612\n",
      "Epoch 1236/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.5973 - val_loss: 134.4319\n",
      "Epoch 1237/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 52.5199 - val_loss: 135.7965\n",
      "Epoch 1238/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.4159 - val_loss: 134.0441\n",
      "Epoch 1239/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 52.3114 - val_loss: 135.3419\n",
      "Epoch 1240/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 52.2393 - val_loss: 133.3125\n",
      "Epoch 1241/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 52.2048 - val_loss: 134.8394\n",
      "Epoch 1242/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 52.0631 - val_loss: 132.9697\n",
      "Epoch 1243/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 52.0127 - val_loss: 134.3607\n",
      "Epoch 1244/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.8919 - val_loss: 132.6021\n",
      "Epoch 1245/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 51.8306 - val_loss: 133.8947\n",
      "Epoch 1246/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 51.7243 - val_loss: 132.5157\n",
      "Epoch 1247/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 51.5794 - val_loss: 132.8175\n",
      "Epoch 1248/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 51.4773 - val_loss: 132.0744\n",
      "Epoch 1249/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 51.4040 - val_loss: 133.1451\n",
      "Epoch 1250/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 51.4143 - val_loss: 131.3466\n",
      "Epoch 1251/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.2995 - val_loss: 132.6785\n",
      "Epoch 1252/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 51.2430 - val_loss: 130.9740\n",
      "Epoch 1253/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.1144 - val_loss: 131.9233\n",
      "Epoch 1254/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 51.0237 - val_loss: 130.8715\n",
      "Epoch 1255/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.8944 - val_loss: 130.5260\n",
      "Epoch 1256/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.8274 - val_loss: 130.9142\n",
      "Epoch 1257/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 50.7510 - val_loss: 130.1882\n",
      "Epoch 1258/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.6593 - val_loss: 130.6198\n",
      "Epoch 1259/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.6043 - val_loss: 129.8624\n",
      "Epoch 1260/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.4899 - val_loss: 129.5033\n",
      "Epoch 1261/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 50.4110 - val_loss: 130.0143\n",
      "Epoch 1262/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.3594 - val_loss: 129.2105\n",
      "Epoch 1263/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.2295 - val_loss: 128.4028\n",
      "Epoch 1264/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 50.1982 - val_loss: 128.9951\n",
      "Epoch 1265/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 50.0774 - val_loss: 128.1608\n",
      "Epoch 1266/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.9943 - val_loss: 128.7946\n",
      "Epoch 1267/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.9224 - val_loss: 127.9363\n",
      "Epoch 1268/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 49.7933 - val_loss: 128.1295\n",
      "Epoch 1269/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.7307 - val_loss: 127.2462\n",
      "Epoch 1270/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 49.6267 - val_loss: 127.4824\n",
      "Epoch 1271/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 49.5375 - val_loss: 126.5750\n",
      "Epoch 1272/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.4558 - val_loss: 126.8549\n",
      "Epoch 1273/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 49.3431 - val_loss: 125.9249\n",
      "Epoch 1274/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.2830 - val_loss: 126.7987\n",
      "Epoch 1275/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 49.1859 - val_loss: 125.3234\n",
      "Epoch 1276/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 49.1047 - val_loss: 126.1800\n",
      "Epoch 1277/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 48.9912 - val_loss: 125.2577\n",
      "Epoch 1278/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.8885 - val_loss: 125.5821\n",
      "Epoch 1279/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 48.7950 - val_loss: 124.0889\n",
      "Epoch 1280/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 48.7486 - val_loss: 125.4838\n",
      "Epoch 1281/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 48.6705 - val_loss: 124.0751\n",
      "Epoch 1282/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 48.5339 - val_loss: 124.3699\n",
      "Epoch 1283/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 48.4134 - val_loss: 124.0794\n",
      "Epoch 1284/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.3178 - val_loss: 123.7960\n",
      "Epoch 1285/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.2229 - val_loss: 122.9167\n",
      "Epoch 1286/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.1677 - val_loss: 123.3199\n",
      "Epoch 1287/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 48.0413 - val_loss: 122.4211\n",
      "Epoch 1288/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 47.9762 - val_loss: 122.8663\n",
      "Epoch 1289/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.8584 - val_loss: 121.9498\n",
      "Epoch 1290/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.7800 - val_loss: 122.4347\n",
      "Epoch 1291/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 47.6789 - val_loss: 120.8214\n",
      "Epoch 1292/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.6512 - val_loss: 121.9352\n",
      "Epoch 1293/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 47.4960 - val_loss: 120.4063\n",
      "Epoch 1294/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 47.4503 - val_loss: 121.4462\n",
      "Epoch 1295/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 47.3156 - val_loss: 119.9760\n",
      "Epoch 1296/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 47.2531 - val_loss: 120.9647\n",
      "Epoch 1297/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 47.1370 - val_loss: 119.5376\n",
      "Epoch 1298/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 47.0581 - val_loss: 120.4894\n",
      "Epoch 1299/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 46.9701 - val_loss: 118.6309\n",
      "Epoch 1300/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 46.9368 - val_loss: 119.9897\n",
      "Epoch 1301/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.8014 - val_loss: 118.2749\n",
      "Epoch 1302/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 46.7510 - val_loss: 119.0407\n",
      "Epoch 1303/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.6013 - val_loss: 118.8235\n",
      "Epoch 1304/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.5165 - val_loss: 118.1203\n",
      "Epoch 1305/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.4305 - val_loss: 117.9300\n",
      "Epoch 1306/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 46.3397 - val_loss: 117.7497\n",
      "Epoch 1307/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.2476 - val_loss: 117.5798\n",
      "Epoch 1308/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.1635 - val_loss: 116.8377\n",
      "Epoch 1309/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 46.0833 - val_loss: 117.3526\n",
      "Epoch 1310/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 46.0061 - val_loss: 116.0010\n",
      "Epoch 1311/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.9524 - val_loss: 117.5638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1312/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 46.0045 - val_loss: 114.9961\n",
      "Epoch 1313/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 45.9269 - val_loss: 116.8922\n",
      "Epoch 1314/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 45.7882 - val_loss: 115.1021\n",
      "Epoch 1315/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.6474 - val_loss: 115.7319\n",
      "Epoch 1316/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 45.5507 - val_loss: 114.7326\n",
      "Epoch 1317/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 45.4823 - val_loss: 115.4001\n",
      "Epoch 1318/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 45.4201 - val_loss: 114.3750\n",
      "Epoch 1319/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.3231 - val_loss: 114.7012\n",
      "Epoch 1320/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.2453 - val_loss: 113.6441\n",
      "Epoch 1321/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 45.2356 - val_loss: 114.8802\n",
      "Epoch 1322/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.1781 - val_loss: 113.3968\n",
      "Epoch 1323/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.0767 - val_loss: 114.5846\n",
      "Epoch 1324/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 45.0647 - val_loss: 113.1304\n",
      "Epoch 1325/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 44.9228 - val_loss: 113.9242\n",
      "Epoch 1326/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.8895 - val_loss: 112.4668\n",
      "Epoch 1327/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.8471 - val_loss: 113.6284\n",
      "Epoch 1328/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.7756 - val_loss: 112.1950\n",
      "Epoch 1329/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.7007 - val_loss: 112.9733\n",
      "Epoch 1330/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 44.6011 - val_loss: 111.5345\n",
      "Epoch 1331/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.6250 - val_loss: 112.7758\n",
      "Epoch 1332/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.5110 - val_loss: 111.3647\n",
      "Epoch 1333/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.4705 - val_loss: 112.1266\n",
      "Epoch 1334/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.3459 - val_loss: 111.1386\n",
      "Epoch 1335/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.3289 - val_loss: 111.9314\n",
      "Epoch 1336/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.2448 - val_loss: 110.9126\n",
      "Epoch 1337/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 44.1816 - val_loss: 111.7333\n",
      "Epoch 1338/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.1519 - val_loss: 109.8984\n",
      "Epoch 1339/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 44.1903 - val_loss: 111.1423\n",
      "Epoch 1340/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 44.0069 - val_loss: 109.8025\n",
      "Epoch 1341/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 44.0286 - val_loss: 111.0074\n",
      "Epoch 1342/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.9405 - val_loss: 109.6804\n",
      "Epoch 1343/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 43.8695 - val_loss: 110.8557\n",
      "Epoch 1344/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 43.8733 - val_loss: 109.1777\n",
      "Epoch 1345/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.7926 - val_loss: 110.2933\n",
      "Epoch 1346/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.7318 - val_loss: 109.0147\n",
      "Epoch 1347/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.6405 - val_loss: 110.1200\n",
      "Epoch 1348/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.6615 - val_loss: 108.8401\n",
      "Epoch 1349/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.5058 - val_loss: 109.2698\n",
      "Epoch 1350/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 43.4713 - val_loss: 107.9555\n",
      "Epoch 1351/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.5000 - val_loss: 109.1005\n",
      "Epoch 1352/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 43.4025 - val_loss: 107.4739\n",
      "Epoch 1353/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 43.4145 - val_loss: 108.5722\n",
      "Epoch 1354/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.2760 - val_loss: 106.9869\n",
      "Epoch 1355/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.3316 - val_loss: 108.0507\n",
      "Epoch 1356/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 43.1560 - val_loss: 106.8744\n",
      "Epoch 1357/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 43.1892 - val_loss: 107.6493\n",
      "Epoch 1358/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 43.0676 - val_loss: 107.2950\n",
      "Epoch 1359/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 43.0307 - val_loss: 107.3633\n",
      "Epoch 1360/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 42.9889 - val_loss: 106.9935\n",
      "Epoch 1361/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 42.9493 - val_loss: 107.0838\n",
      "Epoch 1362/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.9104 - val_loss: 106.2196\n",
      "Epoch 1363/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 42.8817 - val_loss: 106.7636\n",
      "Epoch 1364/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.8206 - val_loss: 105.8628\n",
      "Epoch 1365/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.7906 - val_loss: 105.9982\n",
      "Epoch 1366/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.7345 - val_loss: 106.1430\n",
      "Epoch 1367/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.6772 - val_loss: 105.7351\n",
      "Epoch 1368/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.6369 - val_loss: 105.9128\n",
      "Epoch 1369/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.6108 - val_loss: 104.3499\n",
      "Epoch 1370/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.5876 - val_loss: 104.5492\n",
      "Epoch 1371/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.5242 - val_loss: 104.7588\n",
      "Epoch 1372/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 42.4585 - val_loss: 104.9790\n",
      "Epoch 1373/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.4099 - val_loss: 104.5600\n",
      "Epoch 1374/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 42.3607 - val_loss: 104.1385\n",
      "Epoch 1375/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.3147 - val_loss: 104.4211\n",
      "Epoch 1376/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.2890 - val_loss: 102.6672\n",
      "Epoch 1377/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.2699 - val_loss: 102.9522\n",
      "Epoch 1378/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.1951 - val_loss: 103.2470\n",
      "Epoch 1379/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.1240 - val_loss: 102.1410\n",
      "Epoch 1380/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.1176 - val_loss: 102.4686\n",
      "Epoch 1381/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 42.0374 - val_loss: 102.8050\n",
      "Epoch 1382/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 42.0159 - val_loss: 101.6399\n",
      "Epoch 1383/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.9565 - val_loss: 102.0081\n",
      "Epoch 1384/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 41.9052 - val_loss: 100.8196\n",
      "Epoch 1385/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 41.8743 - val_loss: 101.2175\n",
      "Epoch 1386/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 41.7979 - val_loss: 100.8424\n",
      "Epoch 1387/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 41.7510 - val_loss: 100.4711\n",
      "Epoch 1388/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.7038 - val_loss: 100.1039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1389/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.6562 - val_loss: 99.7410\n",
      "Epoch 1390/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.6082 - val_loss: 99.3830\n",
      "Epoch 1391/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 41.5616 - val_loss: 99.9752\n",
      "Epoch 1392/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.5420 - val_loss: 98.5871\n",
      "Epoch 1393/1500\n",
      "87/87 [==============================] - 0s 69us/step - loss: 41.4700 - val_loss: 99.1797\n",
      "Epoch 1394/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 41.4263 - val_loss: 97.8071\n",
      "Epoch 1395/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 41.3846 - val_loss: 99.2144\n",
      "Epoch 1396/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.3771 - val_loss: 97.9372\n",
      "Epoch 1397/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 41.2711 - val_loss: 97.5872\n",
      "Epoch 1398/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 41.2227 - val_loss: 97.2428\n",
      "Epoch 1399/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 41.1739 - val_loss: 96.9046\n",
      "Epoch 1400/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 41.1249 - val_loss: 96.5728\n",
      "Epoch 1401/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 41.0754 - val_loss: 96.2477\n",
      "Epoch 1402/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 41.0277 - val_loss: 97.0400\n",
      "Epoch 1403/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 41.0312 - val_loss: 94.8430\n",
      "Epoch 1404/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 41.0984 - val_loss: 97.1814\n",
      "Epoch 1405/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 41.0410 - val_loss: 95.5690\n",
      "Epoch 1406/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.8611 - val_loss: 95.2228\n",
      "Epoch 1407/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.8145 - val_loss: 94.8803\n",
      "Epoch 1408/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.7747 - val_loss: 95.2710\n",
      "Epoch 1409/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.7596 - val_loss: 93.5907\n",
      "Epoch 1410/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 40.8166 - val_loss: 95.1921\n",
      "Epoch 1411/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.7410 - val_loss: 93.6663\n",
      "Epoch 1412/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.6715 - val_loss: 94.6165\n",
      "Epoch 1413/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 40.6515 - val_loss: 93.1218\n",
      "Epoch 1414/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.6179 - val_loss: 94.0463\n",
      "Epoch 1415/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.5628 - val_loss: 92.5759\n",
      "Epoch 1416/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.5646 - val_loss: 93.4808\n",
      "Epoch 1417/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 40.4749 - val_loss: 92.0298\n",
      "Epoch 1418/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 40.5115 - val_loss: 92.9196\n",
      "Epoch 1419/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.3969 - val_loss: 92.1007\n",
      "Epoch 1420/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.3883 - val_loss: 93.0089\n",
      "Epoch 1421/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.3864 - val_loss: 91.5314\n",
      "Epoch 1422/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.3330 - val_loss: 92.4236\n",
      "Epoch 1423/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.2962 - val_loss: 90.9686\n",
      "Epoch 1424/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.2780 - val_loss: 91.8483\n",
      "Epoch 1425/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.2221 - val_loss: 91.0269\n",
      "Epoch 1426/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 40.1809 - val_loss: 90.8142\n",
      "Epoch 1427/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 40.1530 - val_loss: 90.6015\n",
      "Epoch 1428/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.1244 - val_loss: 90.3891\n",
      "Epoch 1429/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.0952 - val_loss: 90.1775\n",
      "Epoch 1430/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 40.0655 - val_loss: 89.9673\n",
      "Epoch 1431/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.0351 - val_loss: 89.7590\n",
      "Epoch 1432/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 40.0042 - val_loss: 89.5532\n",
      "Epoch 1433/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.9726 - val_loss: 89.3502\n",
      "Epoch 1434/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 39.9405 - val_loss: 89.1509\n",
      "Epoch 1435/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.9079 - val_loss: 88.9556\n",
      "Epoch 1436/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.8747 - val_loss: 88.7650\n",
      "Epoch 1437/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.8409 - val_loss: 88.5796\n",
      "Epoch 1438/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.8065 - val_loss: 88.4002\n",
      "Epoch 1439/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.7716 - val_loss: 88.2271\n",
      "Epoch 1440/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.7405 - val_loss: 87.0124\n",
      "Epoch 1441/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.7403 - val_loss: 87.8986\n",
      "Epoch 1442/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.6930 - val_loss: 86.7002\n",
      "Epoch 1443/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.6561 - val_loss: 87.5607\n",
      "Epoch 1444/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.6445 - val_loss: 86.3752\n",
      "Epoch 1445/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.5755 - val_loss: 86.2149\n",
      "Epoch 1446/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.5391 - val_loss: 86.0623\n",
      "Epoch 1447/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.5079 - val_loss: 84.8174\n",
      "Epoch 1448/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.5437 - val_loss: 86.4961\n",
      "Epoch 1449/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.5091 - val_loss: 85.3815\n",
      "Epoch 1450/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.4280 - val_loss: 84.2731\n",
      "Epoch 1451/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.4095 - val_loss: 85.0406\n",
      "Epoch 1452/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.3792 - val_loss: 83.9306\n",
      "Epoch 1453/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 39.3265 - val_loss: 84.6974\n",
      "Epoch 1454/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.3299 - val_loss: 83.5867\n",
      "Epoch 1455/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 39.2493 - val_loss: 83.4632\n",
      "Epoch 1456/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.2288 - val_loss: 82.3286\n",
      "Epoch 1457/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 39.2539 - val_loss: 83.8732\n",
      "Epoch 1458/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 39.2279 - val_loss: 82.8216\n",
      "Epoch 1459/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 39.1502 - val_loss: 81.7735\n",
      "Epoch 1460/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.1174 - val_loss: 83.1770\n",
      "Epoch 1461/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.1464 - val_loss: 82.1754\n",
      "Epoch 1462/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.0716 - val_loss: 81.1754\n",
      "Epoch 1463/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 39.0119 - val_loss: 81.0332\n",
      "Epoch 1464/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.9802 - val_loss: 80.9007\n",
      "Epoch 1465/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.9555 - val_loss: 79.8461\n",
      "Epoch 1466/1500\n",
      "87/87 [==============================] - 0s 58us/step - loss: 39.0190 - val_loss: 81.9510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1467/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.9940 - val_loss: 81.0173\n",
      "Epoch 1468/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.9229 - val_loss: 80.0830\n",
      "Epoch 1469/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.8596 - val_loss: 80.0646\n",
      "Epoch 1470/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.8384 - val_loss: 80.0508\n",
      "Epoch 1471/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.8205 - val_loss: 79.0400\n",
      "Epoch 1472/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.8383 - val_loss: 80.5674\n",
      "Epoch 1473/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.8272 - val_loss: 79.5874\n",
      "Epoch 1474/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.7543 - val_loss: 79.5721\n",
      "Epoch 1475/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.7376 - val_loss: 78.5581\n",
      "Epoch 1476/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.7236 - val_loss: 79.4001\n",
      "Epoch 1477/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.6992 - val_loss: 78.3690\n",
      "Epoch 1478/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.6728 - val_loss: 78.3747\n",
      "Epoch 1479/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.6500 - val_loss: 78.3866\n",
      "Epoch 1480/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.6266 - val_loss: 78.4052\n",
      "Epoch 1481/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.6025 - val_loss: 78.4312\n",
      "Epoch 1482/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.5779 - val_loss: 78.4649\n",
      "Epoch 1483/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.5526 - val_loss: 78.5069\n",
      "Epoch 1484/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.5340 - val_loss: 77.2375\n",
      "Epoch 1485/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 38.5137 - val_loss: 77.3043\n",
      "Epoch 1486/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.4877 - val_loss: 77.3800\n",
      "Epoch 1487/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4611 - val_loss: 77.4652\n",
      "Epoch 1488/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.4337 - val_loss: 77.5604\n",
      "Epoch 1489/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4057 - val_loss: 77.6659\n",
      "Epoch 1490/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3832 - val_loss: 76.2090\n",
      "Epoch 1491/1500\n",
      "87/87 [==============================] - 0s 57us/step - loss: 38.3682 - val_loss: 77.5569\n",
      "Epoch 1492/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.3621 - val_loss: 74.7597\n",
      "Epoch 1493/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.4790 - val_loss: 76.8903\n",
      "Epoch 1494/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2983 - val_loss: 76.8492\n",
      "Epoch 1495/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.2828 - val_loss: 76.0698\n",
      "Epoch 1496/1500\n",
      "87/87 [==============================] - 0s 23us/step - loss: 38.2631 - val_loss: 76.0447\n",
      "Epoch 1497/1500\n",
      "87/87 [==============================] - 0s 34us/step - loss: 38.2401 - val_loss: 76.0253\n",
      "Epoch 1498/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.2167 - val_loss: 76.0120\n",
      "Epoch 1499/1500\n",
      "87/87 [==============================] - 0s 35us/step - loss: 38.1926 - val_loss: 76.0056\n",
      "Epoch 1500/1500\n",
      "87/87 [==============================] - 0s 46us/step - loss: 38.1793 - val_loss: 75.0932\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=128, epochs=1500,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXhU5dn48e+dyb7vCwRI2HdZIqCCIiAKWtCKC9VXRS11bfvythVb2+r704raVvS11qposVpxF+qOLIoLYECQHcKajSRANiAJWZ7fH+ckDJDJRmYmy/25rnOdmec8M3PnwOTOebYjxhiUUkopAB9vB6CUUqrt0KSglFKqjiYFpZRSdTQpKKWUqqNJQSmlVB1NCkoppepoUlCqmUQkRUSMiPg2oe4tIvLV2b6PUp6iSUF1aCKyT0ROiEjsaeUb7F/IKd6JTKm2SZOC6gz2AjNrn4jIECDIe+Eo1XZpUlCdwb+Am5ye3wy84lxBRCJE5BURKRCR/SLygIj42MccIvJnETkkInuAy+t57QIRyRWRbBF5WEQczQ1SRLqIyBIROSIiGSLyU6djo0QkXURKRCRPRP5qlweKyKsiclhEikTkOxFJaO5nK1VLk4LqDFYD4SIywP5lfR3w6ml1/g+IAHoCF2ElkVn2sZ8CVwDDgTRgxmmvXQhUAb3tOpOB21sQ5+tAFtDF/ow/ichE+9hTwFPGmHCgF/CmXX6zHXc3IAa4AyhrwWcrBWhSUJ1H7dXCJcB2ILv2gFOiuN8YU2qM2Qf8Bfgvu8q1wHxjTKYx5gjwqNNrE4ApwC+NMceMMfnAk8D1zQlORLoBY4H7jDHlxpgNwItOMVQCvUUk1hhz1Biz2qk8BuhtjKk2xqwzxpQ057OVcqZJQXUW/wJ+AtzCaU1HQCzgD+x3KtsPdLUfdwEyTztWqwfgB+TazTdFwD+A+GbG1wU4YowpdRHDbUBfYLvdRHSF08/1KbBIRHJE5HER8WvmZytVR5OC6hSMMfuxOpynAu+edvgQ1l/cPZzKunPyaiIXq3nG+VitTKACiDXGRNpbuDFmUDNDzAGiRSSsvhiMMbuMMTOxks1jwNsiEmKMqTTGPGSMGQicj9XMdRNKtZAmBdWZ3AZMMMYccy40xlRjtdE/IiJhItIDmMPJfoc3gZ+LSLKIRAFznV6bC3wG/EVEwkXER0R6ichFzQnMGJMJfAM8anceD7XjfQ1ARG4UkThjTA1QZL+sWkQuFpEhdhNYCVZyq27OZyvlTJOC6jSMMbuNMekuDt8LHAP2AF8B/wZeso+9gNVEsxFYz5lXGjdhNT9tBQqBt4GkFoQ4E0jBump4D/ijMWapfewyYIuIHMXqdL7eGFMOJNqfVwJsA77gzE50pZpM9CY7SimlaumVglJKqTqaFJRSStXRpKCUUqqOJgWllFJ12vWSvbGxsSYlJcXbYSilVLuybt26Q8aYuPqOteukkJKSQnq6qxGGSiml6iMi+10d0+YjpZRSdTQpKKWUqqNJQSmlVJ123aeglFJNVVlZSVZWFuXl5d4OxWMCAwNJTk7Gz6/pC+dqUlBKdQpZWVmEhYWRkpKCiHg7HLczxnD48GGysrJITU1t8uu0+Ugp1SmUl5cTExPTKRICgIgQExPT7CsjTQpKqU6jsySEWi35eTtl89F3+46watchQvwdBPs7CPL3JTzQl/jwQOLDAogLC8DPoflSKdX5dMqksH5/IU8v29VgnegQf+LDAkiKCKRLZBBdIoPoau+7RAaSGB6IryYOpVQTHT58mIkTJwJw8OBBHA4HcXHWpOK1a9fi7+/f6HvMmjWLuXPn0q9fP7fF2a7vp5CWlmZaOqO5psZQVlnN8RPVlJ2opriskvzScvJLK8gvqSC/tJy8kgoOlpSRU1TOkWMnTnm9j0Bi+MmE0S06iJ6xoaTGhdArNpSIYL1NrlJtybZt2xgwYIC3wwDgwQcfJDQ0lF/96lenlBtjMMbg49N6f3DW93OLyDpjTFp99TvllQKAj48QEuBLSIDzKYhwWb/sRDU5xWXkFFlbdmEZ2UXl5BSVsSGziI825VJVczLBxoT4kxobQq+4UAYkhTGoawQDksIJDei0p1wpVY+MjAyuvPJKxo4dy5o1a/jggw946KGHWL9+PWVlZVx33XX84Q9/AGDs2LE888wzDB48mNjYWO644w4+/vhjgoODWbx4MfHx8Wcdj/6GaqIgfwe94kLpFRda7/HK6hoyjxxnT8Ex9h46xp5DR9ldcIzPt+XxRnomACKQEhPCwC7hDEuO5NzUaAZ1Cdf+C6U87KH/bGFrTkmrvufALuH88UeDWvTarVu38vLLL/Pcc88BMG/ePKKjo6mqquLiiy9mxowZDBw48JTXFBcXc9FFFzFv3jzmzJnDSy+9xNy5c+t7+2bRpNBK/Bw+9IwLpedpScMYQ35pBVtyitmSXcLmnGI2Zhbx4Q+5AAT7OxjePZJzU6IZ2zuW4d2jcPh0rhESSnV2vXr14txzz617/vrrr7NgwQKqqqrIyclh69atZySFoKAgpkyZAsDIkSNZtWpVq8TitqQgIv2AN5yKegJ/AF6xy1OAfcC1xphCscZOPQVMBY4Dtxhj1rsrPk8RERLCA0kID2RC/4S68vyScr7bV8javYdZu6+Qp5btYv7nu4gM9uPCPnFM6B/PRX3jiAppvPNJKdU8Lf2L3l1CQkLqHu/atYunnnqKtWvXEhkZyY033ljvXAPnjmmHw0FVVVWrxOK2pGCM2QEMAxARB5ANvAfMBZYZY+aJyFz7+X3AFKCPvY0G/m7vO6T48EAuH5rE5UOTACg+XsmqjAJWbC9g5Y58lmzMwddHuLBvHNPO6cIlAxNO6/9QSnVEJSUlhIWFER4eTm5uLp9++imXXXaZxz7fU79lJgK7jTH7RWQ6MN4uXwisxEoK04FXjDUcarWIRIpIkjEm10MxelVEsB9XDO3CFUO7UFNj+CG7mI835fKfjTks355PoJ8PUwYnceOYHozoHtnpJuEo1VmMGDGCgQMHMnjwYHr27MkFF1zg0c/3yJBUEXkJWG+MeUZEiowxkU7HCo0xUSLyATDPGPOVXb4MuM8Y43LM6dkMSW0vamoM6fsLWbwhm8UbcjhaUcXApHBuHNODK4d3Idhfrx6Uaoq2NCTVk5o7JNXtw15ExB+YBrzVWNV6ys7IWCIyW0TSRSS9oKCgNUJs03x8hFGp0Txy1RDW/HYij1w1mBpj+O17mxj72AqeWb6L4rJKb4eplOogPDEWcgrWVUKe/TxPRJIA7H2+XZ4FdHN6XTKQc/qbGWOeN8akGWPSamcDdhYhAb7cMLoHH/9iHG/+7DzOSY7gz5/tZOy85Tzx6XaKj2tyUEqdHU+0PcwEXnd6vgS4GZhn7xc7ld8jIouwOpiL3dafkL0eMteAf4i1+YVAcDSEJkBYIvgGuOVjW4uIdfUwKnUUm7OLeXZlBs+u3M2rqw9w74Te/Nd5PQjwdXg7TKVUO+TWpCAiwcAlwM+ciucBb4rIbcAB4Bq7/COs4agZWENSZ7ktsD0rYdlDro8HRUFoopUgIrpCRHeI7AYR3SAiGcK7gm/bGCo6uGsEz94wki05xcz7eDsPf7iNhd/u47dTBnDZ4ETtkFZKNUvnXPuouhIqSuHEMag8DhVHoewIlOZCaZ61P2rvi7Osx6cQCEuyE0UyRPaA2L4Q2wdiekNQZL0f6wlf7izgTx9tY/vBUib0j+ehaYPoFh3stXiUaiu0o/kkXfvodA4/q7koOLpp9SvLoSQbijOhKNNKFMWZUHTAaorauhhqnCaOhMSfTBJJQyHpHIgfBH6B7vl5nFzYN47ze8Xwz2/28delO5n85Jf89yV9uG1sT50prZRqVOdMCs3lFwgxvaytPtWVULgfDu2Ew7us/aEM2PIurHvZquPjC3EDrATRfQykXABRqdaCSK3M1+HD7eN6MmVIEn9cvIU/fbSdz7bk8eR1w/SqQSkvGT9+PPfffz+XXnppXdn8+fPZuXMnzz77bL2vCQ0N5ejRo54KEdCk0DocfhDb29qcGWNdTeRugNyNkLMBdnwEG161jod1sZJDyjjoMxnCk1o1rK6RQbxw00gWb8jh9+9vZspTq3hw2iCuHtFV+xqU8rCZM2eyaNGiU5LCokWLeOKJJ7wY1Zk0KbiTCET1sLaB062ymho4tAP2fw37voa9X8ImewpH0jDoN8XaEoe2ylWEiHDl8K6kpUQx582N/OqtjXyTcYhHrhpCkL+OUFLKU2bMmMEDDzxARUUFAQEB7Nu3j5ycHIYNG8bEiRMpLCyksrKShx9+mOnTp3stTk0KnubjA/EDrO3c262rifxtsPNj2PkprJwHKx+1+iSGXgtDroGolLP+2OSoYF7/6RieWZ7B/GU72XawlH/cOJLuMdqcpDqhj+fCwU2t+56JQ2DKPJeHY2JiGDVqFJ988gnTp09n0aJFXHfddQQFBfHee+8RHh7OoUOHGDNmDNOmTfPa1bwu5O9tIpAwEMb9D9z2Gfw6A66YDyFxsPxheOoc+OcVsHUJVJ/dKogOH+EXk/rw0i3nkl14nB898xVf7uz4s8KVaitqm5DAajqaOXMmxhh++9vfMnToUCZNmkR2djZ5eaePePQcvVJoa0JiIW2WtRUdgB/egHUL4c3/suZJpN1qbWcx7PXifvF8cO84Zv8rnVn//I4/XTWY687t3oo/hFJtXAN/0bvTlVdeyZw5c+ruqjZixAj++c9/UlBQwLp16/Dz8yMlJaXepbI9Ra8U2rLI7nDhr+HnG+C6V61mpGUPwfyhsOJRKCtq8Vt3jwnm7TvP54Lesdz3zib+8tkO2vOcFaXag9DQUMaPH8+tt97KzJkzAesOavHx8fj5+bFixQr279/v1Rg1KbQHDl8Y8CO45QP42ZeQOg6+mAfzh8DKx6xJeC0QGuDLgpvTuC6tG/+3PIP/eWsjVdU1rRy8UsrZzJkz2bhxI9dffz0AN9xwA+np6aSlpfHaa6/Rv39/r8anzUftTdI5cP1rVifZF4/Byj/Bun/CpAetTmmf5uV5P4cP864eQpfIIJ78fCcVlTXMv36Y3jdaKTe56qqrTrkqj42N5dtvv623rqfnKIBeKbRfiUOsJqVZn0BYArw3GxZcAgc3N/utRKwO6AcuH8CHm3K567X1VFRVuyFopVRbp0mhvetxHty+HK58Dor2w/MXwfJHoKqi2W91+7ie/O/0QSzdmsfP/rVOE4NSnZAmhY7AxweGzYS711pNSF8+Ds+Ns2ZQN9NN56Xw6I+HsHJHAb94fYP2MagOpbMNpmjJz6tJoSMJjoarnoMb3rFWgX1xEnz7rDVBrhlmjurOH64YyCdbDnL/u5uoqelcXyTVMQUGBnL48OFOkxiMMRw+fJjAwOYtxKkdzR1Rn0lw59ew+G749H7r/hFX/h1CYpr8FreOTaW4rJKnlu0iPMiPBy4foOslqXYtOTmZrKwsOsNtfGsFBgaSnJzcrNdoUuiogqPh+n/D2hfgs9/BC+Ph+tchcXCT3+KXk/pQXFbJgq/2khQRyO3jerovXqXczM/Pj9TUVG+H0eZp81FHJgKjZ8Otn1jLey+YDNv+04yXC3+4YiBTBifyyEfbWLrVe1PvlVKeoUmhM+g6EmavtBbhe+NGWPXXJvcz+PgIf712GEO6RvCLRd+zObvYraEqpbxLk0JnEZYIt3wIg2dYS2V8+jtrGe8mCPJ38OJNaUQG+XHbwu/IL/HeuixKKffSpNCZ+AXCj1+A0XfA6r/B+3dYzUpNEB8eyIs3n0tJWRV3/3s9lTpUVakOya1JQUQiReRtEdkuIttE5DwRiRaRpSKyy95H2XVFRJ4WkQwR+UFERrgztk7LxwcumwcTHrBWYH3zZqg60aSXDuwSzryrh/DdvkIe+3i7mwNVSnmDu68UngI+Mcb0B84BtgFzgWXGmD7AMvs5wBSgj73NBv7u5tg6LxFr9dUpT8COD+GdW5t8xTB9WFduPq8HL361lw9/yHVzoEopT3NbUhCRcOBCYAGAMeaEMaYImA4stKstBK60H08HXjGW1UCkiLTuTYvVqUbPtq4atv0H3rm9yTfx+d3lAxnRPZLfvL2R3QWeX7BLKeU+7rxS6AkUAC+LyPci8qKIhAAJxphcAHsfb9fvCmQ6vT7LLjuFiMwWkXQRSe9Mk1DcZsydMPkR2Po+LL6rSZ3P/r4+PHvDSPx9ffjlog2cqNL+BaU6CncmBV9gBPB3Y8xw4Bgnm4rqU9902TPGTRpjnjfGpBlj0uLi4lon0s7u/HvgYruPYdlDTXpJYkQg864eyqbsYv66dKebA1RKeYo7k0IWkGWMWWM/fxsrSeTVNgvZ+3yn+t2cXp8M5LgxPuXswl9Zt/n8er41C7oJLh2UyMxR3fnHl7v5ZvchNweolPIEtyUFY8xBIFNE+tlFE4GtwBLgZrvsZmCx/XgJcJM9CmkMUFzbzKQ8QASm/hn6TYWPft3kmc+/v2IAqTEhzHljI0XHmzaKSSnVdrl79NG9wGsi8gMwDPgTMA+4RER2AZfYzwE+AvYAGcALwF1ujk2dzscBVy+A5DR4d3aTbtgT7O/LU9cP59DRCv73g60eCFIp5U7SnpeRTUtLM+np6d4Oo+MpPQjPjweHv7U8RnB0oy/5y2c7+L/lGbw861wu7hffaH2llPeIyDpjTFp9x3RGszpTWKJ1q8/SXHjrliYNVb1nQm/6xIfyu3c3UVretDkPSqm2R5OCql9yGlzxJOz9Aj7/Y6PVA3wdPD5jKAdLypmns52Varc0KSjXht8I594O3z4DOz5pvHr3KG69IJXX1hxgzZ7DHghQKdXaNCmohk1+BBKGwPt3QknjI4T/Z3I/kqOC+P3izbponlLtkCYF1TC/QLjmZagqt0Yk1VQ3WD3I38EffzSInXlHWfjNPs/EqJRqNZoUVONi+1hzGPatsm7Q04hJA+KZ0D+eJ5fuJE/vvaBUu6JJQTXNsJ9YN+j5Yh7kbmywqojwxx8NpLLG8MiH2zwUoFKqNWhSUE0jAlOfgOAYeP+uRu/B0CMmhDsv6sWSjTl8u1s7nZVqLzQpqKYLjoYr5kPeZlj150ar3zm+F10jg3j4w63U1LTfSZJKdSaaFFTz9J8KQ6+DVX9ptBkp0M/Bby7rx5acEt7fkO2hAJVSZ0OTgmq+y+bZzUh3Nzrb+UdDuzA0OYInPt1BeWXDI5eUUt6nSUE1X3C01b+Qtwm+a3iZbR8f4bdTB5BbXM6Cr/Z6KEClVEtpUlAtM2Aa9L4Elj8CJQ2vcD6mZwyXDEzg7yt3c+hohYcCVEq1hCYF1TIiMPVxqD4Bn/2u0epzp/SnrLKaZ5ZneCA4pVRLaVJQLRfdE8b9D2x+B/asbLBqr7hQZoxI5t9rDpBTVOaZ+JRSzaZJQZ2dC35hJYcPfwXVDS+Zfe/E3hgMz6zQqwWl2ipNCurs+AXCpY/C4V2Q/nKDVZOjgrn+3O68+V0mmUeOeyhApVRzaFJQZ6/vpZB6Iax8FMqKGqx698W98fERnl62y0PBKaWaQ5OCOnsi1hLbZYXWpLYGJEYEcuPoHrz7fTZ7Dx3zUIBKqabSpKBaR9JQa9G8Nc9B4b4Gq945vhf+Dh/+b7leLSjV1rg1KYjIPhHZJCIbRCTdLosWkaUissveR9nlIiJPi0iGiPwgIiPcGZtygwkPgI8vfP5gg9XiwgK4flQ3lmzIIatQ+xaUaks8caVwsTFmmDEmzX4+F1hmjOkDLLOfA0wB+tjbbODvHohNtabwLnDePbDlvUbXRfrpuJ6IwAtf7vFQcEqppvBG89F0YKH9eCFwpVP5K8ayGogUkSQvxKfOxnl3Q2AErPhTg9W6RAZx1fCuLPouk4JSneWsVFvh7qRggM9EZJ2IzLbLEowxuQD2Pt4u7wpkOr02yy47hYjMFpF0EUkvKChwY+iqRYIi4fyfw85PICu9wap3XNSLE9U1vPS1romkVFvh7qRwgTFmBFbT0N0icmEDdaWesjMW4TfGPG+MSTPGpMXFxbVWnKo1jb7DWkV1xSMNVusZF8rUwUm8+u1+issanvimlPIMtyYFY0yOvc8H3gNGAXm1zUL2Pt+ungV0c3p5MpDjzviUmwSEwgW/hN3LYf+3DVa9c3wvSiuqeHX1fg8Fp5RqiNuSgoiEiEhY7WNgMrAZWALcbFe7GVhsP14C3GSPQhoDFNc2M6l26NzbITSh0auFwV0jGNcnloXf7ONEVY2HglNKueLOK4UE4CsR2QisBT40xnwCzAMuEZFdwCX2c4CPgD1ABvACcJcbY1Pu5h8MY/8b9q2CA6sbrHrrBankl1bw0Sb9G0ApbxNj2u+9c9PS0kx6esOdmcqLThyH+YOhaxrc8KbLajU1hklPfkGIvy9L7rkAkfq6l5RSrUVE1jlNEziFzmhW7uMfbHU67/oUDm52Wc3HR7j1glQ2ZReTvr/QgwEqpU6nSUG516ifgn8ofD2/wWpXj0gmMtiPBat0eKpS3qRJQblXUBSkzbJuxHPE9S/8IH8HPxnVnc+2HtRltZXyIk0Kyv3G3G2tifTN0w1Wu+m8FHxEWPjNPs/EpZQ6gyYF5X7hSXDOTPj+NTjqehZ6YkQglw5O5K11WZRXVnswQKVULU0KyjPOuweqK2Bdw3dnu3F0D4rLKvngBx2eqpQ3aFJQnhHXF3pPgu9ehKoTLquN6RlNr7gQneGslJdoUlCeM/pOOJoHW993WUVEuGF0DzZkFrE5u9iDwSmlQJOC8qReEyCmD6z+OzQwafLqkckE+vnw2poDHgxOKQWaFJQn+fjA6J9BznrI+s5ltYggP6ad04XFG7IpLdfVU5XyJE0KyrPOmQkBEbD62Qar3TC6B8dPVPPe99keCkwpBZoUlKcFhMLIm2DrEihxvTL6Od0iGdw1nEVrM13WUUq1Pk0KyvPSbgVTDd+/2mC1a9O6sTW3hC052uGslKdoUlCeF90Tel4M6xZCjetJatPO6YK/w4e30rM8GJxSnZsmBeUdabOgJAsyPndZJTLYn0sGJbB4Q7begEcpD9GkoLyj31QIiYf0hmc4XzMymcLjlSzbluehwJTq3DQpKO9w+MHwG617LRS7HmE0rk8cieGBvJmuHc5KeYImBeU9I2+2JrF9/y+XVRw+wtUju/LFzgLySso9GJxSnZMmBeU9USnWLOf1r0B1lctqM0Z2o8bAu+t1zoJS7takpCAivUQkwH48XkR+LiKRTXytQ0S+F5EP7OepIrJGRHaJyBsi4m+XB9jPM+zjKS37kVS7MvIWKMmGPStcVkmNDeHclCjeXpdJe76nuFLtQVOvFN4BqkWkN7AASAX+3cTX/gLY5vT8MeBJY0wfoBC4zS6/DSg0xvQGnrTrqY6u72UQFA0bGv7v9OMRyewuOMaWnBIPBaZU59TUpFBjjKkCrgLmG2P+G0hq7EUikgxcDrxoPxdgAvC2XWUhcKX9eLr9HPv4RLu+6sh8/WHIDNj+IZQVuaw2ZXAifg5h8QZtQlLKnZqaFCpFZCZwM/CBXebXhNfNB34D1A4yjwGK7AQDkAV0tR93BTIB7OPFdv1TiMhsEUkXkfSCAtd38VLtyDkzrRvwbHnPZZXIYH8u6hvPko05VNdoE5JS7tLUpDALOA94xBizV0RSgQbXKBCRK4B8Y8w65+J6qpomHDtZYMzzxpg0Y0xaXFxc06JXbVuX4RDXHza+3mC16cO6kFdSwdq9RzwUmFKdT5OSgjFmqzHm58aY10UkCggzxsxr5GUXANNEZB+wCKvZaD4QKSK+dp1koHZVtCygG4B9PALQb39nIGJdLWSugcO7XVabNCCBEH8HSzZqE5JS7tLU0UcrRSRcRKKBjcDLIvLXhl5jjLnfGJNsjEkBrgeWG2NuAFYAM+xqNwOL7cdL7OfYx5cbHWrSeQy9DsSnwauFIH8Hlw5K5KNNB6mocr1mklKq5ZrafBRhjCkBfgy8bIwZCUxq4WfeB8wRkQysPoMFdvkCIMYunwPMbeH7q/YoPMlaJG/jIqhxvc7RtGFdKC6r5MudhzwYnFKdR1OTgq+IJAHXcrKjucmMMSuNMVfYj/cYY0YZY3obY64xxlTY5eX289728T3N/RzVzg37CRRnwv6vXVa5oHcsMSH+OgpJKTdpalL4X+BTYLcx5jsR6Qnscl9YqlPqNwX8gmHLuy6r+Dl8uHxoEp9vy+NohetZ0EqplmlqR/Nbxpihxpg77ed7jDFXuzc01en4h1iJYcv7UO363sw/OqcL5ZU1LN+e78HglOocmtrRnCwi74lIvojkicg79sQ0pVrX4Kuh7Ajs+cJllZHdo4gPC+CTzbkeDEypzqGpzUcvY40O6oI1yew/dplSrav3JAiIgM3vuKzi4yNcOiiRFdsLOH5Cm5CUak1NTQpxxpiXjTFV9vZPQGeOqdbnGwADfgTbP4BK10tlTxmSSFllNV/s0FntSrWmpiaFQyJyo73iqUNEbgQOuzMw1YkN/jFUlEDGUpdVRqVEExPiz0ebD3owMKU6vqYmhVuxhqMeBHKxJpfNcldQqpNLvQiCYxpsQvJ1+DB5UALLt+VRXqkT2ZRqLU0dfXTAGDPNGBNnjIk3xlyJNZFNqdbn8IWBV8KOT6DiqMtqUwYncexENat26UQ2pVrL2dx5bU6rRaHU6Qb/GKrKrHs4u3Berxgigvz4eJOOQlKqtZxNUtB7HSj36X4eBMfCNtcT6P0cPkwemMDSbXm6FpJSreRskoIuVqfcx8cB/S+HXZ81OgqptLyKbzJ03INSraHBpCAipSJSUs9WijVnQSn3GTANThyFPStdVrmgdywh/g6WbsvzXFxKdWANJgVjTJgxJryeLcwY49vQa5U6a6kXQkA4bPuPyyoBvg4u6hfHsm151Ogd2ZQ6a2fTfKSUe/n6Q9/LYMeHUO165vKkAQnklVSwOafYg8Ep1TFpUlBt24AfQVlhg8tpX9wvHh+Bz7dqE5JSZ0uTgmrbek8E36AGm5CiQvxJS4lm6TZdNVWps6VJQbVt/iFWYtj+QYN3ZLtkQALbckvIKjzuweCU6ng0Kai2b8A0KM2F7HUuq0wamADAMr1aUOqsaMm/n5wAAByQSURBVFJQbV/fySAO2PmJyyqpsSH0igvhcx2aqtRZ0aSg2r6gKGuG807XS16ANQpp9Z7DlJS7vmubUqphbksKIhIoImtFZKOIbBGRh+zyVBFZIyK7ROQNEfG3ywPs5xn28RR3xabaob6XQt4mKM5yWWXSwAQqqw1f7tR7LCjVUu68UqgAJhhjzgGGAZeJyBjgMeBJY0wfoBC4za5/G1BojOkNPGnXU8rS9zJr30AT0ojuUUSH+LNUh6Yq1WJuSwrGUrvusZ+9GWAC8LZdvhC40n483X6OfXyiiOiie8oS2weiUhtsQnL4COP7xbFyRwFV1a5HKimlXHNrn4J9l7YNQD6wFNgNFBljaqenZmHd8xl7nwlgHy8GYup5z9kiki4i6QUF2kzQaYhYVwt7voATx1xWmzQggeKyStbtL/RgcEp1HG5NCsaYamPMMCAZGAUMqK+ava/vquCMxWyMMc8bY9KMMWlxcXqb6E6l76VQXQF7v3RZZVyfWPwcwrLtOjRVqZbwyOgjY0wRsBIYA0SKSO1ieslAjv04C+gGYB+PAI54Ij7VTvS4APxDG+xXCAv0Y0zPGB2aqlQLuXP0UZyIRNqPg4BJwDZgBdY9ngFuBhbbj5fYz7GPLzfG6LKX6iRff+g1wepXaOC/xsT+8ewpOMbeQ66bmZRS9XPnlUISsEJEfgC+A5YaYz4A7gPmiEgGVp/BArv+AiDGLp8DzHVjbKq96jfFmt2cu9FllYkDamc369WCUs3ltnsiGGN+AIbXU74Hq3/h9PJy4Bp3xaM6iN6XWPuMz6HLsHqrdIsOpl9CGJ9vy+P2cT09GJxS7Z/OaFbtS2gcJA6F3csbrDZxQDzf7Suk+LjOblaqOTQpqPan90TIXAPlJS6rTByQQHWNYeVOHYWkVHNoUlDtT6+JUFMF+1a5rDKsWyQxIf66aqpSzaRJQbU/3UZbQ1Mzlrms4vARLu4fz8od+VTq7GalmkyTgmp/fP0hZZzV2dzA0NRJA+IpKa8ifZ/OblaqqTQpqPap90Qo2g9H9risMq5PHP4OHx2aqlQzaFJQ7VOvCda+gSakkABfxvSyZjfrPEilmkaTgmqfYnpBVArsdp0UAC4dlMC+w8fZfrDUM3Ep1c5pUlDtV6+JsHcVVJ1wWWXywERE4OPNBz0YmFLtlyYF1X71ngiVxyBztcsqcWEBjEqJ5pPNuR4MTKn2S5OCar9SxoE4rHssNGDK4ER25h0lI/9og/WUUpoUVHsWGA5dRzR4fwWAywYnAejVglJNoElBtW+pF0L2Oqhw3ZGcGBHIiO6RfLRJ+xWUaowmBdW+pV4Iphr2f9tgtalDktiaW8L+w3qPBaUaoklBtW/dRoPDH/Y23K9w6aBE4CxHIZ04BsXZUJQJeVutiXM1uoSG6ljcdj8FpTzCL8hKDI30K3SLDmZocgQfbz7IHRf1av7nbF0C786GqrLTPj8YYvtYcyZ8g6CmEmL7QcIg6yomMLz5n6WUF2lSUO1f6oWw4k9w/AgER7usdtngRB7/ZAfZRWV0jQxq+vtnfgdv3wqJQ2DETeDjgIAwa+nuA99afRpbFzf+PgAX/houug8cflBVAb4BTY9DKQ+Q9jz9Py0tzaSnp3s7DOVtB1bDS5fCtf+CgdNcVtt76BgX/3klv79iILeNTW3ae1dVwHPjoPI43LEKgqJc18tcC9//y7pdaCNXLmcYNdvaonsBxlroz6F/syn3EJF1xpi0+o7p/zrV/nUdCX4h1i/iBpJCamwIA5LC+c/GnKYnhVV/hUM74Ia3XScEsP7iTx1nbc6MgfIi2LUU3v2pNQvbVMOelafWW/u8tTmL6w/VJ2DMXTDsBvAPblrMSp0FtyUFEekGvAIkAjXA88aYp0QkGngDSAH2AdcaYwpFRICngKnAceAWY8x6d8WnOhCHH/Q4v9HOZoCrhnfhTx9tJyO/lN7xYQ1XPrIXvnoSBs+APpe0LDYRK5kMvdbaalWdgLxNcHg3LL7b+uV/uoLt1v6jX1lbrdF3QFU5DLkGUsa2LC6lXHBb85GIJAFJxpj1IhIGrAOuBG4Bjhhj5onIXCDKGHOfiEwF7sVKCqOBp4wxoxv6DG0+UnW+fhqW/h7mbIfwJJfVCkorGPPoMm4fl8r9UwY0/J5v3mT9hX/vOgjv0soBN6Ao0+q3+PZvsOY56y5zTTFylpUkBl1lNWcVZ0FcX/fGqtolrzQfGWNygVz7camIbAO6AtOB8Xa1hcBK4D67/BVjZanVIhIpIkn2+yjVsNQLrf2+Vaf+RX6auLAALu4Xz7vrs/n15H74OlyMys7banUeX/hrzyYEgMhu1v7SR6yt1oljULgPVj8L37965uvWvWxt79x2avlF90HvSZB0jnZsq0Z5pE9BRFKA4cAaIKH2F70xJldE4u1qXYFMp5dl2WWaFFTjEodAYKS1DlIDSQHgmrRkPt+Wxxc7C5g4IKH+Sl89afVTjLnLDcG2kH+INdR1+t+srdaRPbDlfdj0FuRvPfN1XzxmbbUCwqGiBHpcAGPnQJ9J7o9dtRtuTwoiEgq8A/zSGFNidR3UX7WesjPatkRkNjAboHv37q0VpmrvfBxW08m+xkf9TOgfT0yIP2+lZ9WfFI7sgc1vWwmhgSGubUZ0Txg3x9pqHc2H/V9D7g9QkmNdQZVkW8cqSqz9/q+tzdmwG+HCX0F0EzviVYfj1qQgIn5YCeE1Y8y7dnFebbOQ3e+Qb5dnAd2cXp4M5Jz+nsaY54HnwepTcFvwqv1JvQi2f2B1EDfwS83P4cNVw7uy8Nt9HD5aQUzoaU0qX80HHz84/173xutOofFW38Kgq04tLy+Bgh2wwMXVwYZXra3f5ZA2y+oITxpqdeSrTsGdo48EWABsM8b81enQEuBmYJ69X+xUfo+ILMLqaC7W/gTVLD0vsvZ7v2z0L91r0rrx4ld7eX/DacNTi7Nhw7+tSWphiW4M1ksCw6HbufBgsfXcGOvKKP0l+PaZk/V2fGhtzmL7WaOwRtysHdgdmDtHH40FVgGbsIakAvwWq1/hTaA7cAC4xhhzxE4izwCXYQ1JnWWMaXBokY4+UqcwBv7Sz7rPwowFjVaf9sxXnKiq4ZNfXniycMnPrU7cn38PUT3cGGwbV1EK2evhFdfzPvDxtZLn1D9bzXeq3fDW6KOvqL+fAGBiPfUNcLe74lGdgIg1CmnPF1aCcN1/BcCMkcn8YfEWtuaUMDApDNa/AusXwnn3dO6EANYyHj0vgj8WQVkhFO2HNc/Dxn+frFNTZV1hpL9kPb/8r9aEu5QLvBOzahW6zIXqWNa/AkvuhbtWQ3zD8xAKj51g9J8+45H+B7im/B3ITofu58FNS8DX30MBt0M1NfDl49YVVXGm63q/2WtN3GskOSvP02UuVOeRavcrZCxrOClUVRC14w2+DH6cxN2ZmMgeyBXzYfh/6ZpDjfHxgfFzrQ2g6AB8/xp8Me/Ueo/bfTUTHrCuvvyasQih8hq9UlAdz3PjrDbu2SvPPFZeYk3wWv13KM2lJHIgv82fyIwb72L8QA9PUuuIampg5yfwn1/AsfwzjwfHwvn3wNj/9nxsqk5DVwp6kx3V8Qy9FnK+h/ztJ8tK8+DzB+HJwbD0DxDbF/7rPQLv/oqvAy/k7Q16q85W4eMD/afCr3dZI5x+uQniB548fvyQ9e/wYIS1vfNTa6a2ajP0Oll1PEOvhxWPwopHYNKD8M3TsOF1a9G5gdPhgl9A1xEA+APTzunC699lUnjsBFEh2pfQqiK7w132rVKPH4E3bjx1wtymN60NrFFMQ66BoEjPx6nqaPOR6pi+fAKWP2w9dgTAsJ9Yk9Fizrzr2vaDJVw2fxX3T+nPz1pyVzbVfJXl8M+p1g2KXLn2FRgwTTuq3UA7mlXnM3YO1FSDOOyJaC7WOAL6J4YzOjWaf63ez+3jeuLw0V9CbucXCD9dbj0+fgRyN1h758X83rzJ2icMgZsWQ0iM5+PshLRPQXVMPg5rdMxFv24wIdS65fwUsgrLWLG9ns5R5V7B0dBrAgyZAXMzYfDVpx7P2wRP9LT6IPZ95Z0YOxFNCkoBlwxMICkikIXf7vN2KJ1bYDjMeMnqpL4/68xVav95uZUc/nUVFO73TowdnCYFpQBfhw83junBql2H2JlX6u1wFFizqi971EoQ9+2D839+8tju5fDU0JOjmPK3W3ezU2dNk4JStpmjuhMW4MsTn+7wdijqdEFRMPn/WQliznbrFqnOnh0ND8fB30Zbd65rxwNovE2TglK26BB/7hjfi6Vb81i794i3w1GuhCdZCx4+WAy/2gWhTqvZFmyH+YPhoUhrAl1pnvfibKd0SKpSTspOVHPxn1eSEBHI+3edTwM3hVJtTcFO+Nu59R/rdzlcuxAcfp6NqY3SGc1KNVGQv4M5k/uyMbOIjzbpLOd2Ja6vdfXwxyK457Q/Fnd8CP8v1up/WP0cVJZ5J8Z2QJOCUqe5ekQy/RPDePzT7Zyoqmn8BaptEYHYPlaCeCDfWozP2Sf3wSOJ1pInGxdZ6zWpOtp8pFQ9Vu7I55aXv+PXl/bj7ot7ezsc1Vryt8EXj8GW904t9w+1JtPF9fNOXB7WUPORJgWl6mGM4Z5/f8/Hm3N55icjmDokydshqdaW+V3996oOiYc7VnXM27HaNCko1QLHKqq46aW1fH+gkPunDOC2san46BIYHU9VhXVf79dOG+Ya08eaK3HLh+Af7J3Y3ESTglItVHaiml++8T2fbsljYFI4lwxMoHd8KD3jQugZG0qQv96buEMpzoanh1kr6p4u6Ry4/nWI6Or5uFqZJgWlzoIxhve+z+aFVXvZfrDklHlRXSIC6RlXmyRC6h53iQjSq4r2LuNzePVq18e7DIfrXoWIZM/F1Eq8khRE5CXgCiDfGDPYLosG3gBSgH3AtcaYQrEGgz8FTAWOA7cYY9Y39hmaFJSnlVdWs/fQMfYUHGNPwVH2HLL3Bccoraiqqxfo50NKTAi9ahOGfWXRMy6EsEAdK9+uGAOHM+ClS+H44frr9L8C+k2BQT9uF01N3koKFwJHgVecksLjwBFjzDwRmQtEGWPuE5GpwL1YSWE08JQxZnRjn6FJQbUVxhgKjlbYyeLUhJFZWEZ1zcnvWVxYQN1VRS+nhJEcFYSvQ0eJt3llRXBgNbw7GyqKzzweGAETfg8jZ7XZ+317rflIRFKAD5ySwg5gvDEmV0SSgJXGmH4i8g/78eun12vo/TUpqPbgRFUNB44cY3c9CaPweGVdPT+H0D06uK4Jqlds7VVGKNF6R7i2yxjrfhBrnof9X0HRgZPHBk6HYTdC38nei68ebekmOwm1v+jtxBBvl3cFMp3qZdllZyQFEZkNzAbo3r27e6NVqhX4+/rQOz6M3vFhZxwrPHaCPYes5ifnpqgvdhRwovrkpKrIYL9T+ix6xlpXGd1jggnw1c5urxKx+heu+rv1fO8q+Pg+KM6ErYutDaDnxXDVc21+qKunrxSKjDGRTscLjTFRIvIh8Kgx5iu7fBnwG2NMA/fq0ysF1XFVVdeQXVTGnoJj7D6t7yK/tKKuno9At+jgMxJG7/hQYkP9de0mb6qpsWZPr33+zGOhiXDrxxDd0/Nx0bauFPJEJMmp+aj2NldZQDeneslAjodjU6rN8HX40CMmhB4xIVzcP/6UY6Xllad0du+2H3+75zDllSevLiKC/OgdH0qfeCtJ1G46MspDfHxg6hPWlrkW3rkdiuwbAx09CE8Ptzqoe0+EPpe2maGunr5SeAI47NTRHG2M+Y2IXA7cw8mO5qeNMaMae3+9UlDqpJoaQ25JOXsKjpKRb2278o+yO/8oh4+dHHcf7O+gV9zJJNEvIYx+iWF0jdRk4RGHd8Nnv4eDmwBjNTPVGnIt/Gg++Ie4NQRvjT56HRgPxAJ5wB+B94E3ge7AAeAaY8wRe0jqM8BlWENSZxljGv1tr0lBqaY5cuyEnSRK6xJGRv5RcovL6+oE+vnYVxZhdcmib0IYyVGaLNzGGMheDy9OOPPYj1+APpMhKPLMY2dJJ68ppepVWl7JzrxSduYdZVeelTR25R3lYMmpyaJXXCh9E8LqmqP6JITRPToYhyaL1pO3BV6aUv8w1wkPQPK5ENcfQhOszu2zoElBKdUsJeWV7Mo7ys680rpmqIy8UnKcriwCfGuThZUk+iaE0U+vLM6eMbBnBbw1C8qL6q8z4iaY9BAER7foIzQpKKVaRWl5pZUk7KsK6wrj1GQR5OeoSxT9EsLomxhG/8Qw4sMCdDRUS5SXwKFdsPMT+PLxk+WTH4bz723RW2pSUEq5VWl5JbvyrQSx46B1hbEjr5QCp+GzUcF+9E8Mp39SGAPsfZ/4MF1UsDkqjoKpsTqn4we2uBmpLQ1JVUp1QGGBfozoHsWI7lGnlB85doIdB0vZmVfK9oMlbMstZdHaTMoqqwFrnkVKTAj9k8KshJEYxoCkcB0J5UpAqLUPHOS2j9CkoJRym+gQf87rFcN5vWLqympqDAeOHK9LEtsPlrA1p+SUe2KHBvjSN8EaMls7fLZPfBhdo4K0c9vNtPlIKdUmHKuosq8oStmeW8K2g6XsKTjKoaMn51j4+/qQEhNMamwIqbGh9IwNIdVetjw6RGdwN5U2Hyml2ryQAF+Gd49i+GlNUMXHK8kosEZB1a4RtbvgGMu351NZffKP2vBAX1Lj7EThtPWMCyHYX3/VNZWeKaVUmxYR7MfIHtGM7HHq8Muq6hpyisrrFhTce8ja1u49wnvfZ59SNzE80EoS9lVFamwIyVHBdI0KIjRAfw0607OhlGqXfB0+dI8JpntMMOP7nXqs7EQ1+48cY2/d6rPH2HvoKB9vyj1luXKwrjC6RAaRHBVEl8iTW9fIQLpEBhEfFtip+jE0KSilOpwgf4c9min8jGOFx06w9/AxsgrLyCk6uWUXlfPdvkKKy05NGr4+QmKElSCSIgKJDvEnMsifyGA/IoL8iLD3kUH28yC/dn2zJE0KSqlOJSrEn6gQ/zOGz9YqLa8kt7ic7CLnpFFOdmEZ3x8oovDYiVNuvVqf0ABfIoL8CA/yIyzAl9BAX0LtfVigr1UW4EtIgC9B/g6C/KwtwN4H+TsI9PMhyM9BoJ+DAF8fj3Wia1JQSiknYYF+hAX60TfhzJsi1aqsrqG4rPLkdtzaFx0/QZFTeUlZJUcrqsgvLWdPQRVHK6ooLa+ioqrG5XvXRwQCfR11CSTAz4dfTurLtHO6nO2PewZNCkop1Ux+Dh9iQwOIDQ1o0etPVNVwrMJKEuWV1ZRVVlN2wtqXV9acUlZeVU25faz2eFllNVHBfq38U1k0KSillIf5+/rg72s1Y7U17bc3RCmlVKvTpKCUUqqOJgWllFJ1NCkopZSqo0lBKaVUHU0KSiml6mhSUEopVUeTglJKqTrt+iY7IlIA7G/hy2OBQ60YjjtojGevrccHbT/Gth4faIzN1cMYE1ffgXadFM6GiKS7uvNQW6Exnr22Hh+0/RjbenygMbYmbT5SSilVR5OCUkqpOp05KTzv7QCaQGM8e209Pmj7Mbb1+EBjbDWdtk9BKaXUmTrzlYJSSqnTaFJQSilVp1MmBRG5TER2iEiGiMz1UgzdRGSFiGwTkS0i8gu7PFpElorILnsfZZeLiDxtx/yDiIzwYKwOEfleRD6wn6eKyBo7xjdExN8uD7CfZ9jHUzwQW6SIvC0i2+1zeV5bO4ci8t/2v/FmEXldRAK9fQ5F5CURyReRzU5lzT5vInKzXX+XiNzs5viesP+dfxCR90Qk0unY/XZ8O0TkUqdyt33X64vR6divRMSISKz93OPnsMWMMZ1qAxzAbqAn4A9sBAZ6IY4kYIT9OAzYCQwEHgfm2uVzgcfsx1OBjwEBxgBrPBjrHODfwAf28zeB6+3HzwF32o/vAp6zH18PvOGB2BYCt9uP/YHItnQOga7AXiDI6dzd4u1zCFwIjAA2O5U167wB0cAeex9lP45yY3yTAV/78WNO8Q20v8cBQKr9/Xa4+7teX4x2eTfgU6yJtbHeOoct/rm8+eFe+YHhPOBTp+f3A/e3gbgWA5cAO4AkuywJ2GE//gcw06l+XT03x5UMLAMmAB/Y/6kPOX05686n/UU4z37sa9cTN8YWbv/CldPK28w5xEoKmfaX3tc+h5e2hXMIpJz2S7dZ5w2YCfzDqfyUeq0d32nHrgJesx+f8h2uPYee+K7XFyPwNnAOsI+TScEr57AlW2dsPqr9ktbKssu8xm4iGA6sARKMMbkA9j7eruatuOcDvwFq7OcxQJExpqqeOOpitI8X2/XdpSdQALxsN2+9KCIhtKFzaIzJBv4MHABysc7JOtrOOXTW3PPmze/SrVh/edNAHB6PT0SmAdnGmI2nHWozMTamMyYFqafMa+NyRSQUeAf4pTGmpKGq9ZS5NW4RuQLIN8asa2Icno7RF+vy/e/GmOHAMaxmD1e8cQ6jgOlYzRpdgBBgSgNxtKn/nzZXMXklVhH5HVAFvFZb5CIOj8YnIsHA74A/1HfYRSxt7t+7MyaFLKw2v1rJQI43AhERP6yE8Jox5l27OE9EkuzjSUC+Xe6NuC8AponIPmARVhPSfCBSRHzriaMuRvt4BHDEjfFlAVnGmDX287exkkRbOoeTgL3GmAJjTCXwLnA+beccOmvuefP4+bQ7Yq8AbjB2e0sbiq8XVvLfaH9nkoH1IpLYhmJsVGdMCt8BfezRH/5YnXlLPB2EiAiwANhmjPmr06ElQO0IhJux+hpqy2+yRzGMAYprL/XdxRhzvzEm2RiTgnWelhtjbgBWADNcxFgb+wy7vtv+6jHGHAQyRaSfXTQR2EobOodYzUZjRCTY/jevjbFNnMPTNPe8fQpMFpEo+4posl3mFiJyGXAfMM0Yc/y0uK+3R26lAn2AtXj4u26M2WSMiTfGpNjfmSyswSQHaSPnsEm82aHhrQ1rJMBOrJEJv/NSDGOxLhN/ADbY21Ss9uNlwC57H23XF+BvdsybgDQPxzuek6OPemJ96TKAt4AAuzzQfp5hH+/pgbiGAen2eXwfawRHmzqHwEPAdmAz8C+sUTJePYfA61h9HJVYv7xua8l5w2rbz7C3WW6OLwOr/b32+/KcU/3f2fHtAKY4lbvtu15fjKcd38fJjmaPn8OWbrrMhVJKqTqdsflIKaWUC5oUlFJK1dGkoJRSqo4mBaWUUnU0KSillKqjSUGpBohItYhscNpabaVNEUmpb4VNpbzJt/EqSnVqZcaYYd4OQilP0SsFpVpARPaJyGMistbeetvlPURkmb1m/jIR6W6XJ9j3ANhob+fbb+UQkRfEut/CZyIS5LUfSik0KSjVmKDTmo+uczpWYowZBTyDtSYU9uNXjDFDsRZse9oufxr4whhzDtb6TFvs8j7A34wxg4Ai4Go3/zxKNUhnNCvVABE5aowJrad8HzDBGLPHXtjwoDEmRkQOYd2ToNIuzzXGxIpIAZBsjKlweo8UYKkxpo/9/D7AzxjzsPt/MqXqp1cKSrWccfHYVZ36VDg9rkb7+ZSXaVJQquWuc9p/az/+Bms1ToAbgK/sx8uAO6HuntfhngpSqebQv0qUaliQiGxwev6JMaZ2WGqAiKzB+uNqpl32c+AlEfk11l3hZtnlvwCeF5HbsK4I7sRaYVOpNkX7FJRqAbtPIc0Yc8jbsSjVmrT5SCmlVB29UlBKKVVHrxSUUkrV0aSglFKqjiYFpZRSdTQpKKWUqqNJQSmlVJ3/D3MwXOIjkzjmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"modelden.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"modelden.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
