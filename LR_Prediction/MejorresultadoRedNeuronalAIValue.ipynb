{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('InterpolatedWithCAPEX2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>D REVENUE</th>\n",
       "      <th>U CR</th>\n",
       "      <th>D OE</th>\n",
       "      <th>D NOI</th>\n",
       "      <th>U CAPEX</th>\n",
       "      <th>U CWK</th>\n",
       "      <th>D FCF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>1884.372544</td>\n",
       "      <td>976.202014</td>\n",
       "      <td>475.249997</td>\n",
       "      <td>757.519678</td>\n",
       "      <td>207.477947</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>856.600959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1884.566826</td>\n",
       "      <td>983.762225</td>\n",
       "      <td>485.004015</td>\n",
       "      <td>734.017979</td>\n",
       "      <td>207.303532</td>\n",
       "      <td>3638.472896</td>\n",
       "      <td>810.859727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1884.761107</td>\n",
       "      <td>991.322435</td>\n",
       "      <td>494.758033</td>\n",
       "      <td>710.516281</td>\n",
       "      <td>207.129117</td>\n",
       "      <td>3676.945791</td>\n",
       "      <td>765.118495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1884.955389</td>\n",
       "      <td>998.882646</td>\n",
       "      <td>504.512051</td>\n",
       "      <td>687.014582</td>\n",
       "      <td>206.954702</td>\n",
       "      <td>3715.418687</td>\n",
       "      <td>719.377263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1880.767673</td>\n",
       "      <td>1006.377690</td>\n",
       "      <td>481.542613</td>\n",
       "      <td>511.922217</td>\n",
       "      <td>207.283705</td>\n",
       "      <td>3792.839197</td>\n",
       "      <td>732.414605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>1785.193900</td>\n",
       "      <td>972.274049</td>\n",
       "      <td>461.469501</td>\n",
       "      <td>414.377788</td>\n",
       "      <td>98.125058</td>\n",
       "      <td>2795.231076</td>\n",
       "      <td>733.164793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2018-09-30</td>\n",
       "      <td>1730.706270</td>\n",
       "      <td>953.165883</td>\n",
       "      <td>488.056175</td>\n",
       "      <td>424.307526</td>\n",
       "      <td>95.323030</td>\n",
       "      <td>2858.452805</td>\n",
       "      <td>729.764211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>1667.921494</td>\n",
       "      <td>856.900384</td>\n",
       "      <td>471.261535</td>\n",
       "      <td>434.874583</td>\n",
       "      <td>92.144220</td>\n",
       "      <td>2849.606546</td>\n",
       "      <td>737.713784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>1605.136718</td>\n",
       "      <td>760.634885</td>\n",
       "      <td>454.466895</td>\n",
       "      <td>445.441639</td>\n",
       "      <td>88.965411</td>\n",
       "      <td>2840.760287</td>\n",
       "      <td>745.663358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>1542.351942</td>\n",
       "      <td>664.369385</td>\n",
       "      <td>437.672255</td>\n",
       "      <td>456.008696</td>\n",
       "      <td>85.786601</td>\n",
       "      <td>2831.914028</td>\n",
       "      <td>753.612931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dates    D REVENUE         U CR        D OE       D NOI     U CAPEX  \\\n",
       "0    2009-12-31  1884.372544   976.202014  475.249997  757.519678  207.477947   \n",
       "1    2010-01-31  1884.566826   983.762225  485.004015  734.017979  207.303532   \n",
       "2    2010-02-28  1884.761107   991.322435  494.758033  710.516281  207.129117   \n",
       "3    2010-03-31  1884.955389   998.882646  504.512051  687.014582  206.954702   \n",
       "4    2010-04-30  1880.767673  1006.377690  481.542613  511.922217  207.283705   \n",
       "..          ...          ...          ...         ...         ...         ...   \n",
       "104  2018-08-31  1785.193900   972.274049  461.469501  414.377788   98.125058   \n",
       "105  2018-09-30  1730.706270   953.165883  488.056175  424.307526   95.323030   \n",
       "106  2018-10-31  1667.921494   856.900384  471.261535  434.874583   92.144220   \n",
       "107  2018-11-30  1605.136718   760.634885  454.466895  445.441639   88.965411   \n",
       "108  2018-12-31  1542.351942   664.369385  437.672255  456.008696   85.786601   \n",
       "\n",
       "           U CWK       D FCF  \n",
       "0    3600.000000  856.600959  \n",
       "1    3638.472896  810.859727  \n",
       "2    3676.945791  765.118495  \n",
       "3    3715.418687  719.377263  \n",
       "4    3792.839197  732.414605  \n",
       "..           ...         ...  \n",
       "104  2795.231076  733.164793  \n",
       "105  2858.452805  729.764211  \n",
       "106  2849.606546  737.713784  \n",
       "107  2840.760287  745.663358  \n",
       "108  2831.914028  753.612931  \n",
       "\n",
       "[109 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753.612931\n",
      "-0.4658808455080132\n"
     ]
    }
   ],
   "source": [
    "max = {'D REVENUE':df['D REVENUE'].max(), 'U CR':df['U CR'].max(), 'D OE':df['D OE'].max(), \n",
    "       'D NOI':df['D NOI'].max(),'U CAPEX':df['U CAPEX'].max(), 'U CWK':df['U CWK'].max()} \n",
    "min = {'D REVENUE':df['D REVENUE'].min(), 'U CR':df['U CR'].min(), 'D OE':df['D OE'].min(), \n",
    "       'D NOI':df['D NOI'].min(),'U CAPEX':df['U CAPEX'].min(), 'U CWK':df['U CWK'].min()} \n",
    "filas, columnas = df.count()-1, len(df.columns)-1\n",
    "dataset = df.values\n",
    "DFCF = dataset[filas, columnas][1]\n",
    "print(DFCF)\n",
    "x1 = df['D REVENUE'].corr(df['D FCF']) \n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2009-12-31', 1884.372544, 976.2020142, 475.24999739999987,\n",
       "        757.519678, 207.4779469, 3600.0, 856.6009594000002],\n",
       "       ['2010-01-31', 1884.5668256666668, 983.7622247333335, 485.0040154,\n",
       "        734.0179793333333, 207.30353190000002, 3638.472895666667,\n",
       "        810.8597272333334],\n",
       "       ['2010-02-28', 1884.761107333333, 991.3224352666666,\n",
       "        494.75803339999993, 710.5162806666667, 207.12911689999999,\n",
       "        3676.945791333333, 765.1184950666667],\n",
       "       ['2010-03-31', 1884.955389, 998.8826458, 504.5120514, 687.014582,\n",
       "        206.9547019, 3715.418687, 719.3772629],\n",
       "       ['2010-04-30', 1880.767673333333, 1006.3776901999997,\n",
       "        481.5426128333333, 511.92221730000006, 207.28370463333331,\n",
       "        3792.8391966666654, 732.4146046],\n",
       "       ['2010-05-31', 1876.5799576666668, 1013.8727346000001,\n",
       "        458.5731742666666, 336.8298526000001, 207.61270736666665,\n",
       "        3870.2597063333333, 745.4519462999999],\n",
       "       ['2010-06-30', 1872.392242, 1021.367779, 435.6037357, 161.7374879,\n",
       "        207.9417101, 3947.680216, 758.489288],\n",
       "       ['2010-07-31', 1853.240114, 1015.1698886666666, 421.8261523,\n",
       "        187.6358415, 207.7144804333333, 3965.748594666666,\n",
       "        756.6899263333332],\n",
       "       ['2010-08-31', 1834.087986, 1008.9719983333333, 408.0485689,\n",
       "        213.5341951, 207.48725076666665, 3983.816973333333,\n",
       "        754.8905646666667],\n",
       "       ['2010-09-30', 1814.935858, 1002.774108, 394.2709855, 239.4325487,\n",
       "        207.2600211, 4001.885352, 753.0912030000002],\n",
       "       ['2010-10-31', 1851.320774, 1008.5464486666666, 411.1460836666666,\n",
       "        246.73378476666667, 212.21753106666665, 4093.646761333333,\n",
       "        715.3372166666667],\n",
       "       ['2010-11-30', 1887.70569, 1014.3187893333335, 428.0211818333333,\n",
       "        254.03502083333333, 217.1750410333333, 4185.408170666667,\n",
       "        677.5832303333334],\n",
       "       ['2010-12-31', 1924.090606, 1020.09113, 444.89628, 261.3362569,\n",
       "        222.132551, 4277.16958, 639.829244],\n",
       "       ['2011-01-31', 1840.631611, 1004.3123142000001, 439.1857453333333,\n",
       "        253.3520595, 221.3439679, 4255.687593333333, 625.4113584666667],\n",
       "       ['2011-02-28', 1757.172616, 988.5334983999999, 433.47521066666667,\n",
       "        245.36786209999997, 220.5553848, 4234.205606666666,\n",
       "        610.9934729333334],\n",
       "       ['2011-03-31', 1673.713621, 972.7546826, 427.764676, 237.3836647,\n",
       "        219.7668017, 4212.723620000002, 596.5755874],\n",
       "       ['2011-04-30', 1697.5040236666666, 943.8024453, 430.2420961,\n",
       "        237.04147333333333, 223.6841742333333, 4253.569522333334,\n",
       "        596.6860897999999],\n",
       "       ['2011-05-31', 1721.2944263333334, 914.850208, 432.71951619999993,\n",
       "        236.69928196666663, 227.60154676666667, 4294.415424666667,\n",
       "        596.7965922000002],\n",
       "       ['2011-06-30', 1745.084829, 885.8979707000002, 435.1969363,\n",
       "        236.3570906, 231.5189193, 4335.261327, 596.9070946],\n",
       "       ['2011-07-31', 1775.998593, 875.2949231666668, 444.6121419,\n",
       "        236.2388491, 227.72962546666668, 4329.278552666667,\n",
       "        628.3878588333333],\n",
       "       ['2011-08-31', 1806.912357, 864.6918756333333, 454.02734749999996,\n",
       "        236.1206076, 223.94033163333333, 4323.295778333333,\n",
       "        659.8686230666667],\n",
       "       ['2011-09-30', 1837.826121, 854.0888281, 463.44255310000005,\n",
       "        236.0023661, 220.1510378, 4317.313004, 691.3493873],\n",
       "       ['2011-10-31', 1885.0705423333332, 857.2014145333335,\n",
       "        470.0062231666667, 244.5463731, 212.89141523333333,\n",
       "        4309.167693666666, 690.0065109333333],\n",
       "       ['2011-11-30', 1932.314963666667, 860.3140009666665,\n",
       "        476.56989323333335, 253.0903801, 205.63179266666666,\n",
       "        4301.022383333333, 688.6636345666667],\n",
       "       ['2011-12-31', 1979.559385, 863.4265874, 483.13356330000005,\n",
       "        261.6343871, 198.3721701, 4292.877073, 687.3207582],\n",
       "       ['2012-01-31', 1910.3834399999998, 858.4811889666668,\n",
       "        483.72593293333335, 263.6296255, 198.3738698333333, 4291.272806,\n",
       "        711.6253066666667],\n",
       "       ['2012-02-29', 1841.2074949999999, 853.5357905333334,\n",
       "        484.3183025666667, 265.6248639, 198.37556956666668,\n",
       "        4289.668538999998, 735.9298551333335],\n",
       "       ['2012-03-31', 1772.03155, 848.5903921, 484.9106722, 267.6201023,\n",
       "        198.3772693, 4288.064272, 760.2344036000002],\n",
       "       ['2012-04-30', 1805.3226533333332, 852.7747259666667,\n",
       "        500.10971653333337, 262.53434903333334, 203.80507153333332,\n",
       "        4248.180273333333, 760.4925157666668],\n",
       "       ['2012-05-31', 1838.6137566666666, 856.9590598333334,\n",
       "        515.3087608666667, 257.44859576666664, 209.23287376666664,\n",
       "        4208.296274666666, 760.7506279333335],\n",
       "       ['2012-06-30', 1871.90486, 861.1433937, 530.5078052, 252.3628425,\n",
       "        214.660676, 4168.412276, 761.0087401000002],\n",
       "       ['2012-07-31', 1870.0189613333332, 888.6962120999999,\n",
       "        512.9147551333333, 274.75087963333334, 210.84279596666664,\n",
       "        4100.139316, 761.8369197333334],\n",
       "       ['2012-08-31', 1868.1330626666668, 916.2490305,\n",
       "        495.32170506666665, 297.1389167666666, 207.02491593333332,\n",
       "        4031.8663560000005, 762.6650993666667],\n",
       "       ['2012-09-30', 1866.247164, 943.8018489, 477.728655, 319.5269539,\n",
       "        203.2070359, 3963.593396, 763.493279],\n",
       "       ['2012-10-31', 1872.4550559999998, 871.4747433666665,\n",
       "        485.99178150000006, 323.89898873333334, 198.78250549999998,\n",
       "        3581.357899333333, 776.451154],\n",
       "       ['2012-11-30', 1878.6629480000001, 799.1476378333333, 494.254908,\n",
       "        328.27102356666666, 194.3579751, 3199.1224026666664, 789.409029],\n",
       "       ['2012-12-31', 1884.87084, 726.8205323, 502.5180345, 332.6430584,\n",
       "        189.9334447, 2816.886906, 802.366904],\n",
       "       ['2013-01-31', 1896.30791, 753.0427803333333, 514.5447965666667,\n",
       "        333.5149285333333, 192.01279616666667, 2893.339288,\n",
       "        793.3964250333332],\n",
       "       ['2013-02-28', 1907.7449800000002, 779.2650283666666,\n",
       "        526.5715586333333, 334.38679866666666, 194.09214763333333,\n",
       "        2969.79167, 784.4259460666666],\n",
       "       ['2013-03-31', 1919.18205, 805.4872763999998, 538.5983207,\n",
       "        335.2586688, 196.1714991, 3046.244052, 775.4554671],\n",
       "       ['2013-04-30', 1867.7861953333331, 783.4079046999999,\n",
       "        530.8571651000001, 335.86746453333336, 195.6670658333333,\n",
       "        3048.921046333333, 766.0225778666667],\n",
       "       ['2013-05-31', 1816.3903406666666, 761.328533, 523.1160095,\n",
       "        336.47626026666666, 195.16263256666667, 3051.5980406666667,\n",
       "        756.5896886333333],\n",
       "       ['2013-06-30', 1764.994486, 739.2491613, 515.3748539, 337.085056,\n",
       "        194.6581993, 3054.275035, 747.1567994],\n",
       "       ['2013-07-31', 1768.6169343333331, 757.7170030999998, 520.4034373,\n",
       "        329.2451542333333, 198.2425908, 2952.306149666667, 757.3171348],\n",
       "       ['2013-08-31', 1772.2393826666666, 776.1848449, 525.4320207000002,\n",
       "        321.40525246666664, 201.8269823, 2850.337264333333,\n",
       "        767.4774702000001],\n",
       "       ['2013-09-30', 1775.861831, 794.6526867, 530.4606041000002,\n",
       "        313.5653506999999, 205.4113738, 2748.368379, 777.6378056],\n",
       "       ['2013-10-31', 1781.312525333333, 813.7964820333333,\n",
       "        531.6057520333334, 320.4643399333333, 203.1795522,\n",
       "        2446.6776800000002, 776.7867944],\n",
       "       ['2013-11-30', 1786.7632196666668, 832.9402773666667,\n",
       "        532.7508999666667, 327.36332916666663, 200.9477306, 2144.986981,\n",
       "        775.9357832000001],\n",
       "       ['2013-12-31', 1792.2139140000004, 852.0840727, 533.8960479,\n",
       "        334.2623184, 198.715909, 1843.296282, 775.0847719999998],\n",
       "       ['2014-01-31', 1772.1792736666669, 861.7296213999998, 528.3798523,\n",
       "        342.6429393, 191.0202412333333, 1839.5208699999998,\n",
       "        774.5279383333333],\n",
       "       ['2014-02-28', 1752.144633333333, 871.3751701, 522.8636567,\n",
       "        351.0235602, 183.32457346666664, 1835.7454579999999,\n",
       "        773.9711046666665],\n",
       "       ['2014-03-31', 1732.109993, 881.0207187999998, 517.3474611,\n",
       "        359.4041811, 175.6289057, 1831.970046, 773.414271],\n",
       "       ['2014-04-30', 1721.8949903333335, 890.6855609999999,\n",
       "        507.92859023333335, 349.91219946666666, 168.82089786666666,\n",
       "        1848.738663666667, 760.1008118333333],\n",
       "       ['2014-05-31', 1711.6799876666666, 900.3504032,\n",
       "        498.50971936666673, 340.4202178333333, 162.01289003333332,\n",
       "        1865.507281333333, 746.7873526666666],\n",
       "       ['2014-06-30', 1701.464985, 910.0152454, 489.09084850000005,\n",
       "        330.9282362, 155.2048822, 1882.275899, 733.4738934999998],\n",
       "       ['2014-07-31', 1728.4231946666666, 914.2662125, 481.6856930666667,\n",
       "        331.95640223333334, 150.8917052333333, 1896.6301780000001,\n",
       "        727.0572944999999],\n",
       "       ['2014-08-31', 1755.3814043333334, 918.5171796,\n",
       "        474.28053763333327, 332.98456826666666, 146.57852826666667,\n",
       "        1910.984457, 720.6406955],\n",
       "       ['2014-09-30', 1782.339614, 922.7681467, 466.8753822,\n",
       "        334.0127343000001, 142.2653513, 1925.338736, 714.2240965],\n",
       "       ['2014-10-31', 1725.137324333333, 965.4992654666668,\n",
       "        478.28002593333326, 415.6834258, 141.41725303333334,\n",
       "        1946.0452103333328, 728.1806636666665],\n",
       "       ['2014-11-30', 1667.935034666667, 1008.2303842333333,\n",
       "        489.68466966666665, 497.35411730000004, 140.56915476666666,\n",
       "        1966.751684666667, 742.1372308333333],\n",
       "       ['2014-12-31', 1610.732745, 1050.961503, 501.0893134, 579.0248088,\n",
       "        139.7210565, 1987.458159, 756.093798],\n",
       "       ['2015-01-31', 1746.422448666667, 1054.429111, 508.5803035333333,\n",
       "        579.2034107000001, 137.07026573333334, 2005.468421,\n",
       "        771.8714318333333],\n",
       "       ['2015-02-28', 1882.112152333333, 1057.8967189999998,\n",
       "        516.0712936666666, 579.3820125999998, 134.41947496666666,\n",
       "        2023.478683, 787.6490656666667],\n",
       "       ['2015-03-31', 2017.801856, 1061.364327, 523.5622838,\n",
       "        579.5606144999998, 131.7686842, 2041.488945, 803.4266995],\n",
       "       ['2015-04-30', 2107.344116, 1096.44801, 526.1169451000002,\n",
       "        577.4851386666667, 131.0318083, 2038.723757, 790.2704643666667],\n",
       "       ['2015-05-31', 2196.8863760000004, 1131.531693, 528.6716064,\n",
       "        575.4096628333333, 130.2949324, 2035.9585690000001,\n",
       "        777.1142292333334],\n",
       "       ['2015-06-30', 2286.428636, 1166.615376, 531.2262677, 573.334187,\n",
       "        129.5580565, 2033.193381, 763.9579941000002],\n",
       "       ['2015-07-31', 2528.4880129999997, 1194.063472, 562.655131,\n",
       "        551.8155191666667, 128.9129271, 2038.7990923333327, 725.2341621],\n",
       "       ['2015-08-31', 2770.5473899999997, 1221.511568, 594.0839943000002,\n",
       "        530.2968513333334, 128.2677977, 2044.4048036666666, 686.5103301],\n",
       "       ['2015-09-30', 3012.606767, 1248.959664, 625.5128576000002,\n",
       "        508.7781835, 127.6226683, 2050.010515, 647.7864981],\n",
       "       ['2015-10-31', 2903.7366663333332, 1269.6701176666666,\n",
       "        641.4096041, 508.9514627, 117.39104201, 2031.2492326666666,\n",
       "        646.7970369666667],\n",
       "       ['2015-11-30', 2794.8665656666667, 1290.3805713333334,\n",
       "        657.3063506000002, 509.1247419, 107.15941572, 2012.4879503333332,\n",
       "        645.8075758333333],\n",
       "       ['2015-12-31', 2685.996465, 1311.091025, 673.2030971, 509.2980211,\n",
       "        96.92778943, 1993.726668, 644.8181147000001],\n",
       "       ['2016-01-31', 2834.140780666666, 1366.4444733333332,\n",
       "        674.0792667333334, 500.8613563, 382.7185262866667, 2146.965891,\n",
       "        645.8685558666667],\n",
       "       ['2016-02-29', 2982.285096333333, 1421.7979216666665,\n",
       "        674.9554363666666, 492.4246915, 668.5092631433333, 2300.205114,\n",
       "        646.9189970333333],\n",
       "       ['2016-03-31', 3130.429412, 1477.1513699999996, 675.831606,\n",
       "        483.9880267, 954.3, 2453.444337, 647.9694382],\n",
       "       ['2016-04-30', 3060.8571920000004, 1508.7480799999996,\n",
       "        653.6836567666667, 473.8452801, 669.9162089666665,\n",
       "        2448.972803333333, 637.6159774666667],\n",
       "       ['2016-05-31', 2991.284972, 1540.34479, 631.5357075333333,\n",
       "        463.7025335, 385.5324179333333, 2444.5012696666668,\n",
       "        627.2625167333333],\n",
       "       ['2016-06-30', 2921.712752, 1571.9415, 609.3877583, 453.5597869,\n",
       "        101.1486269, 2440.029736, 616.909056],\n",
       "       ['2016-07-31', 2979.422658, 1553.679082333333, 612.0762081333334,\n",
       "        473.88566743333337, 103.19915363333334, 2475.9393219999997,\n",
       "        608.7162381333333],\n",
       "       ['2016-08-31', 3037.132564, 1535.416664666667, 614.7646579666666,\n",
       "        494.2115479666666, 105.24968036666668, 2511.848908,\n",
       "        600.5234202666667],\n",
       "       ['2016-09-30', 3094.84247, 1517.1542470000004, 617.4531078,\n",
       "        514.5374285, 107.3002071, 2547.758494, 592.3306024],\n",
       "       ['2016-10-31', 2941.914112333333, 1493.7506856666669,\n",
       "        601.3888980666667, 497.1393087333333, 106.27214706666666,\n",
       "        2492.278597666667, 605.3801967666667],\n",
       "       ['2016-11-30', 2788.9857546666667, 1470.347124333333,\n",
       "        585.3246883333334, 479.7411889666667, 105.24408703333332,\n",
       "        2436.798701333333, 618.4297911333333],\n",
       "       ['2016-12-31', 2636.057397, 1446.943563, 569.2604786, 462.3430692,\n",
       "        104.216027, 2381.318805, 631.4793855],\n",
       "       ['2017-01-31', 2629.386708333333, 1421.7832626666666,\n",
       "        553.3237412999999, 463.41115033333335, 105.35134376666666,\n",
       "        2394.287123333333, 669.1150419666667],\n",
       "       ['2017-02-28', 2622.7160196666664, 1396.6229623333334,\n",
       "        537.3870039999998, 464.47923146666665, 106.48666053333334,\n",
       "        2407.2554416666667, 706.7506984333335],\n",
       "       ['2017-03-31', 2616.045331, 1371.462662, 521.4502666999998,\n",
       "        465.5473126, 107.6219773, 2420.2237600000008, 744.3863549],\n",
       "       ['2017-04-30', 2582.999953333333, 1385.6997396666666,\n",
       "        514.2855105000001, 456.63566276666666, 110.51347520000002,\n",
       "        2409.258488666667, 756.4180495333335],\n",
       "       ['2017-05-31', 2549.9545756666666, 1399.9368173333332,\n",
       "        507.12075430000004, 447.72401293333337, 113.4049731,\n",
       "        2398.293217333333, 768.4497441666666],\n",
       "       ['2017-06-30', 2516.909198, 1414.173895, 499.9559981, 438.8123631,\n",
       "        116.296471, 2387.327946, 780.4814388],\n",
       "       ['2017-07-31', 2529.782464, 1411.1019353333334,\n",
       "        495.39090156666674, 419.9090564333334, 113.3286994,\n",
       "        2482.2845416666664, 773.9692328666665],\n",
       "       ['2017-08-31', 2542.65573, 1408.0299756666666, 490.8258050333334,\n",
       "        401.00574976666667, 110.3609278, 2577.241137333333,\n",
       "        767.4570269333334],\n",
       "       ['2017-09-30', 2555.528996, 1404.958016, 486.2607085, 382.1024431,\n",
       "        107.3931562, 2672.197733, 760.944821],\n",
       "       ['2017-10-31', 2493.5604476666667, 1340.4287853333333,\n",
       "        485.59986143333333, 386.4377233, 106.82599553333333,\n",
       "        2640.667416333333, 748.7807167999999],\n",
       "       ['2017-11-30', 2431.591899333333, 1275.8995546666667,\n",
       "        484.9390143666667, 390.7730035, 106.25883486666667,\n",
       "        2609.1370996666665, 736.6166125999998],\n",
       "       ['2017-12-31', 2369.623351, 1211.370324, 484.27816730000006,\n",
       "        395.1082837, 105.6916742, 2577.606783, 724.4525083999998],\n",
       "       ['2018-01-31', 2307.4944116666666, 1212.1657916666666,\n",
       "        479.0572197666667, 392.9066318, 102.59903074333334,\n",
       "        2577.6159756666666, 729.9813198666666],\n",
       "       ['2018-02-28', 2245.365472333333, 1212.9612593333334,\n",
       "        473.83627223333326, 390.7049799, 99.50638728666668,\n",
       "        2577.625168333333, 735.5101313333333],\n",
       "       ['2018-03-31', 2183.236533, 1213.756727, 468.6153247, 388.503328,\n",
       "        96.41374383, 2577.634361, 741.0389428],\n",
       "       ['2018-04-30', 2086.8807423333333, 1146.0012783333334,\n",
       "        448.50893473333326, 390.50832306666666, 98.85220068666668,\n",
       "        2608.0187796666664, 740.6812809333335],\n",
       "       ['2018-05-31', 1990.5249516666665, 1078.2458296666666,\n",
       "        428.40254476666667, 392.51331813333326, 101.29065754333334,\n",
       "        2638.403198333333, 740.3236190666668],\n",
       "       ['2018-06-30', 1894.169161, 1010.490381, 408.2961548, 394.5183132,\n",
       "        103.7291144, 2668.787617, 739.9659572],\n",
       "       ['2018-07-31', 1839.681530666667, 991.3822151, 434.8828281333333,\n",
       "        404.4480508333333, 100.92708618333334, 2732.0093463333333,\n",
       "        736.5653751333334],\n",
       "       ['2018-08-31', 1785.1939003333332, 972.2740492,\n",
       "        461.46950146666666, 414.37778846666674, 98.12505796666667,\n",
       "        2795.231075666666, 733.1647930666667],\n",
       "       ['2018-09-30', 1730.70627, 953.1658833, 488.0561748, 424.3075261,\n",
       "        95.32302975, 2858.452805, 729.764211],\n",
       "       ['2018-10-31', 1667.921494, 856.9003839999998, 471.2615347666666,\n",
       "        434.87458276666666, 92.14422016333332, 2849.606546,\n",
       "        737.7137843333335],\n",
       "       ['2018-11-30', 1605.136718, 760.6348846999998, 454.4668947333333,\n",
       "        445.44163943333336, 88.96541057666668, 2840.760287,\n",
       "        745.6633576666667],\n",
       "       ['2018-12-31', 1542.351942, 664.3693853999998, 437.67225470000005,\n",
       "        456.0086961, 85.78660099, 2831.914028, 753.612931]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,1:5]\n",
    "Y = dataset[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21536771, 0.34358992, 0.28760773, 1.        ],\n",
       "       [0.21549004, 0.35192007, 0.32225043, 0.9605532 ],\n",
       "       [0.21561238, 0.36025022, 0.35689312, 0.92110641],\n",
       "       [0.21573472, 0.36858036, 0.39153581, 0.88165961],\n",
       "       [0.21309775, 0.37683871, 0.30995679, 0.58777307],\n",
       "       [0.21046078, 0.38509706, 0.22837778, 0.29388654],\n",
       "       [0.2078238 , 0.3933554 , 0.14679876, 0.        ],\n",
       "       [0.19576386, 0.38652631, 0.09786584, 0.0434695 ],\n",
       "       [0.18370391, 0.37969722, 0.04893292, 0.086939  ],\n",
       "       [0.17164397, 0.37286814, 0.        , 0.1304085 ],\n",
       "       [0.19455526, 0.37922834, 0.05993416, 0.14266337],\n",
       "       [0.21746656, 0.38558854, 0.11986831, 0.15491825],\n",
       "       [0.24037786, 0.39194874, 0.17980247, 0.16717312],\n",
       "       [0.18782438, 0.37456299, 0.15952074, 0.15377192],\n",
       "       [0.1352709 , 0.35717725, 0.13923902, 0.14037072],\n",
       "       [0.08271742, 0.33979151, 0.1189573 , 0.12696952],\n",
       "       [0.09769806, 0.30789075, 0.12775618, 0.12639516],\n",
       "       [0.11267869, 0.27598999, 0.13655507, 0.1258208 ],\n",
       "       [0.12765932, 0.24408924, 0.14535396, 0.12524645],\n",
       "       [0.14712547, 0.23240637, 0.17879331, 0.12504798],\n",
       "       [0.16659163, 0.2207235 , 0.21223267, 0.12484952],\n",
       "       [0.18605779, 0.20904063, 0.24567202, 0.12465105],\n",
       "       [0.21580723, 0.2124702 , 0.26898377, 0.13899188],\n",
       "       [0.24555667, 0.21589978, 0.29229552, 0.1533327 ],\n",
       "       [0.27530612, 0.21932935, 0.31560727, 0.16767352],\n",
       "       [0.23174656, 0.21388031, 0.31771115, 0.17102246],\n",
       "       [0.18818701, 0.20843127, 0.31981503, 0.1743714 ],\n",
       "       [0.14462746, 0.20298222, 0.32191891, 0.17772034],\n",
       "       [0.16559061, 0.20759269, 0.37590033, 0.16918408],\n",
       "       [0.18655375, 0.21220316, 0.42988176, 0.16064782],\n",
       "       [0.2075169 , 0.21681363, 0.48386319, 0.15211155],\n",
       "       [0.20632937, 0.24717245, 0.42137913, 0.18968911],\n",
       "       [0.20514183, 0.27753127, 0.35889507, 0.22726666],\n",
       "       [0.2039543 , 0.30789009, 0.29641102, 0.26484421],\n",
       "       [0.20786336, 0.22819714, 0.32575861, 0.27218252],\n",
       "       [0.21177242, 0.14850418, 0.3551062 , 0.27952084],\n",
       "       [0.21568148, 0.06881122, 0.38445379, 0.28685915],\n",
       "       [0.22288331, 0.09770397, 0.42716844, 0.28832255],\n",
       "       [0.23008515, 0.12659671, 0.46988309, 0.28978595],\n",
       "       [0.23728698, 0.15548945, 0.51259773, 0.29124936],\n",
       "       [0.20492341, 0.1311615 , 0.48510399, 0.2922712 ],\n",
       "       [0.17255984, 0.10683355, 0.45761024, 0.29329304],\n",
       "       [0.14019627, 0.08250559, 0.4301165 , 0.29431489],\n",
       "       [0.1424773 , 0.10285422, 0.44797618, 0.28115588],\n",
       "       [0.14475833, 0.12320284, 0.46583586, 0.26799687],\n",
       "       [0.14703936, 0.14355146, 0.48369555, 0.25483787],\n",
       "       [0.15047162, 0.16464487, 0.48776269, 0.26641758],\n",
       "       [0.15390388, 0.18573829, 0.49182984, 0.2779973 ],\n",
       "       [0.15733614, 0.2068317 , 0.49589698, 0.28957702],\n",
       "       [0.14472048, 0.21745956, 0.47630548, 0.3036436 ],\n",
       "       [0.13210482, 0.22808742, 0.45671398, 0.31771019],\n",
       "       [0.11948917, 0.23871528, 0.43712248, 0.33177677],\n",
       "       [0.11305686, 0.2493644 , 0.4036701 , 0.31584481],\n",
       "       [0.10662455, 0.26001352, 0.37021773, 0.29991284],\n",
       "       [0.10019224, 0.27066264, 0.33676536, 0.28398088],\n",
       "       [0.11716762, 0.27534652, 0.31046496, 0.28570662],\n",
       "       [0.13414299, 0.28003041, 0.28416457, 0.28743236],\n",
       "       [0.15111837, 0.2847143 , 0.25786417, 0.2891581 ],\n",
       "       [0.11509853, 0.33179719, 0.29836928, 0.42623956],\n",
       "       [0.07907869, 0.37888008, 0.33887439, 0.56332102],\n",
       "       [0.04305886, 0.42596297, 0.3793795 , 0.70040248],\n",
       "       [0.12850161, 0.42978373, 0.40598475, 0.70070225],\n",
       "       [0.21394436, 0.43360448, 0.43259   , 0.70100203],\n",
       "       [0.2993871 , 0.43742523, 0.45919525, 0.70130181],\n",
       "       [0.35577117, 0.47608186, 0.46826847, 0.69781819],\n",
       "       [0.41215523, 0.5147385 , 0.47734168, 0.69433458],\n",
       "       [0.46853929, 0.55339513, 0.4864149 , 0.69085096],\n",
       "       [0.62096219, 0.58363857, 0.59803869, 0.65473261],\n",
       "       [0.7733851 , 0.613882  , 0.70966248, 0.61861427],\n",
       "       [0.925808  , 0.64412543, 0.82128627, 0.58249592],\n",
       "       [0.85725335, 0.66694505, 0.87774568, 0.58278676],\n",
       "       [0.78869869, 0.68976468, 0.93420509, 0.58307761],\n",
       "       [0.72014404, 0.7125843 , 0.9906645 , 0.58336845],\n",
       "       [0.81342936, 0.77357499, 0.99377633, 0.5692078 ],\n",
       "       [0.90671468, 0.83456568, 0.99688817, 0.55504714],\n",
       "       [1.        , 0.89555637, 1.        , 0.54088649],\n",
       "       [0.95619092, 0.93037091, 0.92133861, 0.52386224],\n",
       "       [0.91238183, 0.96518546, 0.84267722, 0.50683799],\n",
       "       [0.86857275, 1.        , 0.76401584, 0.48981373],\n",
       "       [0.90491222, 0.97987772, 0.77356422, 0.52393003],\n",
       "       [0.9412517 , 0.95975545, 0.78311261, 0.55804632],\n",
       "       [0.97759118, 0.93963317, 0.792661  , 0.59216262],\n",
       "       [0.88129339, 0.91384617, 0.73560682, 0.56296047],\n",
       "       [0.78499559, 0.88805917, 0.67855264, 0.53375832],\n",
       "       [0.6886978 , 0.86227217, 0.62149846, 0.50455617],\n",
       "       [0.68449732, 0.83454953, 0.56489702, 0.50634891],\n",
       "       [0.68029684, 0.80682688, 0.50829558, 0.50814165],\n",
       "       [0.67609636, 0.77910423, 0.45169414, 0.50993438],\n",
       "       [0.65528794, 0.79479123, 0.42624755, 0.49497649],\n",
       "       [0.63447952, 0.81047822, 0.40080097, 0.48001859],\n",
       "       [0.61367111, 0.82616521, 0.37535438, 0.46506069],\n",
       "       [0.6217773 , 0.8227804 , 0.35914083, 0.43333214],\n",
       "       [0.6298835 , 0.81939559, 0.34292729, 0.40160358],\n",
       "       [0.63798969, 0.81601078, 0.32671374, 0.36987503],\n",
       "       [0.59896858, 0.74490984, 0.32436665, 0.37715165],\n",
       "       [0.55994747, 0.6738089 , 0.32201957, 0.38442827],\n",
       "       [0.52092636, 0.60270796, 0.31967248, 0.39170489],\n",
       "       [0.48180425, 0.60358444, 0.30112959, 0.38800949],\n",
       "       [0.44268214, 0.60446092, 0.2825867 , 0.38431409],\n",
       "       [0.40356003, 0.6053374 , 0.26404381, 0.3806187 ],\n",
       "       [0.34288554, 0.53068168, 0.19263329, 0.38398401],\n",
       "       [0.28221105, 0.45602596, 0.12122277, 0.38734933],\n",
       "       [0.22153656, 0.38137024, 0.04981225, 0.39071464],\n",
       "       [0.18722612, 0.36031608, 0.14423836, 0.40738137],\n",
       "       [0.15291569, 0.33926193, 0.23866447, 0.42404809],\n",
       "       [0.11860525, 0.31820777, 0.33309058, 0.44071482],\n",
       "       [0.07907017, 0.21213851, 0.27344218, 0.45845126],\n",
       "       [0.03953508, 0.10606926, 0.21379378, 0.4761877 ],\n",
       "       [0.        , 0.        , 0.15414538, 0.49392414]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87, 4) (11, 4) (11, 4) (87,) (11,) (11,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(6, activation='elu', input_shape=(4,)),\n",
    "    Dense(32, activation='elu'),\n",
    "    Dense(12, activation='elu'),\n",
    "    Dense(1, activation='elu'),\n",
    "])\n",
    "#softplus, selu, elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87 samples, validate on 11 samples\n",
      "Epoch 1/1500\n",
      "87/87 [==============================] - 0s 3ms/step - loss: 180.6163 - val_loss: 223.3942\n",
      "Epoch 2/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 180.4944 - val_loss: 223.2666\n",
      "Epoch 3/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 180.3705 - val_loss: 223.1367\n",
      "Epoch 4/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 180.2464 - val_loss: 223.0042\n",
      "Epoch 5/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 180.1182 - val_loss: 222.8689\n",
      "Epoch 6/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 179.9869 - val_loss: 222.7302\n",
      "Epoch 7/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 179.8520 - val_loss: 222.5877\n",
      "Epoch 8/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 179.7142 - val_loss: 222.4410\n",
      "Epoch 9/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 179.5708 - val_loss: 222.2897\n",
      "Epoch 10/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 179.4252 - val_loss: 222.1335\n",
      "Epoch 11/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 179.2720 - val_loss: 221.9720\n",
      "Epoch 12/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 179.1168 - val_loss: 221.8054\n",
      "Epoch 13/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 178.9546 - val_loss: 221.6337\n",
      "Epoch 14/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 178.7880 - val_loss: 221.4564\n",
      "Epoch 15/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 181.453 - 0s 92us/step - loss: 178.6170 - val_loss: 221.2734\n",
      "Epoch 16/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 178.4382 - val_loss: 221.0845\n",
      "Epoch 17/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 178.2547 - val_loss: 220.8893\n",
      "Epoch 18/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 178.0637 - val_loss: 220.6872\n",
      "Epoch 19/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 177.8673 - val_loss: 220.4778\n",
      "Epoch 20/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 177.6643 - val_loss: 220.2611\n",
      "Epoch 21/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 177.4495 - val_loss: 220.0367\n",
      "Epoch 22/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 177.2314 - val_loss: 219.8033\n",
      "Epoch 23/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 177.0038 - val_loss: 219.5612\n",
      "Epoch 24/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 176.7666 - val_loss: 219.3101\n",
      "Epoch 25/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 176.5259 - val_loss: 219.0499\n",
      "Epoch 26/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 176.2684 - val_loss: 218.7811\n",
      "Epoch 27/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 176.0087 - val_loss: 218.5027\n",
      "Epoch 28/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 175.7347 - val_loss: 218.2147\n",
      "Epoch 29/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 175.4549 - val_loss: 217.9163\n",
      "Epoch 30/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 175.1632 - val_loss: 217.6077\n",
      "Epoch 31/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 174.8589 - val_loss: 217.2881\n",
      "Epoch 32/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 174.5483 - val_loss: 216.9564\n",
      "Epoch 33/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 174.2258 - val_loss: 216.6134\n",
      "Epoch 34/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 173.8914 - val_loss: 216.2590\n",
      "Epoch 35/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 173.5456 - val_loss: 215.8932\n",
      "Epoch 36/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 173.1820 - val_loss: 215.5147\n",
      "Epoch 37/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 172.8188 - val_loss: 215.1223\n",
      "Epoch 38/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 172.4326 - val_loss: 214.7174\n",
      "Epoch 39/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 172.0366 - val_loss: 214.2986\n",
      "Epoch 40/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 171.6288 - val_loss: 213.8656\n",
      "Epoch 41/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 171.2029 - val_loss: 213.4181\n",
      "Epoch 42/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 170.7656 - val_loss: 212.9548\n",
      "Epoch 43/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 170.3121 - val_loss: 212.4755\n",
      "Epoch 44/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 169.8450 - val_loss: 211.9799\n",
      "Epoch 45/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 169.3640 - val_loss: 211.4683\n",
      "Epoch 46/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 168.8596 - val_loss: 210.9407\n",
      "Epoch 47/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 168.3457 - val_loss: 210.3952\n",
      "Epoch 48/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 167.8147 - val_loss: 209.8309\n",
      "Epoch 49/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 167.2647 - val_loss: 209.2507\n",
      "Epoch 50/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 170.129 - 0s 104us/step - loss: 166.6999 - val_loss: 208.6531\n",
      "Epoch 51/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 166.1150 - val_loss: 208.0377\n",
      "Epoch 52/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 165.5119 - val_loss: 207.4033\n",
      "Epoch 53/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 164.8943 - val_loss: 206.7487\n",
      "Epoch 54/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 164.2582 - val_loss: 206.0715\n",
      "Epoch 55/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 163.5903 - val_loss: 205.3743\n",
      "Epoch 56/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 162.9123 - val_loss: 204.6544\n",
      "Epoch 57/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 162.2119 - val_loss: 203.9129\n",
      "Epoch 58/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 161.4841 - val_loss: 203.1492\n",
      "Epoch 59/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 160.7420 - val_loss: 202.3620\n",
      "Epoch 60/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 159.9664 - val_loss: 201.5510\n",
      "Epoch 61/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 159.1754 - val_loss: 200.7136\n",
      "Epoch 62/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 158.3608 - val_loss: 199.8502\n",
      "Epoch 63/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 157.5227 - val_loss: 198.9619\n",
      "Epoch 64/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 156.6496 - val_loss: 198.0486\n",
      "Epoch 65/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 155.7593 - val_loss: 197.1076\n",
      "Epoch 66/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 154.8366 - val_loss: 196.1385\n",
      "Epoch 67/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 153.8954 - val_loss: 195.1400\n",
      "Epoch 68/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 152.9194 - val_loss: 194.1131\n",
      "Epoch 69/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 151.9191 - val_loss: 193.0569\n",
      "Epoch 70/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 150.8863 - val_loss: 191.9712\n",
      "Epoch 71/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 149.8282 - val_loss: 190.8546\n",
      "Epoch 72/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 148.7400 - val_loss: 189.7075\n",
      "Epoch 73/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 147.6071 - val_loss: 188.5280\n",
      "Epoch 74/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 146.4727 - val_loss: 187.3136\n",
      "Epoch 75/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 145.2780 - val_loss: 186.0683\n",
      "Epoch 76/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 144.0733 - val_loss: 184.7894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 142.8198 - val_loss: 183.4788\n",
      "Epoch 78/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 141.5325 - val_loss: 182.1327\n",
      "Epoch 79/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 140.2160 - val_loss: 180.7480\n",
      "Epoch 80/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 138.8625 - val_loss: 179.3238\n",
      "Epoch 81/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 137.4820 - val_loss: 177.8611\n",
      "Epoch 82/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 136.0587 - val_loss: 176.3632\n",
      "Epoch 83/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 134.5815 - val_loss: 174.8287\n",
      "Epoch 84/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 133.0939 - val_loss: 173.2530\n",
      "Epoch 85/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 131.5520 - val_loss: 171.6391\n",
      "Epoch 86/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 129.9768 - val_loss: 169.9851\n",
      "Epoch 87/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 128.3643 - val_loss: 168.2909\n",
      "Epoch 88/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 126.7026 - val_loss: 166.5556\n",
      "Epoch 89/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 125.0106 - val_loss: 164.7765\n",
      "Epoch 90/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 123.2690 - val_loss: 162.9538\n",
      "Epoch 91/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 121.5011 - val_loss: 161.0872\n",
      "Epoch 92/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 119.6559 - val_loss: 159.1774\n",
      "Epoch 93/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 123.447 - 0s 161us/step - loss: 117.8031 - val_loss: 157.2182\n",
      "Epoch 94/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 115.8901 - val_loss: 155.2140\n",
      "Epoch 95/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 113.9417 - val_loss: 153.1653\n",
      "Epoch 96/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 111.9275 - val_loss: 151.0722\n",
      "Epoch 97/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 109.8819 - val_loss: 148.9950\n",
      "Epoch 98/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 107.7984 - val_loss: 147.3060\n",
      "Epoch 99/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 105.6855 - val_loss: 145.5881\n",
      "Epoch 100/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 103.6163 - val_loss: 143.8569\n",
      "Epoch 101/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 101.6307 - val_loss: 142.1170\n",
      "Epoch 102/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 99.7548 - val_loss: 140.3705\n",
      "Epoch 103/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 98.0713 - val_loss: 138.6237\n",
      "Epoch 104/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 96.3782 - val_loss: 136.9002\n",
      "Epoch 105/1500\n",
      "87/87 [==============================] - 0s 218us/step - loss: 94.7268 - val_loss: 135.1938\n",
      "Epoch 106/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 93.1558 - val_loss: 133.5051\n",
      "Epoch 107/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 91.7063 - val_loss: 132.1997\n",
      "Epoch 108/1500\n",
      "87/87 [==============================] - 0s 127us/step - loss: 90.3267 - val_loss: 130.9610\n",
      "Epoch 109/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 89.2566 - val_loss: 129.7812\n",
      "Epoch 110/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 88.3687 - val_loss: 128.7756\n",
      "Epoch 111/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 87.5477 - val_loss: 128.0177\n",
      "Epoch 112/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 86.8667 - val_loss: 127.2805\n",
      "Epoch 113/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 86.1130 - val_loss: 126.5635\n",
      "Epoch 114/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 85.4730 - val_loss: 125.8590\n",
      "Epoch 115/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 84.9428 - val_loss: 125.4506\n",
      "Epoch 116/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 84.2598 - val_loss: 125.0702\n",
      "Epoch 117/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 83.7828 - val_loss: 124.6856\n",
      "Epoch 118/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 83.3510 - val_loss: 124.3130\n",
      "Epoch 119/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 82.9905 - val_loss: 123.9730\n",
      "Epoch 120/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 82.5410 - val_loss: 123.6605\n",
      "Epoch 121/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 82.2571 - val_loss: 123.3548\n",
      "Epoch 122/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 81.9905 - val_loss: 123.0677\n",
      "Epoch 123/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 81.6877 - val_loss: 122.8092\n",
      "Epoch 124/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 81.4735 - val_loss: 122.5620\n",
      "Epoch 125/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 81.2569 - val_loss: 122.3254\n",
      "Epoch 126/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 81.0763 - val_loss: 122.1026\n",
      "Epoch 127/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 80.8629 - val_loss: 121.8950\n",
      "Epoch 128/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 80.7216 - val_loss: 121.6872\n",
      "Epoch 129/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 80.5527 - val_loss: 121.4929\n",
      "Epoch 130/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 80.4160 - val_loss: 121.3141\n",
      "Epoch 131/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 80.2364 - val_loss: 121.1547\n",
      "Epoch 132/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 80.0967 - val_loss: 120.9961\n",
      "Epoch 133/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 79.9436 - val_loss: 120.8371\n",
      "Epoch 134/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 79.7995 - val_loss: 120.6709\n",
      "Epoch 135/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 79.6791 - val_loss: 120.4959\n",
      "Epoch 136/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 79.5511 - val_loss: 120.3227\n",
      "Epoch 137/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 79.4347 - val_loss: 120.1600\n",
      "Epoch 138/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 79.2700 - val_loss: 120.0099\n",
      "Epoch 139/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 79.1512 - val_loss: 119.8470\n",
      "Epoch 140/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 79.0178 - val_loss: 119.6791\n",
      "Epoch 141/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 78.8804 - val_loss: 119.5025\n",
      "Epoch 142/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 78.7266 - val_loss: 119.3138\n",
      "Epoch 143/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 78.5796 - val_loss: 119.1111\n",
      "Epoch 144/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 78.4606 - val_loss: 118.8999\n",
      "Epoch 145/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 78.3671 - val_loss: 118.7024\n",
      "Epoch 146/1500\n",
      "87/87 [==============================] - 0s 113us/step - loss: 78.1890 - val_loss: 118.5297\n",
      "Epoch 147/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 78.0831 - val_loss: 118.3518\n",
      "Epoch 148/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 77.9566 - val_loss: 118.1840\n",
      "Epoch 149/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 77.8299 - val_loss: 118.0196\n",
      "Epoch 150/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 77.6994 - val_loss: 117.8497\n",
      "Epoch 151/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 77.5819 - val_loss: 117.6678\n",
      "Epoch 152/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 77.4570 - val_loss: 117.4832\n",
      "Epoch 153/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 115us/step - loss: 77.3548 - val_loss: 117.2968\n",
      "Epoch 154/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 77.2575 - val_loss: 117.1234\n",
      "Epoch 155/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 77.1394 - val_loss: 116.9691\n",
      "Epoch 156/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 77.0362 - val_loss: 116.8240\n",
      "Epoch 157/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 76.9235 - val_loss: 116.6827\n",
      "Epoch 158/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 76.8547 - val_loss: 116.5420\n",
      "Epoch 159/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 76.7387 - val_loss: 116.4240\n",
      "Epoch 160/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 76.6417 - val_loss: 116.3085\n",
      "Epoch 161/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 76.5517 - val_loss: 116.1922\n",
      "Epoch 162/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 76.4560 - val_loss: 116.0697\n",
      "Epoch 163/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 76.3937 - val_loss: 115.9551\n",
      "Epoch 164/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 76.2856 - val_loss: 115.8665\n",
      "Epoch 165/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 76.2158 - val_loss: 115.7688\n",
      "Epoch 166/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 76.1291 - val_loss: 115.6637\n",
      "Epoch 167/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 76.0569 - val_loss: 115.5402\n",
      "Epoch 168/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 75.9899 - val_loss: 115.4207\n",
      "Epoch 169/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 75.8954 - val_loss: 115.3167\n",
      "Epoch 170/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 75.8212 - val_loss: 115.2137\n",
      "Epoch 171/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 75.7426 - val_loss: 115.1203\n",
      "Epoch 172/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 75.6621 - val_loss: 115.0329\n",
      "Epoch 173/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 75.5816 - val_loss: 114.9374\n",
      "Epoch 174/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 75.5101 - val_loss: 114.8248\n",
      "Epoch 175/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 75.4116 - val_loss: 114.6985\n",
      "Epoch 176/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 75.3499 - val_loss: 114.5405\n",
      "Epoch 177/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 75.2508 - val_loss: 114.3965\n",
      "Epoch 178/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 75.1556 - val_loss: 114.2577\n",
      "Epoch 179/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 75.0856 - val_loss: 114.1209\n",
      "Epoch 180/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 74.9851 - val_loss: 113.9998\n",
      "Epoch 181/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 74.9012 - val_loss: 113.8812\n",
      "Epoch 182/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 74.8283 - val_loss: 113.7703\n",
      "Epoch 183/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 74.7397 - val_loss: 113.6747\n",
      "Epoch 184/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 74.6574 - val_loss: 113.5790\n",
      "Epoch 185/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 74.5670 - val_loss: 113.4681\n",
      "Epoch 186/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 74.4854 - val_loss: 113.3231\n",
      "Epoch 187/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 74.4036 - val_loss: 113.1705\n",
      "Epoch 188/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 74.2863 - val_loss: 113.0137\n",
      "Epoch 189/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 74.2184 - val_loss: 112.8386\n",
      "Epoch 190/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 74.1140 - val_loss: 112.6828\n",
      "Epoch 191/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 73.9989 - val_loss: 112.5343\n",
      "Epoch 192/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 73.9212 - val_loss: 112.3800\n",
      "Epoch 193/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 73.8001 - val_loss: 112.2301\n",
      "Epoch 194/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 73.7154 - val_loss: 112.0636\n",
      "Epoch 195/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 73.6277 - val_loss: 111.8960\n",
      "Epoch 196/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 73.5291 - val_loss: 111.7222\n",
      "Epoch 197/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 73.4313 - val_loss: 111.5271\n",
      "Epoch 198/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 73.3573 - val_loss: 111.3177\n",
      "Epoch 199/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 73.2796 - val_loss: 111.1248\n",
      "Epoch 200/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 73.1675 - val_loss: 110.9554\n",
      "Epoch 201/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 73.0764 - val_loss: 110.7843\n",
      "Epoch 202/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 73.0086 - val_loss: 110.6197\n",
      "Epoch 203/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 72.9000 - val_loss: 110.4764\n",
      "Epoch 204/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 72.8273 - val_loss: 110.3338\n",
      "Epoch 205/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 72.7326 - val_loss: 110.1982\n",
      "Epoch 206/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 72.6497 - val_loss: 110.0521\n",
      "Epoch 207/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 72.5538 - val_loss: 109.8933\n",
      "Epoch 208/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 72.4763 - val_loss: 109.7172\n",
      "Epoch 209/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 72.3898 - val_loss: 109.5457\n",
      "Epoch 210/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 72.3055 - val_loss: 109.3851\n",
      "Epoch 211/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 72.1879 - val_loss: 109.2286\n",
      "Epoch 212/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 72.1266 - val_loss: 109.0472\n",
      "Epoch 213/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 72.0233 - val_loss: 108.8858\n",
      "Epoch 214/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 71.9368 - val_loss: 108.7322\n",
      "Epoch 215/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 71.8804 - val_loss: 108.5816\n",
      "Epoch 216/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 71.7765 - val_loss: 108.4403\n",
      "Epoch 217/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 71.7375 - val_loss: 108.2794\n",
      "Epoch 218/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 71.6425 - val_loss: 108.1438\n",
      "Epoch 219/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 71.5890 - val_loss: 108.0106\n",
      "Epoch 220/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 71.5101 - val_loss: 107.9048\n",
      "Epoch 221/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 71.4317 - val_loss: 107.7983\n",
      "Epoch 222/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 71.4121 - val_loss: 107.6902\n",
      "Epoch 223/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 59.04 - 0s 126us/step - loss: 71.3111 - val_loss: 107.6264\n",
      "Epoch 224/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 71.2573 - val_loss: 107.5305\n",
      "Epoch 225/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 71.1952 - val_loss: 107.4290\n",
      "Epoch 226/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 71.1299 - val_loss: 107.2830\n",
      "Epoch 227/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 71.1202 - val_loss: 107.1264\n",
      "Epoch 228/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 71.0316 - val_loss: 107.0232\n",
      "Epoch 229/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 103us/step - loss: 70.9758 - val_loss: 106.9340\n",
      "Epoch 230/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 70.9251 - val_loss: 106.8589\n",
      "Epoch 231/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.8651 - val_loss: 106.7939\n",
      "Epoch 232/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.8109 - val_loss: 106.7094\n",
      "Epoch 233/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.7692 - val_loss: 106.6214\n",
      "Epoch 234/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 70.7036 - val_loss: 106.5418\n",
      "Epoch 235/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 70.6445 - val_loss: 106.4233\n",
      "Epoch 236/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 70.5963 - val_loss: 106.2860\n",
      "Epoch 237/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.5504 - val_loss: 106.1516\n",
      "Epoch 238/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.5084 - val_loss: 106.0223\n",
      "Epoch 239/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.4570 - val_loss: 105.9049\n",
      "Epoch 240/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 70.4146 - val_loss: 105.7887\n",
      "Epoch 241/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 70.3899 - val_loss: 105.6928\n",
      "Epoch 242/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 70.3234 - val_loss: 105.6339\n",
      "Epoch 243/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.2892 - val_loss: 105.5623\n",
      "Epoch 244/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 70.2328 - val_loss: 105.5129\n",
      "Epoch 245/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 70.1860 - val_loss: 105.4401\n",
      "Epoch 246/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 70.1510 - val_loss: 105.3681\n",
      "Epoch 247/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 70.0997 - val_loss: 105.3139\n",
      "Epoch 248/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.0561 - val_loss: 105.2598\n",
      "Epoch 249/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 70.0077 - val_loss: 105.2106\n",
      "Epoch 250/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 69.9687 - val_loss: 105.1614\n",
      "Epoch 251/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 69.9301 - val_loss: 105.1326\n",
      "Epoch 252/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 69.8856 - val_loss: 105.1197\n",
      "Epoch 253/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 69.8496 - val_loss: 105.1101\n",
      "Epoch 254/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 69.7982 - val_loss: 105.1456\n",
      "Epoch 255/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 69.7626 - val_loss: 105.1847\n",
      "Epoch 256/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 69.7100 - val_loss: 105.2041\n",
      "Epoch 257/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 69.6706 - val_loss: 105.2400\n",
      "Epoch 258/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 69.6205 - val_loss: 105.2568\n",
      "Epoch 259/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 69.5848 - val_loss: 105.2665\n",
      "Epoch 260/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 69.5371 - val_loss: 105.2392\n",
      "Epoch 261/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 69.4980 - val_loss: 105.1958\n",
      "Epoch 262/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 69.4556 - val_loss: 105.1681\n",
      "Epoch 263/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 69.4163 - val_loss: 105.1275\n",
      "Epoch 264/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 69.3759 - val_loss: 105.0872\n",
      "Epoch 265/1500\n",
      "87/87 [==============================] - 0s 184us/step - loss: 69.3376 - val_loss: 105.0462\n",
      "Epoch 266/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 69.3035 - val_loss: 104.9787\n",
      "Epoch 267/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 69.2568 - val_loss: 104.8595\n",
      "Epoch 268/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 69.2219 - val_loss: 104.7365\n",
      "Epoch 269/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 69.1836 - val_loss: 104.6111\n",
      "Epoch 270/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 69.1581 - val_loss: 104.4834\n",
      "Epoch 271/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 69.1289 - val_loss: 104.3740\n",
      "Epoch 272/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 69.0903 - val_loss: 104.2859\n",
      "Epoch 273/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 69.0580 - val_loss: 104.2104\n",
      "Epoch 274/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 69.0168 - val_loss: 104.1488\n",
      "Epoch 275/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 68.9770 - val_loss: 104.0743\n",
      "Epoch 276/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 68.9406 - val_loss: 103.9826\n",
      "Epoch 277/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 68.9138 - val_loss: 103.8951\n",
      "Epoch 278/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 68.8689 - val_loss: 103.8373\n",
      "Epoch 279/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 68.8315 - val_loss: 103.7736\n",
      "Epoch 280/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.8046 - val_loss: 103.6996\n",
      "Epoch 281/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.7570 - val_loss: 103.6594\n",
      "Epoch 282/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 68.7183 - val_loss: 103.6123\n",
      "Epoch 283/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 68.6908 - val_loss: 103.5572\n",
      "Epoch 284/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 68.6477 - val_loss: 103.5408\n",
      "Epoch 285/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.6095 - val_loss: 103.5505\n",
      "Epoch 286/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 68.5656 - val_loss: 103.6017\n",
      "Epoch 287/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 68.5237 - val_loss: 103.6791\n",
      "Epoch 288/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 68.5028 - val_loss: 103.7742\n",
      "Epoch 289/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 68.4479 - val_loss: 103.8373\n",
      "Epoch 290/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 68.4126 - val_loss: 103.9095\n",
      "Epoch 291/1500\n",
      "87/87 [==============================] - 0s 127us/step - loss: 68.3739 - val_loss: 103.9721\n",
      "Epoch 292/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 68.3408 - val_loss: 104.0253\n",
      "Epoch 293/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 68.2994 - val_loss: 104.0768\n",
      "Epoch 294/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.2601 - val_loss: 104.1643\n",
      "Epoch 295/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.2243 - val_loss: 104.2432\n",
      "Epoch 296/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.1827 - val_loss: 104.3227\n",
      "Epoch 297/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.1522 - val_loss: 104.4024\n",
      "Epoch 298/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 68.1112 - val_loss: 104.4555\n",
      "Epoch 299/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 68.0794 - val_loss: 104.4948\n",
      "Epoch 300/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.0386 - val_loss: 104.5057\n",
      "Epoch 301/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 68.0013 - val_loss: 104.5021\n",
      "Epoch 302/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 67.9753 - val_loss: 104.5083\n",
      "Epoch 303/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 67.9232 - val_loss: 104.5739\n",
      "Epoch 304/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 67.9007 - val_loss: 104.6423\n",
      "Epoch 305/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 103us/step - loss: 67.8591 - val_loss: 104.6747\n",
      "Epoch 306/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 67.8250 - val_loss: 104.6829\n",
      "Epoch 307/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 67.7901 - val_loss: 104.6749\n",
      "Epoch 308/1500\n",
      "87/87 [==============================] - 0s 127us/step - loss: 67.7540 - val_loss: 104.6807\n",
      "Epoch 309/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 67.7187 - val_loss: 104.6662\n",
      "Epoch 310/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 67.6864 - val_loss: 104.6402\n",
      "Epoch 311/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 67.6523 - val_loss: 104.5853\n",
      "Epoch 312/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 67.6166 - val_loss: 104.4972\n",
      "Epoch 313/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 67.6007 - val_loss: 104.4294\n",
      "Epoch 314/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 67.5530 - val_loss: 104.4038\n",
      "Epoch 315/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 67.5185 - val_loss: 104.3747\n",
      "Epoch 316/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 67.4865 - val_loss: 104.3552\n",
      "Epoch 317/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 67.4528 - val_loss: 104.3427\n",
      "Epoch 318/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 67.4224 - val_loss: 104.3331\n",
      "Epoch 319/1500\n",
      "87/87 [==============================] - 0s 218us/step - loss: 67.3867 - val_loss: 104.2981\n",
      "Epoch 320/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 67.3519 - val_loss: 104.2758\n",
      "Epoch 321/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 61.26 - 0s 115us/step - loss: 67.3205 - val_loss: 104.2323\n",
      "Epoch 322/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 67.2949 - val_loss: 104.1625\n",
      "Epoch 323/1500\n",
      "87/87 [==============================] - 0s 151us/step - loss: 67.2543 - val_loss: 104.1307\n",
      "Epoch 324/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 67.2208 - val_loss: 104.1108\n",
      "Epoch 325/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 67.1845 - val_loss: 104.0653\n",
      "Epoch 326/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 67.1533 - val_loss: 104.0008\n",
      "Epoch 327/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 67.1182 - val_loss: 103.9467\n",
      "Epoch 328/1500\n",
      "87/87 [==============================] - 0s 195us/step - loss: 67.0871 - val_loss: 103.8957\n",
      "Epoch 329/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 67.0553 - val_loss: 103.8730\n",
      "Epoch 330/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 67.0150 - val_loss: 103.8709\n",
      "Epoch 331/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 66.9843 - val_loss: 103.8612\n",
      "Epoch 332/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 66.9488 - val_loss: 103.8482\n",
      "Epoch 333/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.9164 - val_loss: 103.8195\n",
      "Epoch 334/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 66.8829 - val_loss: 103.7739\n",
      "Epoch 335/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.8490 - val_loss: 103.7285\n",
      "Epoch 336/1500\n",
      "87/87 [==============================] - 0s 184us/step - loss: 66.8153 - val_loss: 103.6766\n",
      "Epoch 337/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 66.7858 - val_loss: 103.6384\n",
      "Epoch 338/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 66.7718 - val_loss: 103.6105\n",
      "Epoch 339/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.7299 - val_loss: 103.6349\n",
      "Epoch 340/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 66.6961 - val_loss: 103.6427\n",
      "Epoch 341/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.6676 - val_loss: 103.6513\n",
      "Epoch 342/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.6218 - val_loss: 103.7180\n",
      "Epoch 343/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.5965 - val_loss: 103.7995\n",
      "Epoch 344/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 66.5661 - val_loss: 103.8448\n",
      "Epoch 345/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.5258 - val_loss: 103.8605\n",
      "Epoch 346/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.4965 - val_loss: 103.8749\n",
      "Epoch 347/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.4640 - val_loss: 103.8618\n",
      "Epoch 348/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 66.4319 - val_loss: 103.8236\n",
      "Epoch 349/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 61.19 - 0s 115us/step - loss: 66.3913 - val_loss: 103.7400\n",
      "Epoch 350/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 66.3638 - val_loss: 103.6449\n",
      "Epoch 351/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.3461 - val_loss: 103.5663\n",
      "Epoch 352/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.3167 - val_loss: 103.5157\n",
      "Epoch 353/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 66.2957 - val_loss: 103.4890\n",
      "Epoch 354/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 66.2687 - val_loss: 103.4906\n",
      "Epoch 355/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 66.2335 - val_loss: 103.4627\n",
      "Epoch 356/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 66.2032 - val_loss: 103.4776\n",
      "Epoch 357/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 66.1683 - val_loss: 103.5264\n",
      "Epoch 358/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 66.1386 - val_loss: 103.5689\n",
      "Epoch 359/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 66.1031 - val_loss: 103.5994\n",
      "Epoch 360/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 66.0718 - val_loss: 103.6276\n",
      "Epoch 361/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 66.0396 - val_loss: 103.6475\n",
      "Epoch 362/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 66.0089 - val_loss: 103.6654\n",
      "Epoch 363/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 65.9764 - val_loss: 103.6683\n",
      "Epoch 364/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 65.9466 - val_loss: 103.6495\n",
      "Epoch 365/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 65.9163 - val_loss: 103.6019\n",
      "Epoch 366/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.8875 - val_loss: 103.5263\n",
      "Epoch 367/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.8719 - val_loss: 103.4823\n",
      "Epoch 368/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 65.8343 - val_loss: 103.4901\n",
      "Epoch 369/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 65.8064 - val_loss: 103.5182\n",
      "Epoch 370/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.7713 - val_loss: 103.5451\n",
      "Epoch 371/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 65.7443 - val_loss: 103.6022\n",
      "Epoch 372/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 65.7054 - val_loss: 103.6457\n",
      "Epoch 373/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 65.6699 - val_loss: 103.7054\n",
      "Epoch 374/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 65.6509 - val_loss: 103.7751\n",
      "Epoch 375/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 65.6044 - val_loss: 103.8238\n",
      "Epoch 376/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 65.5744 - val_loss: 103.8923\n",
      "Epoch 377/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 65.5454 - val_loss: 103.9455\n",
      "Epoch 378/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.5198 - val_loss: 103.9768\n",
      "Epoch 379/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.4869 - val_loss: 103.9751\n",
      "Epoch 380/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 65.4605 - val_loss: 103.9716\n",
      "Epoch 381/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 103us/step - loss: 65.4256 - val_loss: 103.9998\n",
      "Epoch 382/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 65.3998 - val_loss: 104.0227\n",
      "Epoch 383/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 65.3687 - val_loss: 104.0273\n",
      "Epoch 384/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.3392 - val_loss: 104.0482\n",
      "Epoch 385/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.3073 - val_loss: 104.1018\n",
      "Epoch 386/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 65.2902 - val_loss: 104.1465\n",
      "Epoch 387/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.2534 - val_loss: 104.1612\n",
      "Epoch 388/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 65.2270 - val_loss: 104.1812\n",
      "Epoch 389/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 65.1995 - val_loss: 104.1807\n",
      "Epoch 390/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.1648 - val_loss: 104.1431\n",
      "Epoch 391/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.1325 - val_loss: 104.0956\n",
      "Epoch 392/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 65.1006 - val_loss: 104.0354\n",
      "Epoch 393/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 65.0733 - val_loss: 103.9613\n",
      "Epoch 394/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 65.0389 - val_loss: 103.9093\n",
      "Epoch 395/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 65.0042 - val_loss: 103.8785\n",
      "Epoch 396/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.9929 - val_loss: 103.8665\n",
      "Epoch 397/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 64.9470 - val_loss: 103.9164\n",
      "Epoch 398/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 64.9206 - val_loss: 103.9477\n",
      "Epoch 399/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 64.8859 - val_loss: 103.9539\n",
      "Epoch 400/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.8591 - val_loss: 103.9617\n",
      "Epoch 401/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 64.8263 - val_loss: 103.9864\n",
      "Epoch 402/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 64.7989 - val_loss: 104.0180\n",
      "Epoch 403/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.7689 - val_loss: 104.0618\n",
      "Epoch 404/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.7403 - val_loss: 104.1308\n",
      "Epoch 405/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.7129 - val_loss: 104.2006\n",
      "Epoch 406/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 64.7096 - val_loss: 104.2572\n",
      "Epoch 407/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 64.6636 - val_loss: 104.2548\n",
      "Epoch 408/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 64.6367 - val_loss: 104.2371\n",
      "Epoch 409/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.6076 - val_loss: 104.1995\n",
      "Epoch 410/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 64.5725 - val_loss: 104.1847\n",
      "Epoch 411/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 64.5406 - val_loss: 104.1675\n",
      "Epoch 412/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 64.5117 - val_loss: 104.1472\n",
      "Epoch 413/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.4880 - val_loss: 104.1257\n",
      "Epoch 414/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 64.4500 - val_loss: 104.1343\n",
      "Epoch 415/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.4208 - val_loss: 104.1381\n",
      "Epoch 416/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 64.3902 - val_loss: 104.1527\n",
      "Epoch 417/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.3659 - val_loss: 104.1778\n",
      "Epoch 418/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.3382 - val_loss: 104.1681\n",
      "Epoch 419/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.3052 - val_loss: 104.1021\n",
      "Epoch 420/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 64.2709 - val_loss: 103.9864\n",
      "Epoch 421/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 64.2327 - val_loss: 103.8898\n",
      "Epoch 422/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 64.2253 - val_loss: 103.8130\n",
      "Epoch 423/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.1830 - val_loss: 103.7785\n",
      "Epoch 424/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.1568 - val_loss: 103.7282\n",
      "Epoch 425/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.1327 - val_loss: 103.6587\n",
      "Epoch 426/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 64.1029 - val_loss: 103.6160\n",
      "Epoch 427/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 64.0741 - val_loss: 103.5805\n",
      "Epoch 428/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 64.0486 - val_loss: 103.5570\n",
      "Epoch 429/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 64.0252 - val_loss: 103.5626\n",
      "Epoch 430/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.9923 - val_loss: 103.6074\n",
      "Epoch 431/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 63.9617 - val_loss: 103.6267\n",
      "Epoch 432/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 63.9347 - val_loss: 103.6387\n",
      "Epoch 433/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 63.8956 - val_loss: 103.7011\n",
      "Epoch 434/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 63.8696 - val_loss: 103.7706\n",
      "Epoch 435/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 63.8388 - val_loss: 103.8153\n",
      "Epoch 436/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 63.8041 - val_loss: 103.8378\n",
      "Epoch 437/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 63.7815 - val_loss: 103.8324\n",
      "Epoch 438/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 63.7430 - val_loss: 103.7899\n",
      "Epoch 439/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 65.64 - 0s 115us/step - loss: 63.7167 - val_loss: 103.7794\n",
      "Epoch 440/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 63.6824 - val_loss: 103.8007\n",
      "Epoch 441/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 63.6480 - val_loss: 103.8260\n",
      "Epoch 442/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 63.6170 - val_loss: 103.8799\n",
      "Epoch 443/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 63.5898 - val_loss: 103.9172\n",
      "Epoch 444/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.5546 - val_loss: 103.9389\n",
      "Epoch 445/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 63.5173 - val_loss: 103.9969\n",
      "Epoch 446/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 63.4859 - val_loss: 104.0705\n",
      "Epoch 447/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.4518 - val_loss: 104.1404\n",
      "Epoch 448/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.4244 - val_loss: 104.2160\n",
      "Epoch 449/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 63.3990 - val_loss: 104.2924\n",
      "Epoch 450/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.3702 - val_loss: 104.3602\n",
      "Epoch 451/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.3424 - val_loss: 104.4295\n",
      "Epoch 452/1500\n",
      "87/87 [==============================] - 0s 529us/step - loss: 63.3285 - val_loss: 104.5008\n",
      "Epoch 453/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 63.2921 - val_loss: 104.5259\n",
      "Epoch 454/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.2598 - val_loss: 104.5242\n",
      "Epoch 455/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.2237 - val_loss: 104.5048\n",
      "Epoch 456/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.1929 - val_loss: 104.4750\n",
      "Epoch 457/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 103us/step - loss: 63.1540 - val_loss: 104.4207\n",
      "Epoch 458/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 63.1148 - val_loss: 104.3789\n",
      "Epoch 459/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 63.0746 - val_loss: 104.3357\n",
      "Epoch 460/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 63.0382 - val_loss: 104.2934\n",
      "Epoch 461/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 62.9968 - val_loss: 104.2416\n",
      "Epoch 462/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 62.9621 - val_loss: 104.1731\n",
      "Epoch 463/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 65.35 - 0s 115us/step - loss: 62.9264 - val_loss: 104.1325\n",
      "Epoch 464/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.8777 - val_loss: 104.1276\n",
      "Epoch 465/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 62.8422 - val_loss: 104.1161\n",
      "Epoch 466/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 62.8122 - val_loss: 104.0597\n",
      "Epoch 467/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.7671 - val_loss: 103.9447\n",
      "Epoch 468/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 62.7305 - val_loss: 103.8501\n",
      "Epoch 469/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 62.7065 - val_loss: 103.7836\n",
      "Epoch 470/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 62.6595 - val_loss: 103.7718\n",
      "Epoch 471/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.6178 - val_loss: 103.7865\n",
      "Epoch 472/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 62.5895 - val_loss: 103.7975\n",
      "Epoch 473/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 62.5420 - val_loss: 103.7661\n",
      "Epoch 474/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 62.4998 - val_loss: 103.7230\n",
      "Epoch 475/1500\n",
      "87/87 [==============================] - 0s 127us/step - loss: 62.4658 - val_loss: 103.6755\n",
      "Epoch 476/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.4227 - val_loss: 103.6281\n",
      "Epoch 477/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.3862 - val_loss: 103.5729\n",
      "Epoch 478/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 62.3421 - val_loss: 103.5071\n",
      "Epoch 479/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.3010 - val_loss: 103.4193\n",
      "Epoch 480/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 62.2706 - val_loss: 103.3152\n",
      "Epoch 481/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.2319 - val_loss: 103.2488\n",
      "Epoch 482/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 62.1835 - val_loss: 103.2212\n",
      "Epoch 483/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.1378 - val_loss: 103.2079\n",
      "Epoch 484/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 62.1023 - val_loss: 103.1773\n",
      "Epoch 485/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 62.0499 - val_loss: 103.1023\n",
      "Epoch 486/1500\n",
      "87/87 [==============================] - 0s 127us/step - loss: 62.0038 - val_loss: 103.0349\n",
      "Epoch 487/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 61.9550 - val_loss: 102.9573\n",
      "Epoch 488/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 61.9361 - val_loss: 102.8858\n",
      "Epoch 489/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 61.8680 - val_loss: 102.8651\n",
      "Epoch 490/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 61.8247 - val_loss: 102.8437\n",
      "Epoch 491/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 61.7702 - val_loss: 102.8483\n",
      "Epoch 492/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 61.7258 - val_loss: 102.8531\n",
      "Epoch 493/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 61.6762 - val_loss: 102.8692\n",
      "Epoch 494/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 61.6248 - val_loss: 102.9227\n",
      "Epoch 495/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 61.5667 - val_loss: 102.9609\n",
      "Epoch 496/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 61.5181 - val_loss: 103.0089\n",
      "Epoch 497/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 61.4689 - val_loss: 103.0241\n",
      "Epoch 498/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 61.4085 - val_loss: 103.0066\n",
      "Epoch 499/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 61.3500 - val_loss: 102.9889\n",
      "Epoch 500/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 61.2950 - val_loss: 102.9675\n",
      "Epoch 501/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 61.2417 - val_loss: 102.9502\n",
      "Epoch 502/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 61.1834 - val_loss: 102.9668\n",
      "Epoch 503/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 61.1165 - val_loss: 102.9729\n",
      "Epoch 504/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 61.0743 - val_loss: 102.9821\n",
      "Epoch 505/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 60.9970 - val_loss: 102.9296\n",
      "Epoch 506/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 60.9313 - val_loss: 102.8584\n",
      "Epoch 507/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 60.8671 - val_loss: 102.7950\n",
      "Epoch 508/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 60.7980 - val_loss: 102.7137\n",
      "Epoch 509/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 60.7271 - val_loss: 102.5838\n",
      "Epoch 510/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 60.6694 - val_loss: 102.4409\n",
      "Epoch 511/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 60.6208 - val_loss: 102.3567\n",
      "Epoch 512/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 60.5508 - val_loss: 102.3461\n",
      "Epoch 513/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 60.4672 - val_loss: 102.3982\n",
      "Epoch 514/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 60.3854 - val_loss: 102.4793\n",
      "Epoch 515/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 60.3028 - val_loss: 102.5841\n",
      "Epoch 516/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 60.2205 - val_loss: 102.7021\n",
      "Epoch 517/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 65.76 - 0s 103us/step - loss: 60.1601 - val_loss: 102.7951\n",
      "Epoch 518/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 60.0562 - val_loss: 102.8360\n",
      "Epoch 519/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 59.9691 - val_loss: 102.8918\n",
      "Epoch 520/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 59.8789 - val_loss: 103.0024\n",
      "Epoch 521/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 59.8017 - val_loss: 103.1124\n",
      "Epoch 522/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 59.6960 - val_loss: 103.2010\n",
      "Epoch 523/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 59.6058 - val_loss: 103.2887\n",
      "Epoch 524/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 59.5240 - val_loss: 103.3616\n",
      "Epoch 525/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 59.4170 - val_loss: 103.4154\n",
      "Epoch 526/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 59.3227 - val_loss: 103.5103\n",
      "Epoch 527/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 59.2180 - val_loss: 103.5689\n",
      "Epoch 528/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 59.1108 - val_loss: 103.6019\n",
      "Epoch 529/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 59.0036 - val_loss: 103.6305\n",
      "Epoch 530/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 58.9110 - val_loss: 103.6509\n",
      "Epoch 531/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 58.7868 - val_loss: 103.6217\n",
      "Epoch 532/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 58.6763 - val_loss: 103.5658\n",
      "Epoch 533/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 103us/step - loss: 58.5583 - val_loss: 103.4860\n",
      "Epoch 534/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 58.4392 - val_loss: 103.3931\n",
      "Epoch 535/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 58.3397 - val_loss: 103.3154\n",
      "Epoch 536/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 58.2106 - val_loss: 103.2695\n",
      "Epoch 537/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 58.0855 - val_loss: 103.2206\n",
      "Epoch 538/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 57.9639 - val_loss: 103.1957\n",
      "Epoch 539/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 57.8450 - val_loss: 103.1788\n",
      "Epoch 540/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 57.7186 - val_loss: 103.1589\n",
      "Epoch 541/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 57.6092 - val_loss: 103.1579\n",
      "Epoch 542/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 57.4835 - val_loss: 103.1384\n",
      "Epoch 543/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 57.3799 - val_loss: 103.1189\n",
      "Epoch 544/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 57.2693 - val_loss: 103.1334\n",
      "Epoch 545/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 57.1475 - val_loss: 103.1564\n",
      "Epoch 546/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 57.0408 - val_loss: 103.1874\n",
      "Epoch 547/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 56.9001 - val_loss: 103.2048\n",
      "Epoch 548/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 56.7839 - val_loss: 103.2582\n",
      "Epoch 549/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 56.6381 - val_loss: 103.3454\n",
      "Epoch 550/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 56.4875 - val_loss: 103.5107\n",
      "Epoch 551/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 56.3505 - val_loss: 103.6930\n",
      "Epoch 552/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 56.1709 - val_loss: 103.8558\n",
      "Epoch 553/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 56.0454 - val_loss: 104.0240\n",
      "Epoch 554/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 55.8962 - val_loss: 104.1696\n",
      "Epoch 555/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 55.7671 - val_loss: 104.2826\n",
      "Epoch 556/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 55.6011 - val_loss: 104.3635\n",
      "Epoch 557/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 55.4517 - val_loss: 104.4427\n",
      "Epoch 558/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 55.2883 - val_loss: 104.5105\n",
      "Epoch 559/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 55.1427 - val_loss: 104.5496\n",
      "Epoch 560/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 54.9813 - val_loss: 104.5620\n",
      "Epoch 561/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 54.8214 - val_loss: 104.5400\n",
      "Epoch 562/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 54.6444 - val_loss: 104.5448\n",
      "Epoch 563/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 54.4858 - val_loss: 104.5572\n",
      "Epoch 564/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 54.3248 - val_loss: 104.5536\n",
      "Epoch 565/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 54.1549 - val_loss: 104.5209\n",
      "Epoch 566/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 53.9915 - val_loss: 104.5057\n",
      "Epoch 567/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 53.8227 - val_loss: 104.5216\n",
      "Epoch 568/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 53.6592 - val_loss: 104.5401\n",
      "Epoch 569/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 53.4944 - val_loss: 104.5549\n",
      "Epoch 570/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 53.3328 - val_loss: 104.5676\n",
      "Epoch 571/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 53.1577 - val_loss: 104.6000\n",
      "Epoch 572/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 53.0055 - val_loss: 104.6237\n",
      "Epoch 573/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 52.8237 - val_loss: 104.6555\n",
      "Epoch 574/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 52.6595 - val_loss: 104.7377\n",
      "Epoch 575/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 52.4844 - val_loss: 104.8408\n",
      "Epoch 576/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 52.2984 - val_loss: 104.9477\n",
      "Epoch 577/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 52.1392 - val_loss: 105.0665\n",
      "Epoch 578/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 51.9463 - val_loss: 105.1811\n",
      "Epoch 579/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.7906 - val_loss: 105.2794\n",
      "Epoch 580/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 51.5911 - val_loss: 105.3358\n",
      "Epoch 581/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 51.4136 - val_loss: 105.3846\n",
      "Epoch 582/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 51.2184 - val_loss: 105.4345\n",
      "Epoch 583/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 51.0609 - val_loss: 105.5156\n",
      "Epoch 584/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 50.8678 - val_loss: 105.6161\n",
      "Epoch 585/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 50.6761 - val_loss: 105.7105\n",
      "Epoch 586/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 50.4872 - val_loss: 105.8283\n",
      "Epoch 587/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 50.3118 - val_loss: 105.9443\n",
      "Epoch 588/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 50.1256 - val_loss: 106.0609\n",
      "Epoch 589/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 49.9226 - val_loss: 106.1942\n",
      "Epoch 590/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 49.7216 - val_loss: 106.3601\n",
      "Epoch 591/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 49.5725 - val_loss: 106.5341\n",
      "Epoch 592/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 49.3578 - val_loss: 106.6627\n",
      "Epoch 593/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 49.1663 - val_loss: 106.7884\n",
      "Epoch 594/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.9540 - val_loss: 106.9116\n",
      "Epoch 595/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 48.7808 - val_loss: 107.0469\n",
      "Epoch 596/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 48.5509 - val_loss: 107.1399\n",
      "Epoch 597/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 48.3414 - val_loss: 107.2298\n",
      "Epoch 598/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 48.1323 - val_loss: 107.3257\n",
      "Epoch 599/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 47.9129 - val_loss: 107.4123\n",
      "Epoch 600/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.6943 - val_loss: 107.4862\n",
      "Epoch 601/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.4674 - val_loss: 107.5321\n",
      "Epoch 602/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 47.2427 - val_loss: 107.5587\n",
      "Epoch 603/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 47.0099 - val_loss: 107.5583\n",
      "Epoch 604/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 46.7735 - val_loss: 107.5418\n",
      "Epoch 605/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 46.5309 - val_loss: 107.5097\n",
      "Epoch 606/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 46.3034 - val_loss: 107.4614\n",
      "Epoch 607/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 46.0780 - val_loss: 107.4018\n",
      "Epoch 608/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 45.8396 - val_loss: 107.3580\n",
      "Epoch 609/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 45.5956 - val_loss: 107.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 610/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 45.3793 - val_loss: 107.2341\n",
      "Epoch 611/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 45.1419 - val_loss: 107.2042\n",
      "Epoch 612/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 44.9261 - val_loss: 107.2032\n",
      "Epoch 613/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 44.6828 - val_loss: 107.2598\n",
      "Epoch 614/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 44.4484 - val_loss: 107.3229\n",
      "Epoch 615/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 44.1832 - val_loss: 107.4425\n",
      "Epoch 616/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 43.9309 - val_loss: 107.6235\n",
      "Epoch 617/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 43.7121 - val_loss: 107.7993\n",
      "Epoch 618/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 43.4113 - val_loss: 107.9517\n",
      "Epoch 619/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 43.1916 - val_loss: 108.0856\n",
      "Epoch 620/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 43.0115 - val_loss: 108.2039\n",
      "Epoch 621/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 42.8878 - val_loss: 108.3460\n",
      "Epoch 622/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 42.7670 - val_loss: 108.4016\n",
      "Epoch 623/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 42.6795 - val_loss: 108.4376\n",
      "Epoch 624/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 42.6043 - val_loss: 108.4428\n",
      "Epoch 625/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 42.5331 - val_loss: 108.3667\n",
      "Epoch 626/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 42.4361 - val_loss: 108.2358\n",
      "Epoch 627/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 42.3247 - val_loss: 108.0888\n",
      "Epoch 628/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 42.2098 - val_loss: 107.9062\n",
      "Epoch 629/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 42.1338 - val_loss: 107.6953\n",
      "Epoch 630/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 42.0853 - val_loss: 107.5148\n",
      "Epoch 631/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 42.0120 - val_loss: 107.4169\n",
      "Epoch 632/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.9724 - val_loss: 107.4044\n",
      "Epoch 633/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 41.9563 - val_loss: 107.4074\n",
      "Epoch 634/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 41.9061 - val_loss: 107.4240\n",
      "Epoch 635/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.8726 - val_loss: 107.4551\n",
      "Epoch 636/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.8394 - val_loss: 107.4880\n",
      "Epoch 637/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.8286 - val_loss: 107.5052\n",
      "Epoch 638/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.7781 - val_loss: 107.4958\n",
      "Epoch 639/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.7681 - val_loss: 107.4858\n",
      "Epoch 640/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.7297 - val_loss: 107.4921\n",
      "Epoch 641/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.7001 - val_loss: 107.5083\n",
      "Epoch 642/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.6871 - val_loss: 107.5205\n",
      "Epoch 643/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.6681 - val_loss: 107.5218\n",
      "Epoch 644/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.6459 - val_loss: 107.5156\n",
      "Epoch 645/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.6289 - val_loss: 107.5096\n",
      "Epoch 646/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 41.6035 - val_loss: 107.5100\n",
      "Epoch 647/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.5874 - val_loss: 107.5059\n",
      "Epoch 648/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.5655 - val_loss: 107.4925\n",
      "Epoch 649/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.5392 - val_loss: 107.4647\n",
      "Epoch 650/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.5201 - val_loss: 107.4430\n",
      "Epoch 651/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.5060 - val_loss: 107.4337\n",
      "Epoch 652/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 41.5054 - val_loss: 107.4302\n",
      "Epoch 653/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.4839 - val_loss: 107.4415\n",
      "Epoch 654/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.4694 - val_loss: 107.4706\n",
      "Epoch 655/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.4421 - val_loss: 107.4929\n",
      "Epoch 656/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 41.4175 - val_loss: 107.5164\n",
      "Epoch 657/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.4260 - val_loss: 107.5290\n",
      "Epoch 658/1500\n",
      "87/87 [==============================] - 0s 80us/step - loss: 41.4174 - val_loss: 107.5139\n",
      "Epoch 659/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.3941 - val_loss: 107.4987\n",
      "Epoch 660/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.3784 - val_loss: 107.4767\n",
      "Epoch 661/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.3598 - val_loss: 107.4572\n",
      "Epoch 662/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 41.3457 - val_loss: 107.4407\n",
      "Epoch 663/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.3445 - val_loss: 107.4220\n",
      "Epoch 664/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.3331 - val_loss: 107.4153\n",
      "Epoch 665/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.3180 - val_loss: 107.4007\n",
      "Epoch 666/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 41.3040 - val_loss: 107.3975\n",
      "Epoch 667/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.2976 - val_loss: 107.3812\n",
      "Epoch 668/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.2885 - val_loss: 107.3503\n",
      "Epoch 669/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.2757 - val_loss: 107.3387\n",
      "Epoch 670/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.2700 - val_loss: 107.3265\n",
      "Epoch 671/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.2481 - val_loss: 107.2965\n",
      "Epoch 672/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.2403 - val_loss: 107.2591\n",
      "Epoch 673/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.2359 - val_loss: 107.2266\n",
      "Epoch 674/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.2355 - val_loss: 107.2013\n",
      "Epoch 675/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.2281 - val_loss: 107.1864\n",
      "Epoch 676/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.2150 - val_loss: 107.1636\n",
      "Epoch 677/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.2010 - val_loss: 107.1545\n",
      "Epoch 678/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.1861 - val_loss: 107.1419\n",
      "Epoch 679/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 41.1755 - val_loss: 107.1327\n",
      "Epoch 680/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.1620 - val_loss: 107.1335\n",
      "Epoch 681/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.1528 - val_loss: 107.1275\n",
      "Epoch 682/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.1304 - val_loss: 107.1125\n",
      "Epoch 683/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.1177 - val_loss: 107.1071\n",
      "Epoch 684/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.1064 - val_loss: 107.1068\n",
      "Epoch 685/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.1005 - val_loss: 107.1116\n",
      "Epoch 686/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 92us/step - loss: 41.0725 - val_loss: 107.1032\n",
      "Epoch 687/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.0623 - val_loss: 107.0927\n",
      "Epoch 688/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 41.0494 - val_loss: 107.0755\n",
      "Epoch 689/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 41.0442 - val_loss: 107.0703\n",
      "Epoch 690/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 41.0233 - val_loss: 107.0845\n",
      "Epoch 691/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 41.0094 - val_loss: 107.0952\n",
      "Epoch 692/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.9830 - val_loss: 107.0997\n",
      "Epoch 693/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.9705 - val_loss: 107.1044\n",
      "Epoch 694/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.9622 - val_loss: 107.1018\n",
      "Epoch 695/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.9465 - val_loss: 107.0921\n",
      "Epoch 696/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.9348 - val_loss: 107.0864\n",
      "Epoch 697/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.9261 - val_loss: 107.0669\n",
      "Epoch 698/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.9145 - val_loss: 107.0359\n",
      "Epoch 699/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.9041 - val_loss: 107.0217\n",
      "Epoch 700/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.8986 - val_loss: 107.0164\n",
      "Epoch 701/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.8809 - val_loss: 106.9944\n",
      "Epoch 702/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.8628 - val_loss: 106.9822\n",
      "Epoch 703/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.8441 - val_loss: 106.9536\n",
      "Epoch 704/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.8368 - val_loss: 106.9316\n",
      "Epoch 705/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.8246 - val_loss: 106.9164\n",
      "Epoch 706/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.8126 - val_loss: 106.8991\n",
      "Epoch 707/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 40.8059 - val_loss: 106.8835\n",
      "Epoch 708/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.7902 - val_loss: 106.8737\n",
      "Epoch 709/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.7853 - val_loss: 106.8724\n",
      "Epoch 710/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.7583 - val_loss: 106.8975\n",
      "Epoch 711/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.7415 - val_loss: 106.9336\n",
      "Epoch 712/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.7619 - val_loss: 106.9588\n",
      "Epoch 713/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.7255 - val_loss: 106.9631\n",
      "Epoch 714/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.7162 - val_loss: 106.9670\n",
      "Epoch 715/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.7039 - val_loss: 106.9663\n",
      "Epoch 716/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.6974 - val_loss: 106.9617\n",
      "Epoch 717/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.6802 - val_loss: 106.9416\n",
      "Epoch 718/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.6736 - val_loss: 106.9179\n",
      "Epoch 719/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.6566 - val_loss: 106.9007\n",
      "Epoch 720/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.6428 - val_loss: 106.8816\n",
      "Epoch 721/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.6308 - val_loss: 106.8567\n",
      "Epoch 722/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.6124 - val_loss: 106.8149\n",
      "Epoch 723/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.5914 - val_loss: 106.7594\n",
      "Epoch 724/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.5783 - val_loss: 106.6958\n",
      "Epoch 725/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.6152 - val_loss: 106.6584\n",
      "Epoch 726/1500\n",
      "87/87 [==============================] - 0s 112us/step - loss: 40.6351 - val_loss: 106.6468\n",
      "Epoch 727/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.6074 - val_loss: 106.6664\n",
      "Epoch 728/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 40.5607 - val_loss: 106.7142\n",
      "Epoch 729/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.5534 - val_loss: 106.7455\n",
      "Epoch 730/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.5299 - val_loss: 106.7542\n",
      "Epoch 731/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.5100 - val_loss: 106.7478\n",
      "Epoch 732/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.4960 - val_loss: 106.7393\n",
      "Epoch 733/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.5021 - val_loss: 106.7423\n",
      "Epoch 734/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.4567 - val_loss: 106.7784\n",
      "Epoch 735/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.4881 - val_loss: 106.8129\n",
      "Epoch 736/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.4706 - val_loss: 106.8119\n",
      "Epoch 737/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.4581 - val_loss: 106.7921\n",
      "Epoch 738/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.4474 - val_loss: 106.7735\n",
      "Epoch 739/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.4300 - val_loss: 106.7696\n",
      "Epoch 740/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.4330 - val_loss: 106.7659\n",
      "Epoch 741/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.4043 - val_loss: 106.7320\n",
      "Epoch 742/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.3777 - val_loss: 106.6853\n",
      "Epoch 743/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.3829 - val_loss: 106.6306\n",
      "Epoch 744/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.3545 - val_loss: 106.5909\n",
      "Epoch 745/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.3364 - val_loss: 106.5497\n",
      "Epoch 746/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.3238 - val_loss: 106.4998\n",
      "Epoch 747/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.3463 - val_loss: 106.4656\n",
      "Epoch 748/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.3223 - val_loss: 106.4713\n",
      "Epoch 749/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.2925 - val_loss: 106.4788\n",
      "Epoch 750/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.2880 - val_loss: 106.4889\n",
      "Epoch 751/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.2717 - val_loss: 106.4887\n",
      "Epoch 752/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.2587 - val_loss: 106.4861\n",
      "Epoch 753/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.2606 - val_loss: 106.4781\n",
      "Epoch 754/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.2314 - val_loss: 106.4469\n",
      "Epoch 755/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.2378 - val_loss: 106.4223\n",
      "Epoch 756/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.2170 - val_loss: 106.4265\n",
      "Epoch 757/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.1962 - val_loss: 106.4445\n",
      "Epoch 758/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.1804 - val_loss: 106.4788\n",
      "Epoch 759/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 40.1674 - val_loss: 106.5108\n",
      "Epoch 760/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.1698 - val_loss: 106.5376\n",
      "Epoch 761/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 40.1942 - val_loss: 106.5443\n",
      "Epoch 762/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.1619 - val_loss: 106.5216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 763/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.1486 - val_loss: 106.4910\n",
      "Epoch 764/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.1268 - val_loss: 106.4618\n",
      "Epoch 765/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.1120 - val_loss: 106.4299\n",
      "Epoch 766/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.0942 - val_loss: 106.4049\n",
      "Epoch 767/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 40.0804 - val_loss: 106.3808\n",
      "Epoch 768/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 40.0635 - val_loss: 106.3428\n",
      "Epoch 769/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 40.0605 - val_loss: 106.3062\n",
      "Epoch 770/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.0577 - val_loss: 106.2961\n",
      "Epoch 771/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.0359 - val_loss: 106.3113\n",
      "Epoch 772/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 40.0271 - val_loss: 106.3336\n",
      "Epoch 773/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 40.0056 - val_loss: 106.3437\n",
      "Epoch 774/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.9991 - val_loss: 106.3471\n",
      "Epoch 775/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.9914 - val_loss: 106.3409\n",
      "Epoch 776/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.9819 - val_loss: 106.3270\n",
      "Epoch 777/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.9693 - val_loss: 106.3078\n",
      "Epoch 778/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.9561 - val_loss: 106.2900\n",
      "Epoch 779/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 39.9411 - val_loss: 106.2717\n",
      "Epoch 780/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.9256 - val_loss: 106.2436\n",
      "Epoch 781/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.9188 - val_loss: 106.2197\n",
      "Epoch 782/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.9010 - val_loss: 106.2030\n",
      "Epoch 783/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.8880 - val_loss: 106.1821\n",
      "Epoch 784/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 39.8733 - val_loss: 106.1635\n",
      "Epoch 785/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.8593 - val_loss: 106.1365\n",
      "Epoch 786/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.8580 - val_loss: 106.1130\n",
      "Epoch 787/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.8551 - val_loss: 106.1057\n",
      "Epoch 788/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.8347 - val_loss: 106.1173\n",
      "Epoch 789/1500\n",
      "87/87 [==============================] - 0s 517us/step - loss: 39.8028 - val_loss: 106.1385\n",
      "Epoch 790/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.8028 - val_loss: 106.1632\n",
      "Epoch 791/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.7929 - val_loss: 106.1758\n",
      "Epoch 792/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.7876 - val_loss: 106.1878\n",
      "Epoch 793/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.7793 - val_loss: 106.2093\n",
      "Epoch 794/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.7886 - val_loss: 106.2405\n",
      "Epoch 795/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 39.8170 - val_loss: 106.2481\n",
      "Epoch 796/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 39.8163 - val_loss: 106.2328\n",
      "Epoch 797/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.7975 - val_loss: 106.2129\n",
      "Epoch 798/1500\n",
      "87/87 [==============================] - 0s 81us/step - loss: 39.7662 - val_loss: 106.1666\n",
      "Epoch 799/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.7242 - val_loss: 106.1068\n",
      "Epoch 800/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.6944 - val_loss: 106.0482\n",
      "Epoch 801/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 39.6777 - val_loss: 105.9978\n",
      "Epoch 802/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.6581 - val_loss: 105.9652\n",
      "Epoch 803/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.6451 - val_loss: 105.9487\n",
      "Epoch 804/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 39.6316 - val_loss: 105.9452\n",
      "Epoch 805/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.6181 - val_loss: 105.9520\n",
      "Epoch 806/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.6240 - val_loss: 105.9558\n",
      "Epoch 807/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.6003 - val_loss: 105.9386\n",
      "Epoch 808/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.5847 - val_loss: 105.9108\n",
      "Epoch 809/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.5754 - val_loss: 105.8820\n",
      "Epoch 810/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.5705 - val_loss: 105.8549\n",
      "Epoch 811/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.5776 - val_loss: 105.8317\n",
      "Epoch 812/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.5687 - val_loss: 105.8310\n",
      "Epoch 813/1500\n",
      "87/87 [==============================] - 0s 150us/step - loss: 39.5454 - val_loss: 105.8357\n",
      "Epoch 814/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.5241 - val_loss: 105.8380\n",
      "Epoch 815/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 39.5119 - val_loss: 105.8418\n",
      "Epoch 816/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.4959 - val_loss: 105.8405\n",
      "Epoch 817/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.4881 - val_loss: 105.8414\n",
      "Epoch 818/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 39.4776 - val_loss: 105.8483\n",
      "Epoch 819/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.4730 - val_loss: 105.8519\n",
      "Epoch 820/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.4739 - val_loss: 105.8480\n",
      "Epoch 821/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 39.4578 - val_loss: 105.8287\n",
      "Epoch 822/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 39.4475 - val_loss: 105.7981\n",
      "Epoch 823/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.4225 - val_loss: 105.7455\n",
      "Epoch 824/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 47.89 - 0s 115us/step - loss: 39.4206 - val_loss: 105.7042\n",
      "Epoch 825/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.4234 - val_loss: 105.6944\n",
      "Epoch 826/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.4030 - val_loss: 105.7119\n",
      "Epoch 827/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.3758 - val_loss: 105.7344\n",
      "Epoch 828/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.3703 - val_loss: 105.7604\n",
      "Epoch 829/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.3657 - val_loss: 105.7728\n",
      "Epoch 830/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.3857 - val_loss: 105.7716\n",
      "Epoch 831/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.3551 - val_loss: 105.7363\n",
      "Epoch 832/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 39.3253 - val_loss: 105.6812\n",
      "Epoch 833/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.3135 - val_loss: 105.6208\n",
      "Epoch 834/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.3433 - val_loss: 105.5859\n",
      "Epoch 835/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.3621 - val_loss: 105.5935\n",
      "Epoch 836/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.3232 - val_loss: 105.6251\n",
      "Epoch 837/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 39.2691 - val_loss: 105.6596\n",
      "Epoch 838/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.2676 - val_loss: 105.6969\n",
      "Epoch 839/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 149us/step - loss: 39.2988 - val_loss: 105.7157\n",
      "Epoch 840/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.3013 - val_loss: 105.7104\n",
      "Epoch 841/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.2935 - val_loss: 105.7050\n",
      "Epoch 842/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.2908 - val_loss: 105.6927\n",
      "Epoch 843/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.2753 - val_loss: 105.6728\n",
      "Epoch 844/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.2559 - val_loss: 105.6362\n",
      "Epoch 845/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.2156 - val_loss: 105.5885\n",
      "Epoch 846/1500\n",
      "87/87 [==============================] - 0s 109us/step - loss: 39.1781 - val_loss: 105.5359\n",
      "Epoch 847/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.1859 - val_loss: 105.4865\n",
      "Epoch 848/1500\n",
      "87/87 [==============================] - 0s 141us/step - loss: 39.2083 - val_loss: 105.4635\n",
      "Epoch 849/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 39.2259 - val_loss: 105.4676\n",
      "Epoch 850/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 39.1842 - val_loss: 105.5074\n",
      "Epoch 851/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.1255 - val_loss: 105.5557\n",
      "Epoch 852/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.1726 - val_loss: 105.5882\n",
      "Epoch 853/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.1769 - val_loss: 105.5834\n",
      "Epoch 854/1500\n",
      "87/87 [==============================] - 0s 150us/step - loss: 39.1656 - val_loss: 105.5563\n",
      "Epoch 855/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.1397 - val_loss: 105.5125\n",
      "Epoch 856/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 39.1112 - val_loss: 105.4642\n",
      "Epoch 857/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.0842 - val_loss: 105.4338\n",
      "Epoch 858/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.0872 - val_loss: 105.4224\n",
      "Epoch 859/1500\n",
      "87/87 [==============================] - 0s 150us/step - loss: 39.0784 - val_loss: 105.4282\n",
      "Epoch 860/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.0541 - val_loss: 105.4425\n",
      "Epoch 861/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 39.0359 - val_loss: 105.4591\n",
      "Epoch 862/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.0568 - val_loss: 105.4625\n",
      "Epoch 863/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 39.0546 - val_loss: 105.4335\n",
      "Epoch 864/1500\n",
      "87/87 [==============================] - 0s 207us/step - loss: 39.0101 - val_loss: 105.3730\n",
      "Epoch 865/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 39.0208 - val_loss: 105.3263\n",
      "Epoch 866/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 39.0661 - val_loss: 105.3181\n",
      "Epoch 867/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 39.0620 - val_loss: 105.3464\n",
      "Epoch 868/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.9822 - val_loss: 105.3775\n",
      "Epoch 869/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 33.94 - 0s 115us/step - loss: 38.9684 - val_loss: 105.4066\n",
      "Epoch 870/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 39.0052 - val_loss: 105.4220\n",
      "Epoch 871/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 39.0135 - val_loss: 105.4025\n",
      "Epoch 872/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.9840 - val_loss: 105.3581\n",
      "Epoch 873/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.9493 - val_loss: 105.3130\n",
      "Epoch 874/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.9359 - val_loss: 105.2789\n",
      "Epoch 875/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.9427 - val_loss: 105.2660\n",
      "Epoch 876/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.9446 - val_loss: 105.2658\n",
      "Epoch 877/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.9196 - val_loss: 105.2692\n",
      "Epoch 878/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.8993 - val_loss: 105.2893\n",
      "Epoch 879/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.9040 - val_loss: 105.3172\n",
      "Epoch 880/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.9090 - val_loss: 105.3201\n",
      "Epoch 881/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.8995 - val_loss: 105.2955\n",
      "Epoch 882/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.8904 - val_loss: 105.2604\n",
      "Epoch 883/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.8653 - val_loss: 105.2367\n",
      "Epoch 884/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.8595 - val_loss: 105.2221\n",
      "Epoch 885/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.8500 - val_loss: 105.2166\n",
      "Epoch 886/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.8424 - val_loss: 105.2188\n",
      "Epoch 887/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.8455 - val_loss: 105.2252\n",
      "Epoch 888/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.8317 - val_loss: 105.2175\n",
      "Epoch 889/1500\n",
      "87/87 [==============================] - 0s 150us/step - loss: 38.8266 - val_loss: 105.2134\n",
      "Epoch 890/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 36.13 - 0s 218us/step - loss: 38.8171 - val_loss: 105.2167\n",
      "Epoch 891/1500\n",
      "87/87 [==============================] - 0s 184us/step - loss: 38.8118 - val_loss: 105.2256\n",
      "Epoch 892/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.8134 - val_loss: 105.2329\n",
      "Epoch 893/1500\n",
      "87/87 [==============================] - 0s 150us/step - loss: 38.8125 - val_loss: 105.2301\n",
      "Epoch 894/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.8051 - val_loss: 105.2228\n",
      "Epoch 895/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.7972 - val_loss: 105.2030\n",
      "Epoch 896/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.7848 - val_loss: 105.1758\n",
      "Epoch 897/1500\n",
      "87/87 [==============================] - 0s 184us/step - loss: 38.7683 - val_loss: 105.1533\n",
      "Epoch 898/1500\n",
      "87/87 [==============================] - 0s 195us/step - loss: 38.7585 - val_loss: 105.1294\n",
      "Epoch 899/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.7528 - val_loss: 105.1029\n",
      "Epoch 900/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.7538 - val_loss: 105.0775\n",
      "Epoch 901/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.7724 - val_loss: 105.0833\n",
      "Epoch 902/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.7438 - val_loss: 105.1138\n",
      "Epoch 903/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.7503 - val_loss: 105.1314\n",
      "Epoch 904/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.7289 - val_loss: 105.1108\n",
      "Epoch 905/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.7165 - val_loss: 105.0710\n",
      "Epoch 906/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.7345 - val_loss: 105.0531\n",
      "Epoch 907/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.7209 - val_loss: 105.0733\n",
      "Epoch 908/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.6960 - val_loss: 105.0904\n",
      "Epoch 909/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.6881 - val_loss: 105.1133\n",
      "Epoch 910/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.6981 - val_loss: 105.1278\n",
      "Epoch 911/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.6928 - val_loss: 105.1170\n",
      "Epoch 912/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.7123 - val_loss: 105.1040\n",
      "Epoch 913/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.6752 - val_loss: 105.1118\n",
      "Epoch 914/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.6751 - val_loss: 105.1107\n",
      "Epoch 915/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 149us/step - loss: 38.6758 - val_loss: 105.0979\n",
      "Epoch 916/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.6545 - val_loss: 105.0610\n",
      "Epoch 917/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.6355 - val_loss: 105.0115\n",
      "Epoch 918/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.6548 - val_loss: 104.9770\n",
      "Epoch 919/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.6415 - val_loss: 104.9756\n",
      "Epoch 920/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.6233 - val_loss: 104.9893\n",
      "Epoch 921/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.6009 - val_loss: 105.0112\n",
      "Epoch 922/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.6026 - val_loss: 105.0340\n",
      "Epoch 923/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.6126 - val_loss: 105.0338\n",
      "Epoch 924/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.5980 - val_loss: 105.0163\n",
      "Epoch 925/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.5912 - val_loss: 105.0137\n",
      "Epoch 926/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.5710 - val_loss: 105.0278\n",
      "Epoch 927/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.5755 - val_loss: 105.0367\n",
      "Epoch 928/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.5878 - val_loss: 105.0319\n",
      "Epoch 929/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.5885 - val_loss: 105.0126\n",
      "Epoch 930/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.5732 - val_loss: 104.9670\n",
      "Epoch 931/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.5341 - val_loss: 104.9346\n",
      "Epoch 932/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.5318 - val_loss: 104.9148\n",
      "Epoch 933/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.5170 - val_loss: 104.9106\n",
      "Epoch 934/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.5082 - val_loss: 104.9108\n",
      "Epoch 935/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.5006 - val_loss: 104.9126\n",
      "Epoch 936/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.4912 - val_loss: 104.9144\n",
      "Epoch 937/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.4889 - val_loss: 104.9158\n",
      "Epoch 938/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.4921 - val_loss: 104.9030\n",
      "Epoch 939/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.4838 - val_loss: 104.8734\n",
      "Epoch 940/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.4718 - val_loss: 104.8562\n",
      "Epoch 941/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.4666 - val_loss: 104.8487\n",
      "Epoch 942/1500\n",
      "87/87 [==============================] - 0s 127us/step - loss: 38.4578 - val_loss: 104.8351\n",
      "Epoch 943/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.4551 - val_loss: 104.8179\n",
      "Epoch 944/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.4575 - val_loss: 104.8110\n",
      "Epoch 945/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.4543 - val_loss: 104.8162\n",
      "Epoch 946/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.4433 - val_loss: 104.8376\n",
      "Epoch 947/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.4416 - val_loss: 104.8530\n",
      "Epoch 948/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.4422 - val_loss: 104.8489\n",
      "Epoch 949/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.4308 - val_loss: 104.8252\n",
      "Epoch 950/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.4215 - val_loss: 104.7950\n",
      "Epoch 951/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.4319 - val_loss: 104.7711\n",
      "Epoch 952/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.4309 - val_loss: 104.7686\n",
      "Epoch 953/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.4262 - val_loss: 104.7799\n",
      "Epoch 954/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.4141 - val_loss: 104.8002\n",
      "Epoch 955/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.4261 - val_loss: 104.8208\n",
      "Epoch 956/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.4212 - val_loss: 104.8121\n",
      "Epoch 957/1500\n",
      "87/87 [==============================] - 0s 150us/step - loss: 38.4146 - val_loss: 104.7839\n",
      "Epoch 958/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.4031 - val_loss: 104.7622\n",
      "Epoch 959/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.3973 - val_loss: 104.7484\n",
      "Epoch 960/1500\n",
      "87/87 [==============================] - 0s 184us/step - loss: 38.3919 - val_loss: 104.7407\n",
      "Epoch 961/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3946 - val_loss: 104.7477\n",
      "Epoch 962/1500\n",
      "87/87 [==============================] - 0s 207us/step - loss: 38.3765 - val_loss: 104.7755\n",
      "Epoch 963/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3791 - val_loss: 104.8064\n",
      "Epoch 964/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.4030 - val_loss: 104.8254\n",
      "Epoch 965/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.4248 - val_loss: 104.8295\n",
      "Epoch 966/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.4316 - val_loss: 104.8108\n",
      "Epoch 967/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.4102 - val_loss: 104.7822\n",
      "Epoch 968/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3846 - val_loss: 104.7376\n",
      "Epoch 969/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.3874 - val_loss: 104.6978\n",
      "Epoch 970/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3584 - val_loss: 104.6871\n",
      "Epoch 971/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.3524 - val_loss: 104.6890\n",
      "Epoch 972/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.3525 - val_loss: 104.6867\n",
      "Epoch 973/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3405 - val_loss: 104.7025\n",
      "Epoch 974/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.3595 - val_loss: 104.7153\n",
      "Epoch 975/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.3515 - val_loss: 104.6952\n",
      "Epoch 976/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.3299 - val_loss: 104.6513\n",
      "Epoch 977/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.3439 - val_loss: 104.5993\n",
      "Epoch 978/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.3706 - val_loss: 104.5823\n",
      "Epoch 979/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.3887 - val_loss: 104.5904\n",
      "Epoch 980/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.3606 - val_loss: 104.6017\n",
      "Epoch 981/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.3363 - val_loss: 104.6182\n",
      "Epoch 982/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.3180 - val_loss: 104.6361\n",
      "Epoch 983/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3119 - val_loss: 104.6554\n",
      "Epoch 984/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.3089 - val_loss: 104.6831\n",
      "Epoch 985/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.3342 - val_loss: 104.6956\n",
      "Epoch 986/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.3348 - val_loss: 104.6793\n",
      "Epoch 987/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.3165 - val_loss: 104.6458\n",
      "Epoch 988/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3044 - val_loss: 104.6091\n",
      "Epoch 989/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3106 - val_loss: 104.5854\n",
      "Epoch 990/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.3135 - val_loss: 104.5840\n",
      "Epoch 991/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2952 - val_loss: 104.6086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 992/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.2909 - val_loss: 104.6382\n",
      "Epoch 993/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.3239 - val_loss: 104.6471\n",
      "Epoch 994/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.3284 - val_loss: 104.6306\n",
      "Epoch 995/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2981 - val_loss: 104.6292\n",
      "Epoch 996/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.3023 - val_loss: 104.6153\n",
      "Epoch 997/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2878 - val_loss: 104.5777\n",
      "Epoch 998/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.2762 - val_loss: 104.5472\n",
      "Epoch 999/1500\n",
      "87/87 [==============================] - 0s 184us/step - loss: 38.2830 - val_loss: 104.5315\n",
      "Epoch 1000/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.2799 - val_loss: 104.5306\n",
      "Epoch 1001/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2812 - val_loss: 104.5282\n",
      "Epoch 1002/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2730 - val_loss: 104.5184\n",
      "Epoch 1003/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2733 - val_loss: 104.5111\n",
      "Epoch 1004/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.2735 - val_loss: 104.5054\n",
      "Epoch 1005/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2700 - val_loss: 104.5109\n",
      "Epoch 1006/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2628 - val_loss: 104.5232\n",
      "Epoch 1007/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.2609 - val_loss: 104.5475\n",
      "Epoch 1008/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.2744 - val_loss: 104.5428\n",
      "Epoch 1009/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2459 - val_loss: 104.5044\n",
      "Epoch 1010/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.2405 - val_loss: 104.4663\n",
      "Epoch 1011/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2858 - val_loss: 104.4532\n",
      "Epoch 1012/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.2777 - val_loss: 104.4735\n",
      "Epoch 1013/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2556 - val_loss: 104.4883\n",
      "Epoch 1014/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2398 - val_loss: 104.4894\n",
      "Epoch 1015/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.2466 - val_loss: 104.4925\n",
      "Epoch 1016/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.2291 - val_loss: 104.5186\n",
      "Epoch 1017/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.2550 - val_loss: 104.5422\n",
      "Epoch 1018/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2638 - val_loss: 104.5310\n",
      "Epoch 1019/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2481 - val_loss: 104.4919\n",
      "Epoch 1020/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.2306 - val_loss: 104.4561\n",
      "Epoch 1021/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2248 - val_loss: 104.4342\n",
      "Epoch 1022/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2344 - val_loss: 104.4139\n",
      "Epoch 1023/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.2446 - val_loss: 104.3969\n",
      "Epoch 1024/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2539 - val_loss: 104.4074\n",
      "Epoch 1025/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2448 - val_loss: 104.4218\n",
      "Epoch 1026/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.2330 - val_loss: 104.4292\n",
      "Epoch 1027/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2246 - val_loss: 104.4374\n",
      "Epoch 1028/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2239 - val_loss: 104.4574\n",
      "Epoch 1029/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2154 - val_loss: 104.4627\n",
      "Epoch 1030/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 38.2171 - val_loss: 104.4635\n",
      "Epoch 1031/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2172 - val_loss: 104.4560\n",
      "Epoch 1032/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.2143 - val_loss: 104.4372\n",
      "Epoch 1033/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.2098 - val_loss: 104.4091\n",
      "Epoch 1034/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2225 - val_loss: 104.3976\n",
      "Epoch 1035/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.2154 - val_loss: 104.4061\n",
      "Epoch 1036/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.2058 - val_loss: 104.4231\n",
      "Epoch 1037/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.2044 - val_loss: 104.4397\n",
      "Epoch 1038/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.2028 - val_loss: 104.4400\n",
      "Epoch 1039/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.2037 - val_loss: 104.4237\n",
      "Epoch 1040/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1927 - val_loss: 104.3939\n",
      "Epoch 1041/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1926 - val_loss: 104.3664\n",
      "Epoch 1042/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.2161 - val_loss: 104.3512\n",
      "Epoch 1043/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.2143 - val_loss: 104.3634\n",
      "Epoch 1044/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1945 - val_loss: 104.3962\n",
      "Epoch 1045/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1848 - val_loss: 104.4288\n",
      "Epoch 1046/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.2115 - val_loss: 104.4479\n",
      "Epoch 1047/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.2136 - val_loss: 104.4415\n",
      "Epoch 1048/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.2084 - val_loss: 104.4211\n",
      "Epoch 1049/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1946 - val_loss: 104.3893\n",
      "Epoch 1050/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1976 - val_loss: 104.3585\n",
      "Epoch 1051/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1810 - val_loss: 104.3465\n",
      "Epoch 1052/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1824 - val_loss: 104.3412\n",
      "Epoch 1053/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1962 - val_loss: 104.3367\n",
      "Epoch 1054/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1843 - val_loss: 104.3244\n",
      "Epoch 1055/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1891 - val_loss: 104.3285\n",
      "Epoch 1056/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1821 - val_loss: 104.3312\n",
      "Epoch 1057/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.1832 - val_loss: 104.3384\n",
      "Epoch 1058/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1724 - val_loss: 104.3659\n",
      "Epoch 1059/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.1757 - val_loss: 104.3791\n",
      "Epoch 1060/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1792 - val_loss: 104.3839\n",
      "Epoch 1061/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1832 - val_loss: 104.3861\n",
      "Epoch 1062/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1821 - val_loss: 104.3763\n",
      "Epoch 1063/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1744 - val_loss: 104.3635\n",
      "Epoch 1064/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1664 - val_loss: 104.3414\n",
      "Epoch 1065/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.1713 - val_loss: 104.3198\n",
      "Epoch 1066/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1547 - val_loss: 104.3233\n",
      "Epoch 1067/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1494 - val_loss: 104.3333\n",
      "Epoch 1068/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 92us/step - loss: 38.1564 - val_loss: 104.3448\n",
      "Epoch 1069/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1567 - val_loss: 104.3384\n",
      "Epoch 1070/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1537 - val_loss: 104.3270\n",
      "Epoch 1071/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1492 - val_loss: 104.3091\n",
      "Epoch 1072/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1555 - val_loss: 104.2844\n",
      "Epoch 1073/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1504 - val_loss: 104.2884\n",
      "Epoch 1074/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1414 - val_loss: 104.3084\n",
      "Epoch 1075/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1379 - val_loss: 104.3382\n",
      "Epoch 1076/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.1528 - val_loss: 104.3783\n",
      "Epoch 1077/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.2223 - val_loss: 104.3879\n",
      "Epoch 1078/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.2066 - val_loss: 104.3557\n",
      "Epoch 1079/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1660 - val_loss: 104.3181\n",
      "Epoch 1080/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1304 - val_loss: 104.2700\n",
      "Epoch 1081/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1580 - val_loss: 104.2290\n",
      "Epoch 1082/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1600 - val_loss: 104.2222\n",
      "Epoch 1083/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1573 - val_loss: 104.2364\n",
      "Epoch 1084/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1430 - val_loss: 104.2691\n",
      "Epoch 1085/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1367 - val_loss: 104.3019\n",
      "Epoch 1086/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1486 - val_loss: 104.3066\n",
      "Epoch 1087/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1358 - val_loss: 104.2789\n",
      "Epoch 1088/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.1164 - val_loss: 104.2321\n",
      "Epoch 1089/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 38.1230 - val_loss: 104.1889\n",
      "Epoch 1090/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1513 - val_loss: 104.1491\n",
      "Epoch 1091/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 45.08 - 0s 115us/step - loss: 38.2015 - val_loss: 104.1244\n",
      "Epoch 1092/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1990 - val_loss: 104.1395\n",
      "Epoch 1093/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1656 - val_loss: 104.1830\n",
      "Epoch 1094/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1308 - val_loss: 104.2460\n",
      "Epoch 1095/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1247 - val_loss: 104.2952\n",
      "Epoch 1096/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1620 - val_loss: 104.3008\n",
      "Epoch 1097/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1493 - val_loss: 104.2672\n",
      "Epoch 1098/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1134 - val_loss: 104.2201\n",
      "Epoch 1099/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1149 - val_loss: 104.1701\n",
      "Epoch 1100/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1319 - val_loss: 104.1416\n",
      "Epoch 1101/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 38.1457 - val_loss: 104.1256\n",
      "Epoch 1102/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1722 - val_loss: 104.1176\n",
      "Epoch 1103/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1519 - val_loss: 104.1523\n",
      "Epoch 1104/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1511 - val_loss: 104.2063\n",
      "Epoch 1105/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 37.65 - 0s 126us/step - loss: 38.1483 - val_loss: 104.2295\n",
      "Epoch 1106/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1221 - val_loss: 104.2230\n",
      "Epoch 1107/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1194 - val_loss: 104.2100\n",
      "Epoch 1108/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.1165 - val_loss: 104.1866\n",
      "Epoch 1109/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.1095 - val_loss: 104.1685\n",
      "Epoch 1110/1500\n",
      "87/87 [==============================] - 0s 127us/step - loss: 38.1045 - val_loss: 104.1545\n",
      "Epoch 1111/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.1094 - val_loss: 104.1343\n",
      "Epoch 1112/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.1255 - val_loss: 104.1108\n",
      "Epoch 1113/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.1277 - val_loss: 104.1148\n",
      "Epoch 1114/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.1179 - val_loss: 104.1355\n",
      "Epoch 1115/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.1138 - val_loss: 104.1559\n",
      "Epoch 1116/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0939 - val_loss: 104.1586\n",
      "Epoch 1117/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.1001 - val_loss: 104.1613\n",
      "Epoch 1118/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 32.78 - 0s 126us/step - loss: 38.0950 - val_loss: 104.1736\n",
      "Epoch 1119/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0967 - val_loss: 104.1798\n",
      "Epoch 1120/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0955 - val_loss: 104.1775\n",
      "Epoch 1121/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.0943 - val_loss: 104.1753\n",
      "Epoch 1122/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0935 - val_loss: 104.1745\n",
      "Epoch 1123/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0820 - val_loss: 104.1541\n",
      "Epoch 1124/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0718 - val_loss: 104.1430\n",
      "Epoch 1125/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0726 - val_loss: 104.1457\n",
      "Epoch 1126/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.0655 - val_loss: 104.1664\n",
      "Epoch 1127/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0919 - val_loss: 104.1758\n",
      "Epoch 1128/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0808 - val_loss: 104.1483\n",
      "Epoch 1129/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.0726 - val_loss: 104.1142\n",
      "Epoch 1130/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0763 - val_loss: 104.0934\n",
      "Epoch 1131/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0868 - val_loss: 104.0827\n",
      "Epoch 1132/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0862 - val_loss: 104.0862\n",
      "Epoch 1133/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0798 - val_loss: 104.0948\n",
      "Epoch 1134/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0795 - val_loss: 104.1114\n",
      "Epoch 1135/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0685 - val_loss: 104.1168\n",
      "Epoch 1136/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0665 - val_loss: 104.1209\n",
      "Epoch 1137/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0608 - val_loss: 104.1372\n",
      "Epoch 1138/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0711 - val_loss: 104.1336\n",
      "Epoch 1139/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0673 - val_loss: 104.1124\n",
      "Epoch 1140/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0532 - val_loss: 104.0880\n",
      "Epoch 1141/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0549 - val_loss: 104.0685\n",
      "Epoch 1142/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0651 - val_loss: 104.0673\n",
      "Epoch 1143/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 103us/step - loss: 38.0691 - val_loss: 104.0784\n",
      "Epoch 1144/1500\n",
      "87/87 [==============================] - 0s 140us/step - loss: 38.0540 - val_loss: 104.0797\n",
      "Epoch 1145/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0536 - val_loss: 104.0893\n",
      "Epoch 1146/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0440 - val_loss: 104.1116\n",
      "Epoch 1147/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0584 - val_loss: 104.1234\n",
      "Epoch 1148/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0681 - val_loss: 104.1289\n",
      "Epoch 1149/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0741 - val_loss: 104.1257\n",
      "Epoch 1150/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0713 - val_loss: 104.1094\n",
      "Epoch 1151/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0637 - val_loss: 104.0772\n",
      "Epoch 1152/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0419 - val_loss: 104.0528\n",
      "Epoch 1153/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0669 - val_loss: 104.0493\n",
      "Epoch 1154/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0417 - val_loss: 104.0772\n",
      "Epoch 1155/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0599 - val_loss: 104.0876\n",
      "Epoch 1156/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.0556 - val_loss: 104.0748\n",
      "Epoch 1157/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0435 - val_loss: 104.0592\n",
      "Epoch 1158/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.0324 - val_loss: 104.0304\n",
      "Epoch 1159/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0369 - val_loss: 103.9988\n",
      "Epoch 1160/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0625 - val_loss: 103.9788\n",
      "Epoch 1161/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0741 - val_loss: 103.9735\n",
      "Epoch 1162/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0686 - val_loss: 103.9595\n",
      "Epoch 1163/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0822 - val_loss: 103.9539\n",
      "Epoch 1164/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0814 - val_loss: 103.9671\n",
      "Epoch 1165/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0646 - val_loss: 103.9798\n",
      "Epoch 1166/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.0499 - val_loss: 103.9987\n",
      "Epoch 1167/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0437 - val_loss: 104.0206\n",
      "Epoch 1168/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0511 - val_loss: 104.0236\n",
      "Epoch 1169/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0385 - val_loss: 104.0019\n",
      "Epoch 1170/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.0366 - val_loss: 103.9923\n",
      "Epoch 1171/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 38.0374 - val_loss: 103.9925\n",
      "Epoch 1172/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0391 - val_loss: 103.9982\n",
      "Epoch 1173/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0350 - val_loss: 103.9922\n",
      "Epoch 1174/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0343 - val_loss: 103.9770\n",
      "Epoch 1175/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0351 - val_loss: 103.9528\n",
      "Epoch 1176/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0583 - val_loss: 103.9410\n",
      "Epoch 1177/1500\n",
      "87/87 [==============================] - 0s 158us/step - loss: 38.0463 - val_loss: 103.9548\n",
      "Epoch 1178/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 38.0379 - val_loss: 103.9788\n",
      "Epoch 1179/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0293 - val_loss: 104.0048\n",
      "Epoch 1180/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0292 - val_loss: 104.0175\n",
      "Epoch 1181/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0302 - val_loss: 104.0222\n",
      "Epoch 1182/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0349 - val_loss: 104.0214\n",
      "Epoch 1183/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0398 - val_loss: 104.0153\n",
      "Epoch 1184/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0385 - val_loss: 104.0158\n",
      "Epoch 1185/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0217 - val_loss: 103.9935\n",
      "Epoch 1186/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 34.06 - 0s 115us/step - loss: 38.0224 - val_loss: 103.9694\n",
      "Epoch 1187/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0182 - val_loss: 103.9577\n",
      "Epoch 1188/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0208 - val_loss: 103.9418\n",
      "Epoch 1189/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0264 - val_loss: 103.9363\n",
      "Epoch 1190/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0259 - val_loss: 103.9395\n",
      "Epoch 1191/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0212 - val_loss: 103.9494\n",
      "Epoch 1192/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0137 - val_loss: 103.9607\n",
      "Epoch 1193/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 26.05 - 0s 126us/step - loss: 38.0095 - val_loss: 103.9730\n",
      "Epoch 1194/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 35.62 - 0s 138us/step - loss: 38.0061 - val_loss: 103.9811\n",
      "Epoch 1195/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0141 - val_loss: 103.9852\n",
      "Epoch 1196/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0088 - val_loss: 103.9677\n",
      "Epoch 1197/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0047 - val_loss: 103.9399\n",
      "Epoch 1198/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0055 - val_loss: 103.9212\n",
      "Epoch 1199/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.0193 - val_loss: 103.9196\n",
      "Epoch 1200/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0035 - val_loss: 103.9454\n",
      "Epoch 1201/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9947 - val_loss: 103.9809\n",
      "Epoch 1202/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0223 - val_loss: 103.9980\n",
      "Epoch 1203/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0198 - val_loss: 103.9807\n",
      "Epoch 1204/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9960 - val_loss: 103.9390\n",
      "Epoch 1205/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9943 - val_loss: 103.9002\n",
      "Epoch 1206/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0097 - val_loss: 103.8780\n",
      "Epoch 1207/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0175 - val_loss: 103.8778\n",
      "Epoch 1208/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0118 - val_loss: 103.8993\n",
      "Epoch 1209/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0014 - val_loss: 103.9371\n",
      "Epoch 1210/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0016 - val_loss: 103.9594\n",
      "Epoch 1211/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0000 - val_loss: 103.9572\n",
      "Epoch 1212/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9919 - val_loss: 103.9363\n",
      "Epoch 1213/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.9825 - val_loss: 103.9051\n",
      "Epoch 1214/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9862 - val_loss: 103.8748\n",
      "Epoch 1215/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9905 - val_loss: 103.8545\n",
      "Epoch 1216/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9973 - val_loss: 103.8321\n",
      "Epoch 1217/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0123 - val_loss: 103.8125\n",
      "Epoch 1218/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 103us/step - loss: 38.0209 - val_loss: 103.8108\n",
      "Epoch 1219/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 38.0253 - val_loss: 103.8209\n",
      "Epoch 1220/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0164 - val_loss: 103.8207\n",
      "Epoch 1221/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0014 - val_loss: 103.8062\n",
      "Epoch 1222/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0180 - val_loss: 103.8015\n",
      "Epoch 1223/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0001 - val_loss: 103.8303\n",
      "Epoch 1224/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9868 - val_loss: 103.8806\n",
      "Epoch 1225/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9711 - val_loss: 103.9260\n",
      "Epoch 1226/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 38.0298 - val_loss: 103.9602\n",
      "Epoch 1227/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0323 - val_loss: 103.9582\n",
      "Epoch 1228/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0307 - val_loss: 103.9390\n",
      "Epoch 1229/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 38.0138 - val_loss: 103.9140\n",
      "Epoch 1230/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9891 - val_loss: 103.8856\n",
      "Epoch 1231/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9900 - val_loss: 103.8438\n",
      "Epoch 1232/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9854 - val_loss: 103.8190\n",
      "Epoch 1233/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9737 - val_loss: 103.8175\n",
      "Epoch 1234/1500\n",
      "87/87 [==============================] - 0s 195us/step - loss: 37.9717 - val_loss: 103.8268\n",
      "Epoch 1235/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9639 - val_loss: 103.8356\n",
      "Epoch 1236/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9619 - val_loss: 103.8496\n",
      "Epoch 1237/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9672 - val_loss: 103.8634\n",
      "Epoch 1238/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9675 - val_loss: 103.8740\n",
      "Epoch 1239/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9668 - val_loss: 103.8985\n",
      "Epoch 1240/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9767 - val_loss: 103.9126\n",
      "Epoch 1241/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9958 - val_loss: 103.9113\n",
      "Epoch 1242/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 38.0049 - val_loss: 103.8877\n",
      "Epoch 1243/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9628 - val_loss: 103.8271\n",
      "Epoch 1244/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9662 - val_loss: 103.7733\n",
      "Epoch 1245/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9804 - val_loss: 103.7523\n",
      "Epoch 1246/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9875 - val_loss: 103.7485\n",
      "Epoch 1247/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.9870 - val_loss: 103.7467\n",
      "Epoch 1248/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 37.9908 - val_loss: 103.7585\n",
      "Epoch 1249/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9585 - val_loss: 103.8025\n",
      "Epoch 1250/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9569 - val_loss: 103.8494\n",
      "Epoch 1251/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9520 - val_loss: 103.8790\n",
      "Epoch 1252/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9853 - val_loss: 103.9023\n",
      "Epoch 1253/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 38.0157 - val_loss: 103.8932\n",
      "Epoch 1254/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 38.0074 - val_loss: 103.8609\n",
      "Epoch 1255/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.9695 - val_loss: 103.8292\n",
      "Epoch 1256/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9637 - val_loss: 103.8042\n",
      "Epoch 1257/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9504 - val_loss: 103.7936\n",
      "Epoch 1258/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9397 - val_loss: 103.7972\n",
      "Epoch 1259/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9400 - val_loss: 103.8017\n",
      "Epoch 1260/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9478 - val_loss: 103.8040\n",
      "Epoch 1261/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9454 - val_loss: 103.8145\n",
      "Epoch 1262/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9480 - val_loss: 103.8075\n",
      "Epoch 1263/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9415 - val_loss: 103.7843\n",
      "Epoch 1264/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9343 - val_loss: 103.7613\n",
      "Epoch 1265/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9394 - val_loss: 103.7465\n",
      "Epoch 1266/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9345 - val_loss: 103.7513\n",
      "Epoch 1267/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9288 - val_loss: 103.7634\n",
      "Epoch 1268/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9261 - val_loss: 103.7767\n",
      "Epoch 1269/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9288 - val_loss: 103.7911\n",
      "Epoch 1270/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9341 - val_loss: 103.8041\n",
      "Epoch 1271/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9489 - val_loss: 103.8078\n",
      "Epoch 1272/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9434 - val_loss: 103.7837\n",
      "Epoch 1273/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9309 - val_loss: 103.7477\n",
      "Epoch 1274/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9170 - val_loss: 103.7218\n",
      "Epoch 1275/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.9364 - val_loss: 103.7150\n",
      "Epoch 1276/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9293 - val_loss: 103.7329\n",
      "Epoch 1277/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 37.9361 - val_loss: 103.7540\n",
      "Epoch 1278/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9230 - val_loss: 103.7509\n",
      "Epoch 1279/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9208 - val_loss: 103.7466\n",
      "Epoch 1280/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9186 - val_loss: 103.7396\n",
      "Epoch 1281/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9169 - val_loss: 103.7323\n",
      "Epoch 1282/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9154 - val_loss: 103.7257\n",
      "Epoch 1283/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9152 - val_loss: 103.7109\n",
      "Epoch 1284/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9160 - val_loss: 103.7117\n",
      "Epoch 1285/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9152 - val_loss: 103.7066\n",
      "Epoch 1286/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.9151 - val_loss: 103.7018\n",
      "Epoch 1287/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 37.9122 - val_loss: 103.7155\n",
      "Epoch 1288/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9109 - val_loss: 103.7256\n",
      "Epoch 1289/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9102 - val_loss: 103.7205\n",
      "Epoch 1290/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.9111 - val_loss: 103.7119\n",
      "Epoch 1291/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9035 - val_loss: 103.7063\n",
      "Epoch 1292/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9057 - val_loss: 103.7002\n",
      "Epoch 1293/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9042 - val_loss: 103.7035\n",
      "Epoch 1294/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 115us/step - loss: 37.9066 - val_loss: 103.7065\n",
      "Epoch 1295/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.9106 - val_loss: 103.7153\n",
      "Epoch 1296/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9031 - val_loss: 103.7084\n",
      "Epoch 1297/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9063 - val_loss: 103.7031\n",
      "Epoch 1298/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9100 - val_loss: 103.7030\n",
      "Epoch 1299/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9022 - val_loss: 103.6875\n",
      "Epoch 1300/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8993 - val_loss: 103.6870\n",
      "Epoch 1301/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9007 - val_loss: 103.6912\n",
      "Epoch 1302/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8939 - val_loss: 103.6894\n",
      "Epoch 1303/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.8938 - val_loss: 103.6955\n",
      "Epoch 1304/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8936 - val_loss: 103.7106\n",
      "Epoch 1305/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9096 - val_loss: 103.7248\n",
      "Epoch 1306/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9058 - val_loss: 103.7136\n",
      "Epoch 1307/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.9065 - val_loss: 103.6952\n",
      "Epoch 1308/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.9028 - val_loss: 103.6863\n",
      "Epoch 1309/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8958 - val_loss: 103.6913\n",
      "Epoch 1310/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8897 - val_loss: 103.7092\n",
      "Epoch 1311/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8978 - val_loss: 103.7259\n",
      "Epoch 1312/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9153 - val_loss: 103.7320\n",
      "Epoch 1313/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.9283 - val_loss: 103.7258\n",
      "Epoch 1314/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9173 - val_loss: 103.6931\n",
      "Epoch 1315/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.8874 - val_loss: 103.6605\n",
      "Epoch 1316/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8747 - val_loss: 103.6232\n",
      "Epoch 1317/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.9226 - val_loss: 103.6020\n",
      "Epoch 1318/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.9103 - val_loss: 103.6129\n",
      "Epoch 1319/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8980 - val_loss: 103.6272\n",
      "Epoch 1320/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8866 - val_loss: 103.6423\n",
      "Epoch 1321/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8834 - val_loss: 103.6554\n",
      "Epoch 1322/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8832 - val_loss: 103.6529\n",
      "Epoch 1323/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8818 - val_loss: 103.6393\n",
      "Epoch 1324/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8773 - val_loss: 103.6362\n",
      "Epoch 1325/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8771 - val_loss: 103.6360\n",
      "Epoch 1326/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8726 - val_loss: 103.6257\n",
      "Epoch 1327/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8695 - val_loss: 103.6172\n",
      "Epoch 1328/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.8753 - val_loss: 103.6150\n",
      "Epoch 1329/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8757 - val_loss: 103.6173\n",
      "Epoch 1330/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8867 - val_loss: 103.6254\n",
      "Epoch 1331/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8656 - val_loss: 103.6077\n",
      "Epoch 1332/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8769 - val_loss: 103.5979\n",
      "Epoch 1333/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8685 - val_loss: 103.6051\n",
      "Epoch 1334/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8648 - val_loss: 103.6053\n",
      "Epoch 1335/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8642 - val_loss: 103.6094\n",
      "Epoch 1336/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8646 - val_loss: 103.6065\n",
      "Epoch 1337/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8637 - val_loss: 103.5972\n",
      "Epoch 1338/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8696 - val_loss: 103.5966\n",
      "Epoch 1339/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8557 - val_loss: 103.6132\n",
      "Epoch 1340/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8820 - val_loss: 103.6211\n",
      "Epoch 1341/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8623 - val_loss: 103.6001\n",
      "Epoch 1342/1500\n",
      "87/87 [==============================] - 0s 127us/step - loss: 37.8560 - val_loss: 103.5848\n",
      "Epoch 1343/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8533 - val_loss: 103.5851\n",
      "Epoch 1344/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8515 - val_loss: 103.5983\n",
      "Epoch 1345/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8535 - val_loss: 103.6206\n",
      "Epoch 1346/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8720 - val_loss: 103.6296\n",
      "Epoch 1347/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8786 - val_loss: 103.6252\n",
      "Epoch 1348/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8726 - val_loss: 103.6216\n",
      "Epoch 1349/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8659 - val_loss: 103.6004\n",
      "Epoch 1350/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8544 - val_loss: 103.5748\n",
      "Epoch 1351/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8508 - val_loss: 103.5527\n",
      "Epoch 1352/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8511 - val_loss: 103.5531\n",
      "Epoch 1353/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.8488 - val_loss: 103.5665\n",
      "Epoch 1354/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8400 - val_loss: 103.5761\n",
      "Epoch 1355/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8515 - val_loss: 103.5805\n",
      "Epoch 1356/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8452 - val_loss: 103.5662\n",
      "Epoch 1357/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8377 - val_loss: 103.5427\n",
      "Epoch 1358/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8393 - val_loss: 103.5184\n",
      "Epoch 1359/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8608 - val_loss: 103.5026\n",
      "Epoch 1360/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8657 - val_loss: 103.5032\n",
      "Epoch 1361/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8588 - val_loss: 103.5063\n",
      "Epoch 1362/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8517 - val_loss: 103.5206\n",
      "Epoch 1363/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8443 - val_loss: 103.5412\n",
      "Epoch 1364/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8348 - val_loss: 103.5480\n",
      "Epoch 1365/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.8382 - val_loss: 103.5452\n",
      "Epoch 1366/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8317 - val_loss: 103.5259\n",
      "Epoch 1367/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8247 - val_loss: 103.4982\n",
      "Epoch 1368/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8328 - val_loss: 103.4714\n",
      "Epoch 1369/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8462 - val_loss: 103.4481\n",
      "Epoch 1370/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 115us/step - loss: 37.8601 - val_loss: 103.4262\n",
      "Epoch 1371/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8753 - val_loss: 103.4212\n",
      "Epoch 1372/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8781 - val_loss: 103.4342\n",
      "Epoch 1373/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8563 - val_loss: 103.4519\n",
      "Epoch 1374/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8275 - val_loss: 103.4933\n",
      "Epoch 1375/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8184 - val_loss: 103.5438\n",
      "Epoch 1376/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8244 - val_loss: 103.5916\n",
      "Epoch 1377/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8476 - val_loss: 103.6239\n",
      "Epoch 1378/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8811 - val_loss: 103.6287\n",
      "Epoch 1379/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8836 - val_loss: 103.6038\n",
      "Epoch 1380/1500\n",
      "87/87 [==============================] - 0s 150us/step - loss: 37.8844 - val_loss: 103.5633\n",
      "Epoch 1381/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8362 - val_loss: 103.5456\n",
      "Epoch 1382/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8309 - val_loss: 103.5281\n",
      "Epoch 1383/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8225 - val_loss: 103.4948\n",
      "Epoch 1384/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8124 - val_loss: 103.4453\n",
      "Epoch 1385/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8478 - val_loss: 103.4013\n",
      "Epoch 1386/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8330 - val_loss: 103.3897\n",
      "Epoch 1387/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8410 - val_loss: 103.3862\n",
      "Epoch 1388/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.8368 - val_loss: 103.3754\n",
      "Epoch 1389/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8300 - val_loss: 103.3960\n",
      "Epoch 1390/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 37.8292 - val_loss: 103.4312\n",
      "Epoch 1391/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8083 - val_loss: 103.4539\n",
      "Epoch 1392/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8094 - val_loss: 103.4812\n",
      "Epoch 1393/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8176 - val_loss: 103.5073\n",
      "Epoch 1394/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 37.8366 - val_loss: 103.5182\n",
      "Epoch 1395/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8225 - val_loss: 103.5031\n",
      "Epoch 1396/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8194 - val_loss: 103.4727\n",
      "Epoch 1397/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8158 - val_loss: 103.4492\n",
      "Epoch 1398/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8117 - val_loss: 103.4318\n",
      "Epoch 1399/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8063 - val_loss: 103.4155\n",
      "Epoch 1400/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 37.8029 - val_loss: 103.3924\n",
      "Epoch 1401/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 37.8010 - val_loss: 103.3731\n",
      "Epoch 1402/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8001 - val_loss: 103.3604\n",
      "Epoch 1403/1500\n",
      "87/87 [==============================] - 0s 172us/step - loss: 37.8050 - val_loss: 103.3483\n",
      "Epoch 1404/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8081 - val_loss: 103.3309\n",
      "Epoch 1405/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8235 - val_loss: 103.3174\n",
      "Epoch 1406/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8234 - val_loss: 103.3379\n",
      "Epoch 1407/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8248 - val_loss: 103.3803\n",
      "Epoch 1408/1500\n",
      "87/87 [==============================] - 0s 195us/step - loss: 37.7904 - val_loss: 103.4041\n",
      "Epoch 1409/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7939 - val_loss: 103.4308\n",
      "Epoch 1410/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8032 - val_loss: 103.4568\n",
      "Epoch 1411/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7983 - val_loss: 103.4768\n",
      "Epoch 1412/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7995 - val_loss: 103.5015\n",
      "Epoch 1413/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8231 - val_loss: 103.5126\n",
      "Epoch 1414/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8261 - val_loss: 103.4940\n",
      "Epoch 1415/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 37.63 - 0s 126us/step - loss: 37.7982 - val_loss: 103.4523\n",
      "Epoch 1416/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7984 - val_loss: 103.4012\n",
      "Epoch 1417/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.7774 - val_loss: 103.3564\n",
      "Epoch 1418/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7720 - val_loss: 103.3047\n",
      "Epoch 1419/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.8043 - val_loss: 103.2669\n",
      "Epoch 1420/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.8235 - val_loss: 103.2592\n",
      "Epoch 1421/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.8277 - val_loss: 103.2757\n",
      "Epoch 1422/1500\n",
      "87/87 [==============================] - ETA: 0s - loss: 44.74 - 0s 115us/step - loss: 37.8116 - val_loss: 103.3129\n",
      "Epoch 1423/1500\n",
      "87/87 [==============================] - 0s 149us/step - loss: 37.7720 - val_loss: 103.3492\n",
      "Epoch 1424/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7708 - val_loss: 103.3886\n",
      "Epoch 1425/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7670 - val_loss: 103.4253\n",
      "Epoch 1426/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.8014 - val_loss: 103.4505\n",
      "Epoch 1427/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7880 - val_loss: 103.4406\n",
      "Epoch 1428/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7813 - val_loss: 103.4196\n",
      "Epoch 1429/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7849 - val_loss: 103.3870\n",
      "Epoch 1430/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7806 - val_loss: 103.3703\n",
      "Epoch 1431/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7677 - val_loss: 103.3678\n",
      "Epoch 1432/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7675 - val_loss: 103.3655\n",
      "Epoch 1433/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7723 - val_loss: 103.3646\n",
      "Epoch 1434/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7621 - val_loss: 103.3792\n",
      "Epoch 1435/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7762 - val_loss: 103.3860\n",
      "Epoch 1436/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7620 - val_loss: 103.3706\n",
      "Epoch 1437/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7581 - val_loss: 103.3487\n",
      "Epoch 1438/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7570 - val_loss: 103.3278\n",
      "Epoch 1439/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7504 - val_loss: 103.3091\n",
      "Epoch 1440/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7643 - val_loss: 103.2939\n",
      "Epoch 1441/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7499 - val_loss: 103.3035\n",
      "Epoch 1442/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7468 - val_loss: 103.3159\n",
      "Epoch 1443/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7592 - val_loss: 103.3212\n",
      "Epoch 1444/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7461 - val_loss: 103.3065\n",
      "Epoch 1445/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 103us/step - loss: 37.7421 - val_loss: 103.2863\n",
      "Epoch 1446/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7512 - val_loss: 103.2620\n",
      "Epoch 1447/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.7513 - val_loss: 103.2516\n",
      "Epoch 1448/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7520 - val_loss: 103.2384\n",
      "Epoch 1449/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7595 - val_loss: 103.2311\n",
      "Epoch 1450/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.7568 - val_loss: 103.2403\n",
      "Epoch 1451/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7471 - val_loss: 103.2517\n",
      "Epoch 1452/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7345 - val_loss: 103.2766\n",
      "Epoch 1453/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.7339 - val_loss: 103.3073\n",
      "Epoch 1454/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7342 - val_loss: 103.3362\n",
      "Epoch 1455/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7349 - val_loss: 103.3687\n",
      "Epoch 1456/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.7658 - val_loss: 103.3890\n",
      "Epoch 1457/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7662 - val_loss: 103.3819\n",
      "Epoch 1458/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7600 - val_loss: 103.3594\n",
      "Epoch 1459/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.7696 - val_loss: 103.3408\n",
      "Epoch 1460/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7383 - val_loss: 103.3387\n",
      "Epoch 1461/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7355 - val_loss: 103.3188\n",
      "Epoch 1462/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7307 - val_loss: 103.2840\n",
      "Epoch 1463/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7255 - val_loss: 103.2503\n",
      "Epoch 1464/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7156 - val_loss: 103.2137\n",
      "Epoch 1465/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.7127 - val_loss: 103.1748\n",
      "Epoch 1466/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7771 - val_loss: 103.1464\n",
      "Epoch 1467/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7783 - val_loss: 103.1576\n",
      "Epoch 1468/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7517 - val_loss: 103.1842\n",
      "Epoch 1469/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7316 - val_loss: 103.2394\n",
      "Epoch 1470/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7374 - val_loss: 103.2754\n",
      "Epoch 1471/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.7203 - val_loss: 103.2966\n",
      "Epoch 1472/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7182 - val_loss: 103.3307\n",
      "Epoch 1473/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7337 - val_loss: 103.3596\n",
      "Epoch 1474/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 37.7681 - val_loss: 103.3605\n",
      "Epoch 1475/1500\n",
      "87/87 [==============================] - 0s 92us/step - loss: 37.7617 - val_loss: 103.3348\n",
      "Epoch 1476/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7363 - val_loss: 103.3229\n",
      "Epoch 1477/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7302 - val_loss: 103.3106\n",
      "Epoch 1478/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7212 - val_loss: 103.2870\n",
      "Epoch 1479/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7206 - val_loss: 103.2644\n",
      "Epoch 1480/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7196 - val_loss: 103.2497\n",
      "Epoch 1481/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7127 - val_loss: 103.2441\n",
      "Epoch 1482/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7102 - val_loss: 103.2392\n",
      "Epoch 1483/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.7090 - val_loss: 103.2278\n",
      "Epoch 1484/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7106 - val_loss: 103.2075\n",
      "Epoch 1485/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.7053 - val_loss: 103.1887\n",
      "Epoch 1486/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.6872 - val_loss: 103.1443\n",
      "Epoch 1487/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.6848 - val_loss: 103.0932\n",
      "Epoch 1488/1500\n",
      "87/87 [==============================] - 0s 161us/step - loss: 37.7393 - val_loss: 103.0551\n",
      "Epoch 1489/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.7805 - val_loss: 103.0567\n",
      "Epoch 1490/1500\n",
      "87/87 [==============================] - 0s 138us/step - loss: 37.7506 - val_loss: 103.1001\n",
      "Epoch 1491/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.7641 - val_loss: 103.1564\n",
      "Epoch 1492/1500\n",
      "87/87 [==============================] - 0s 104us/step - loss: 37.7203 - val_loss: 103.1697\n",
      "Epoch 1493/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.6870 - val_loss: 103.1544\n",
      "Epoch 1494/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.6835 - val_loss: 103.1435\n",
      "Epoch 1495/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.6800 - val_loss: 103.1337\n",
      "Epoch 1496/1500\n",
      "87/87 [==============================] - 0s 103us/step - loss: 37.6777 - val_loss: 103.1190\n",
      "Epoch 1497/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.6833 - val_loss: 103.1031\n",
      "Epoch 1498/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.6889 - val_loss: 103.1136\n",
      "Epoch 1499/1500\n",
      "87/87 [==============================] - 0s 126us/step - loss: 37.6730 - val_loss: 103.1453\n",
      "Epoch 1500/1500\n",
      "87/87 [==============================] - 0s 115us/step - loss: 37.6708 - val_loss: 103.1684\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=64, epochs=1500,\n",
    "          validation_data=(X_val, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'to_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d619c726b31c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# serialize model to JSON\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# serialize weights to HDF5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'to_json'"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = hist.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "hist.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
