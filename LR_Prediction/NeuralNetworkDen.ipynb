{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('InterpolatedNum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>D REVENUE</th>\n",
       "      <th>U CR</th>\n",
       "      <th>D OE</th>\n",
       "      <th>D NOI</th>\n",
       "      <th>D FCF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>1884.372544</td>\n",
       "      <td>976.202014</td>\n",
       "      <td>475.249997</td>\n",
       "      <td>757.519678</td>\n",
       "      <td>1071.834493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1884.566826</td>\n",
       "      <td>983.762225</td>\n",
       "      <td>485.004015</td>\n",
       "      <td>734.017979</td>\n",
       "      <td>1033.190422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1884.761107</td>\n",
       "      <td>991.322435</td>\n",
       "      <td>494.758033</td>\n",
       "      <td>710.516281</td>\n",
       "      <td>994.546350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1884.955389</td>\n",
       "      <td>998.882646</td>\n",
       "      <td>504.512051</td>\n",
       "      <td>687.014582</td>\n",
       "      <td>955.902279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1880.767673</td>\n",
       "      <td>1006.377690</td>\n",
       "      <td>481.542613</td>\n",
       "      <td>511.922217</td>\n",
       "      <td>928.473553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>1146.881815</td>\n",
       "      <td>433.816967</td>\n",
       "      <td>436.290714</td>\n",
       "      <td>460.509240</td>\n",
       "      <td>733.164793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>1111.642375</td>\n",
       "      <td>415.370593</td>\n",
       "      <td>431.490513</td>\n",
       "      <td>417.984787</td>\n",
       "      <td>729.764211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>1142.095113</td>\n",
       "      <td>422.618925</td>\n",
       "      <td>430.139254</td>\n",
       "      <td>424.521959</td>\n",
       "      <td>737.713784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>1172.547851</td>\n",
       "      <td>429.867258</td>\n",
       "      <td>428.787994</td>\n",
       "      <td>431.059130</td>\n",
       "      <td>745.663358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>1203.000589</td>\n",
       "      <td>437.115591</td>\n",
       "      <td>427.436735</td>\n",
       "      <td>437.596301</td>\n",
       "      <td>753.612931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Dates    D REVENUE         U CR        D OE       D NOI        D FCF\n",
       "0    2009-12-31  1884.372544   976.202014  475.249997  757.519678  1071.834493\n",
       "1    2010-01-31  1884.566826   983.762225  485.004015  734.017979  1033.190422\n",
       "2    2010-02-28  1884.761107   991.322435  494.758033  710.516281   994.546350\n",
       "3    2010-03-31  1884.955389   998.882646  504.512051  687.014582   955.902279\n",
       "4    2010-04-30  1880.767673  1006.377690  481.542613  511.922217   928.473553\n",
       "..          ...          ...          ...         ...         ...          ...\n",
       "119  2019-11-30  1146.881815   433.816967  436.290714  460.509240   733.164793\n",
       "120  2019-12-31  1111.642375   415.370593  431.490513  417.984787   729.764211\n",
       "121  2020-01-31  1142.095113   422.618925  430.139254  424.521959   737.713784\n",
       "122  2020-02-29  1172.547851   429.867258  428.787994  431.059130   745.663358\n",
       "123  2020-03-31  1203.000589   437.115591  427.436735  437.596301   753.612931\n",
       "\n",
       "[124 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2009-12-31', 1884.372544, 976.2020142, 475.24999739999987,\n",
       "        757.519678, 1071.834493],\n",
       "       ['2010-01-31', 1884.5668256666668, 983.7622247333335, 485.0040154,\n",
       "        734.0179793333333, 1033.1904216666667],\n",
       "       ['2010-02-28', 1884.761107333333, 991.3224352666666,\n",
       "        494.75803339999993, 710.5162806666667, 994.5463503333333],\n",
       "       ['2010-03-31', 1884.955389, 998.8826458, 504.5120514, 687.014582,\n",
       "        955.902279],\n",
       "       ['2010-04-30', 1880.767673333333, 1006.3776901999997,\n",
       "        481.5426128333333, 511.92221730000006, 928.4735526333333],\n",
       "       ['2010-05-31', 1876.5799576666668, 1013.8727346000001,\n",
       "        458.5731742666666, 336.8298526000001, 901.0448262666665],\n",
       "       ['2010-06-30', 1872.392242, 1021.367779, 435.6037357, 161.7374879,\n",
       "        873.6160999],\n",
       "       ['2010-07-31', 1853.240114, 1015.1698886666666, 421.8261523,\n",
       "        187.6358415, 863.0179451333332],\n",
       "       ['2010-08-31', 1834.087986, 1008.9719983333333, 408.0485689,\n",
       "        213.5341951, 852.4197903666667],\n",
       "       ['2010-09-30', 1814.935858, 1002.774108, 394.2709855, 239.4325487,\n",
       "        841.8216356],\n",
       "       ['2010-10-31', 1851.320774, 1008.5464486666666, 411.1460836666666,\n",
       "        246.73378476666667, 855.6692487],\n",
       "       ['2010-11-30', 1887.70569, 1014.3187893333335, 428.0211818333333,\n",
       "        254.03502083333333, 869.5168617999999],\n",
       "       ['2010-12-31', 1924.090606, 1020.09113, 444.89628, 261.3362569,\n",
       "        883.3644749],\n",
       "       ['2011-01-31', 1840.631611, 1004.3123142000001, 439.1857453333333,\n",
       "        253.3520595, 874.4433030666668],\n",
       "       ['2011-02-28', 1757.172616, 988.5334983999999, 433.47521066666667,\n",
       "        245.36786209999997, 865.5221312333334],\n",
       "       ['2011-03-31', 1673.713621, 972.7546826, 427.764676, 237.3836647,\n",
       "        856.6009594000002],\n",
       "       ['2011-04-30', 1697.5040236666666, 943.8024453, 430.2420961,\n",
       "        237.04147333333333, 810.8597272333334],\n",
       "       ['2011-05-31', 1721.2944263333334, 914.850208, 432.71951619999993,\n",
       "        236.69928196666663, 765.1184950666667],\n",
       "       ['2011-06-30', 1745.084829, 885.8979707000002, 435.1969363,\n",
       "        236.3570906, 719.3772629],\n",
       "       ['2011-07-31', 1775.998593, 875.2949231666668, 444.6121419,\n",
       "        236.2388491, 732.4146046],\n",
       "       ['2011-08-31', 1806.912357, 864.6918756333333, 454.02734749999996,\n",
       "        236.1206076, 745.4519462999999],\n",
       "       ['2011-09-30', 1837.826121, 854.0888281, 463.44255310000005,\n",
       "        236.0023661, 758.489288],\n",
       "       ['2011-10-31', 1885.0705423333332, 857.2014145333335,\n",
       "        470.0062231666667, 244.5463731, 756.6899263333332],\n",
       "       ['2011-11-30', 1932.314963666667, 860.3140009666665,\n",
       "        476.56989323333335, 253.0903801, 754.8905646666667],\n",
       "       ['2011-12-31', 1979.559385, 863.4265874, 483.13356330000005,\n",
       "        261.6343871, 753.0912030000002],\n",
       "       ['2012-01-31', 1910.3834399999998, 858.4811889666668,\n",
       "        483.72593293333335, 263.6296255, 715.3372166666667],\n",
       "       ['2012-02-29', 1841.2074949999999, 853.5357905333334,\n",
       "        484.3183025666667, 265.6248639, 677.5832303333334],\n",
       "       ['2012-03-31', 1772.03155, 848.5903921, 484.9106722, 267.6201023,\n",
       "        639.829244],\n",
       "       ['2012-04-30', 1805.3226533333332, 852.7747259666667,\n",
       "        500.10971653333337, 262.53434903333334, 625.4113584666667],\n",
       "       ['2012-05-31', 1838.6137566666666, 856.9590598333334,\n",
       "        515.3087608666667, 257.44859576666664, 610.9934729333334],\n",
       "       ['2012-06-30', 1871.90486, 861.1433937, 530.5078052, 252.3628425,\n",
       "        596.5755874],\n",
       "       ['2012-07-31', 1870.0189613333332, 888.6962120999999,\n",
       "        512.9147551333333, 274.75087963333334, 596.6860897999999],\n",
       "       ['2012-08-31', 1868.1330626666668, 916.2490305,\n",
       "        495.32170506666665, 297.1389167666666, 596.7965922000002],\n",
       "       ['2012-09-30', 1866.247164, 943.8018489, 477.728655, 319.5269539,\n",
       "        596.9070946],\n",
       "       ['2012-10-31', 1872.4550559999998, 871.4747433666665,\n",
       "        485.99178150000006, 323.89898873333334, 628.3878588333333],\n",
       "       ['2012-11-30', 1878.6629480000001, 799.1476378333333, 494.254908,\n",
       "        328.27102356666666, 659.8686230666667],\n",
       "       ['2012-12-31', 1884.87084, 726.8205323, 502.5180345, 332.6430584,\n",
       "        691.3493873],\n",
       "       ['2013-01-31', 1896.30791, 753.0427803333333, 514.5447965666667,\n",
       "        333.5149285333333, 690.0065109333333],\n",
       "       ['2013-02-28', 1907.7449800000002, 779.2650283666666,\n",
       "        526.5715586333333, 334.38679866666666, 688.6636345666667],\n",
       "       ['2013-03-31', 1919.18205, 805.4872763999998, 538.5983207,\n",
       "        335.2586688, 687.3207582],\n",
       "       ['2013-04-30', 1867.7861953333331, 783.4079046999999,\n",
       "        530.8571651000001, 335.86746453333336, 711.6253066666667],\n",
       "       ['2013-05-31', 1816.3903406666666, 761.328533, 523.1160095,\n",
       "        336.47626026666666, 735.9298551333335],\n",
       "       ['2013-06-30', 1764.994486, 739.2491613, 515.3748539, 337.085056,\n",
       "        760.2344036000002],\n",
       "       ['2013-07-31', 1768.6169343333331, 757.7170030999998, 520.4034373,\n",
       "        329.2451542333333, 760.4925157666668],\n",
       "       ['2013-08-31', 1772.2393826666666, 776.1848449, 525.4320207000002,\n",
       "        321.40525246666664, 760.7506279333335],\n",
       "       ['2013-09-30', 1775.861831, 794.6526867, 530.4606041000002,\n",
       "        313.5653506999999, 761.0087401000002],\n",
       "       ['2013-10-31', 1781.312525333333, 813.7964820333333,\n",
       "        531.6057520333334, 320.4643399333333, 761.8369197333334],\n",
       "       ['2013-11-30', 1786.7632196666668, 832.9402773666667,\n",
       "        532.7508999666667, 327.36332916666663, 762.6650993666667],\n",
       "       ['2013-12-31', 1792.2139140000004, 852.0840727, 533.8960479,\n",
       "        334.2623184, 763.493279],\n",
       "       ['2014-01-31', 1772.1792736666669, 861.7296213999998, 528.3798523,\n",
       "        342.6429393, 776.451154],\n",
       "       ['2014-02-28', 1752.144633333333, 871.3751701, 522.8636567,\n",
       "        351.0235602, 789.409029],\n",
       "       ['2014-03-31', 1732.109993, 881.0207187999998, 517.3474611,\n",
       "        359.4041811, 802.366904],\n",
       "       ['2014-04-30', 1721.8949903333335, 890.6855609999999,\n",
       "        507.92859023333335, 349.91219946666666, 793.3964250333332],\n",
       "       ['2014-05-31', 1711.6799876666666, 900.3504032,\n",
       "        498.50971936666673, 340.4202178333333, 784.4259460666666],\n",
       "       ['2014-06-30', 1701.464985, 910.0152454, 489.09084850000005,\n",
       "        330.9282362, 775.4554671],\n",
       "       ['2014-07-31', 1728.4231946666666, 914.2662125, 481.6856930666667,\n",
       "        331.95640223333334, 766.0225778666667],\n",
       "       ['2014-08-31', 1755.3814043333334, 918.5171796,\n",
       "        474.28053763333327, 332.98456826666666, 756.5896886333333],\n",
       "       ['2014-09-30', 1782.339614, 922.7681467, 466.8753822,\n",
       "        334.0127343000001, 747.1567994],\n",
       "       ['2014-10-31', 1725.137324333333, 965.4992654666668,\n",
       "        478.28002593333326, 415.6834258, 757.3171348],\n",
       "       ['2014-11-30', 1667.935034666667, 1008.2303842333333,\n",
       "        489.68466966666665, 497.35411730000004, 767.4774702000001],\n",
       "       ['2014-12-31', 1610.732745, 1050.961503, 501.0893134, 579.0248088,\n",
       "        777.6378056],\n",
       "       ['2015-01-31', 1746.422448666667, 1054.429111, 508.5803035333333,\n",
       "        579.2034107000001, 776.7867944],\n",
       "       ['2015-02-28', 1882.112152333333, 1057.8967189999998,\n",
       "        516.0712936666666, 579.3820125999998, 775.9357832000001],\n",
       "       ['2015-03-31', 2017.801856, 1061.364327, 523.5622838,\n",
       "        579.5606144999998, 775.0847719999998],\n",
       "       ['2015-04-30', 2107.344116, 1096.44801, 526.1169451000002,\n",
       "        577.4851386666667, 774.5279383333333],\n",
       "       ['2015-05-31', 2196.8863760000004, 1131.531693, 528.6716064,\n",
       "        575.4096628333333, 773.9711046666665],\n",
       "       ['2015-06-30', 2286.428636, 1166.615376, 531.2262677, 573.334187,\n",
       "        773.414271],\n",
       "       ['2015-07-31', 2528.4880129999997, 1194.063472, 562.655131,\n",
       "        551.8155191666667, 760.1008118333333],\n",
       "       ['2015-08-31', 2770.5473899999997, 1221.511568, 594.0839943000002,\n",
       "        530.2968513333334, 746.7873526666666],\n",
       "       ['2015-09-30', 3012.606767, 1248.959664, 625.5128576000002,\n",
       "        508.7781835, 733.4738934999998],\n",
       "       ['2015-10-31', 2903.7366663333332, 1269.6701176666666,\n",
       "        641.4096041, 508.9514627, 727.0572944999999],\n",
       "       ['2015-11-30', 2794.8665656666667, 1290.3805713333334,\n",
       "        657.3063506000002, 509.1247419, 720.6406955],\n",
       "       ['2015-12-31', 2685.996465, 1311.091025, 673.2030971, 509.2980211,\n",
       "        714.2240965],\n",
       "       ['2016-01-31', 2834.140780666666, 1366.4444733333332,\n",
       "        674.0792667333334, 500.8613563, 728.1806636666665],\n",
       "       ['2016-02-29', 2982.285096333333, 1421.7979216666665,\n",
       "        674.9554363666666, 492.4246915, 742.1372308333333],\n",
       "       ['2016-03-31', 3130.429412, 1477.1513699999996, 675.831606,\n",
       "        483.9880267, 756.093798],\n",
       "       ['2016-04-30', 3060.8571920000004, 1508.7480799999996,\n",
       "        653.6836567666667, 473.8452801, 771.8714318333333],\n",
       "       ['2016-05-31', 2991.284972, 1540.34479, 631.5357075333333,\n",
       "        463.7025335, 787.6490656666667],\n",
       "       ['2016-06-30', 2921.712752, 1571.9415, 609.3877583, 453.5597869,\n",
       "        803.4266995],\n",
       "       ['2016-07-31', 2979.422658, 1553.679082333333, 612.0762081333334,\n",
       "        473.88566743333337, 790.2704643666667],\n",
       "       ['2016-08-31', 3037.132564, 1535.416664666667, 614.7646579666666,\n",
       "        494.2115479666666, 777.1142292333334],\n",
       "       ['2016-09-30', 3094.84247, 1517.1542470000004, 617.4531078,\n",
       "        514.5374285, 763.9579941000002],\n",
       "       ['2016-10-31', 2941.914112333333, 1493.7506856666669,\n",
       "        601.3888980666667, 497.1393087333333, 725.2341621],\n",
       "       ['2016-11-30', 2788.9857546666667, 1470.347124333333,\n",
       "        585.3246883333334, 479.7411889666667, 686.5103301],\n",
       "       ['2016-12-31', 2636.057397, 1446.943563, 569.2604786, 462.3430692,\n",
       "        647.7864981],\n",
       "       ['2017-01-31', 2629.386708333333, 1421.7832626666666,\n",
       "        553.3237412999999, 463.41115033333335, 646.7970369666667],\n",
       "       ['2017-02-28', 2622.7160196666664, 1396.6229623333334,\n",
       "        537.3870039999998, 464.47923146666665, 645.8075758333333],\n",
       "       ['2017-03-31', 2616.045331, 1371.462662, 521.4502666999998,\n",
       "        465.5473126, 644.8181147000001],\n",
       "       ['2017-04-30', 2582.999953333333, 1385.6997396666666,\n",
       "        514.2855105000001, 456.63566276666666, 645.8685558666667],\n",
       "       ['2017-05-31', 2549.9545756666666, 1399.9368173333332,\n",
       "        507.12075430000004, 447.72401293333337, 646.9189970333333],\n",
       "       ['2017-06-30', 2516.909198, 1414.173895, 499.9559981, 438.8123631,\n",
       "        647.9694382],\n",
       "       ['2017-07-31', 2529.782464, 1411.1019353333334,\n",
       "        495.39090156666674, 419.9090564333334, 637.6159774666667],\n",
       "       ['2017-08-31', 2542.65573, 1408.0299756666666, 490.8258050333334,\n",
       "        401.00574976666667, 627.2625167333333],\n",
       "       ['2017-09-30', 2555.528996, 1404.958016, 486.2607085, 382.1024431,\n",
       "        616.909056],\n",
       "       ['2017-10-31', 2493.5604476666667, 1340.4287853333333,\n",
       "        485.59986143333333, 386.4377233, 608.7162381333333],\n",
       "       ['2017-11-30', 2431.591899333333, 1275.8995546666667,\n",
       "        484.9390143666667, 390.7730035, 600.5234202666667],\n",
       "       ['2017-12-31', 2369.623351, 1211.370324, 484.27816730000006,\n",
       "        395.1082837, 592.3306024],\n",
       "       ['2018-01-31', 2307.4944116666666, 1212.1657916666666,\n",
       "        479.0572197666667, 392.9066318, 605.3801967666667],\n",
       "       ['2018-02-28', 2245.365472333333, 1212.9612593333334,\n",
       "        473.83627223333326, 390.7049799, 618.4297911333333],\n",
       "       ['2018-03-31', 2183.236533, 1213.756727, 468.6153247, 388.503328,\n",
       "        631.4793855],\n",
       "       ['2018-04-30', 2086.8807423333333, 1146.0012783333334,\n",
       "        448.50893473333326, 390.50832306666666, 669.1150419666667],\n",
       "       ['2018-05-31', 1990.5249516666665, 1078.2458296666666,\n",
       "        428.40254476666667, 392.51331813333326, 706.7506984333335],\n",
       "       ['2018-06-30', 1894.169161, 1010.490381, 408.2961548, 394.5183132,\n",
       "        744.3863549],\n",
       "       ['2018-07-31', 1839.681530666667, 991.3822151, 434.8828281333333,\n",
       "        404.4480508333333, 756.4180495333335],\n",
       "       ['2018-08-31', 1785.1939003333332, 972.2740492,\n",
       "        461.46950146666666, 414.37778846666674, 768.4497441666666],\n",
       "       ['2018-09-30', 1730.70627, 953.1658833, 488.0561748, 424.3075261,\n",
       "        780.4814388],\n",
       "       ['2018-10-31', 1667.921494, 856.9003839999998, 471.2615347666666,\n",
       "        434.87458276666666, 773.9692328666665],\n",
       "       ['2018-11-30', 1605.136718, 760.6348846999998, 454.4668947333333,\n",
       "        445.44163943333336, 767.4570269333334],\n",
       "       ['2018-12-31', 1542.351942, 664.3693853999998, 437.67225470000005,\n",
       "        456.0086961, 760.944821],\n",
       "       ['2019-01-31', 1513.0525300000002, 657.3813301333333,\n",
       "        442.62654349999997, 467.28399896666673, 748.7807167999999],\n",
       "       ['2019-02-28', 1483.7531179999999, 650.3932748666666,\n",
       "        447.58083230000005, 478.5593018333333, 736.6166125999998],\n",
       "       ['2019-03-31', 1454.453706, 643.4052196, 452.5351211, 489.8346047,\n",
       "        724.4525083999998],\n",
       "       ['2019-04-30', 1394.134012, 601.7953597333334, 451.1815351333333,\n",
       "        506.34549253333336, 729.9813198666666],\n",
       "       ['2019-05-31', 1333.814318, 560.1854998666666, 449.8279491666667,\n",
       "        522.8563803666667, 735.5101313333333],\n",
       "       ['2019-06-30', 1273.494624, 518.57564, 448.4743632, 539.3672682,\n",
       "        741.0389428],\n",
       "       ['2019-07-31', 1254.7833143333332, 502.62033166666674,\n",
       "        447.61328139999995, 541.4308939333333, 740.6812809333335],\n",
       "       ['2019-08-31', 1236.0720046666665, 486.66502333333335,\n",
       "        446.7521996, 543.4945196666666, 740.3236190666668],\n",
       "       ['2019-09-30', 1217.360695, 470.709715, 445.8911178, 545.5581454,\n",
       "        739.9659572],\n",
       "       ['2019-10-31', 1182.1212549999998, 452.26334086666674,\n",
       "        441.0909160666667, 503.0336927333333, 736.5653751333334],\n",
       "       ['2019-11-30', 1146.881815, 433.81696673333335,\n",
       "        436.29071433333326, 460.5092400666666, 733.1647930666667],\n",
       "       ['2019-12-31', 1111.642375, 415.37059260000007, 431.4905126,\n",
       "        417.9847874, 729.764211],\n",
       "       ['2020-01-31', 1142.095113, 422.61892543333335,\n",
       "        430.13925353333326, 424.5219585333333, 737.7137843333335],\n",
       "       ['2020-02-29', 1172.547851, 429.86725826666674,\n",
       "        428.78799446666665, 431.0591296666667, 745.6633576666667],\n",
       "       ['2020-03-31', 1203.000589, 437.1155911, 427.43673539999986,\n",
       "        437.59630080000005, 753.612931]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,1:5]\n",
    "Y = dataset[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38276953, 0.48490881, 0.28760773, 1.        ],\n",
       "       [0.38286577, 0.49144556, 0.32225043, 0.9605532 ],\n",
       "       [0.38296201, 0.4979823 , 0.35689312, 0.92110641],\n",
       "       [0.38305824, 0.50451905, 0.39153581, 0.88165961],\n",
       "       [0.38098387, 0.51099945, 0.30995679, 0.58777307],\n",
       "       [0.3789095 , 0.51747985, 0.22837778, 0.29388654],\n",
       "       [0.37683513, 0.52396025, 0.14679876, 0.        ],\n",
       "       [0.36734818, 0.5186014 , 0.09786584, 0.0434695 ],\n",
       "       [0.35786123, 0.51324255, 0.04893292, 0.086939  ],\n",
       "       [0.34837428, 0.5078837 , 0.        , 0.1304085 ],\n",
       "       [0.36639744, 0.51287461, 0.05993416, 0.14266337],\n",
       "       [0.3844206 , 0.51786552, 0.11986831, 0.15491825],\n",
       "       [0.40244375, 0.52285643, 0.17980247, 0.16717312],\n",
       "       [0.36110259, 0.50921367, 0.15952074, 0.15377192],\n",
       "       [0.31976144, 0.49557092, 0.13923902, 0.14037072],\n",
       "       [0.27842028, 0.48192816, 0.1189573 , 0.12696952],\n",
       "       [0.29020478, 0.45689534, 0.12775618, 0.12639516],\n",
       "       [0.30198928, 0.43186251, 0.13655507, 0.1258208 ],\n",
       "       [0.31377379, 0.40682969, 0.14535396, 0.12524645],\n",
       "       [0.32908683, 0.39766203, 0.17879331, 0.12504798],\n",
       "       [0.34439986, 0.38849437, 0.21223267, 0.12484952],\n",
       "       [0.3597129 , 0.37932671, 0.24567202, 0.12465105],\n",
       "       [0.38311528, 0.38201793, 0.26898377, 0.13899188],\n",
       "       [0.40651766, 0.38470915, 0.29229552, 0.1533327 ],\n",
       "       [0.42992004, 0.38740037, 0.31560727, 0.16767352],\n",
       "       [0.39565395, 0.38312445, 0.31771115, 0.17102246],\n",
       "       [0.36138786, 0.37884854, 0.31981503, 0.1743714 ],\n",
       "       [0.32712176, 0.37457262, 0.32191891, 0.17772034],\n",
       "       [0.34361241, 0.3781905 , 0.37590033, 0.16918408],\n",
       "       [0.36010306, 0.38180838, 0.42988176, 0.16064782],\n",
       "       [0.3765937 , 0.38542626, 0.48386319, 0.15211155],\n",
       "       [0.37565953, 0.40924911, 0.42137913, 0.18968911],\n",
       "       [0.37472535, 0.43307197, 0.35889507, 0.22726666],\n",
       "       [0.37379118, 0.45689482, 0.29641102, 0.26484421],\n",
       "       [0.37686624, 0.394359  , 0.32575861, 0.27218252],\n",
       "       [0.3799413 , 0.33182319, 0.3551062 , 0.27952084],\n",
       "       [0.38301636, 0.26928737, 0.38445379, 0.28685915],\n",
       "       [0.38868168, 0.29195978, 0.42716844, 0.28832255],\n",
       "       [0.394347  , 0.31463219, 0.46988309, 0.28978595],\n",
       "       [0.40001231, 0.3373046 , 0.51259773, 0.29124936],\n",
       "       [0.37455353, 0.31821422, 0.48510399, 0.2922712 ],\n",
       "       [0.34909475, 0.29912385, 0.45761024, 0.29329304],\n",
       "       [0.32363597, 0.28003347, 0.4301165 , 0.29431489],\n",
       "       [0.32543034, 0.29600123, 0.44797618, 0.28115588],\n",
       "       [0.32722471, 0.31196899, 0.46583586, 0.26799687],\n",
       "       [0.32901908, 0.32793674, 0.48369555, 0.25483787],\n",
       "       [0.33171907, 0.34448894, 0.48776269, 0.26641758],\n",
       "       [0.33441905, 0.36104115, 0.49182984, 0.2779973 ],\n",
       "       [0.33711904, 0.37759335, 0.49589698, 0.28957702],\n",
       "       [0.32719494, 0.38593313, 0.47630548, 0.3036436 ],\n",
       "       [0.31727084, 0.39427291, 0.45671398, 0.31771019],\n",
       "       [0.30734674, 0.40261269, 0.43712248, 0.33177677],\n",
       "       [0.30228677, 0.41096915, 0.4036701 , 0.31584481],\n",
       "       [0.2972268 , 0.41932562, 0.37021773, 0.29991284],\n",
       "       [0.29216683, 0.42768208, 0.33676536, 0.28398088],\n",
       "       [0.3055205 , 0.43135757, 0.31046496, 0.28570662],\n",
       "       [0.31887416, 0.43503307, 0.28416457, 0.28743236],\n",
       "       [0.33222783, 0.43870856, 0.25786417, 0.2891581 ],\n",
       "       [0.30389285, 0.47565495, 0.29836928, 0.42623956],\n",
       "       [0.27555787, 0.51260134, 0.33887439, 0.56332102],\n",
       "       [0.24722289, 0.54954772, 0.3793795 , 0.70040248],\n",
       "       [0.31443637, 0.5525459 , 0.40598475, 0.70070225],\n",
       "       [0.38164985, 0.55554408, 0.43259   , 0.70100203],\n",
       "       [0.44886333, 0.55854227, 0.45919525, 0.70130181],\n",
       "       [0.49321782, 0.58887649, 0.46826847, 0.69781819],\n",
       "       [0.53757231, 0.61921072, 0.47734168, 0.69433458],\n",
       "       [0.58192679, 0.64954494, 0.4864149 , 0.69085096],\n",
       "       [0.70183016, 0.67327725, 0.59803869, 0.65473261],\n",
       "       [0.82173354, 0.69700956, 0.70966248, 0.61861427],\n",
       "       [0.94163691, 0.72074186, 0.82128627, 0.58249592],\n",
       "       [0.88770844, 0.73864864, 0.87774568, 0.58278676],\n",
       "       [0.83377997, 0.75655541, 0.93420509, 0.58307761],\n",
       "       [0.7798515 , 0.77446219, 0.9906645 , 0.58336845],\n",
       "       [0.85323433, 0.82232215, 0.99377633, 0.5692078 ],\n",
       "       [0.92661717, 0.87018212, 0.99688817, 0.55504714],\n",
       "       [1.        , 0.91804209, 1.        , 0.54088649],\n",
       "       [0.96553761, 0.9453614 , 0.92133861, 0.52386224],\n",
       "       [0.93107523, 0.9726807 , 0.84267722, 0.50683799],\n",
       "       [0.89661284, 1.        , 0.76401584, 0.48981373],\n",
       "       [0.92519927, 0.98420986, 0.77356422, 0.52393003],\n",
       "       [0.95378569, 0.96841972, 0.78311261, 0.55804632],\n",
       "       [0.98237212, 0.95262958, 0.792661  , 0.59216262],\n",
       "       [0.90661952, 0.93239428, 0.73560682, 0.56296047],\n",
       "       [0.83086693, 0.91215897, 0.67855264, 0.53375832],\n",
       "       [0.75511433, 0.89192367, 0.62149846, 0.50455617],\n",
       "       [0.75181003, 0.87016945, 0.56489702, 0.50634891],\n",
       "       [0.74850572, 0.84841523, 0.50829558, 0.50814165],\n",
       "       [0.74520141, 0.82666101, 0.45169414, 0.50993438],\n",
       "       [0.72883249, 0.83897074, 0.42624755, 0.49497649],\n",
       "       [0.71246356, 0.85128047, 0.40080097, 0.48001859],\n",
       "       [0.69609463, 0.8635902 , 0.37535438, 0.46506069],\n",
       "       [0.70247137, 0.86093411, 0.35914083, 0.43333214],\n",
       "       [0.7088481 , 0.85827802, 0.34292729, 0.40160358],\n",
       "       [0.71522483, 0.85562192, 0.32671374, 0.36987503],\n",
       "       [0.6845289 , 0.79982834, 0.32436665, 0.37715165],\n",
       "       [0.65383297, 0.74403476, 0.32201957, 0.38442827],\n",
       "       [0.62313704, 0.68824118, 0.31967248, 0.39170489],\n",
       "       [0.59236166, 0.68892897, 0.30112959, 0.38800949],\n",
       "       [0.56158628, 0.68961675, 0.2825867 , 0.38431409],\n",
       "       [0.5308109 , 0.69030453, 0.26404381, 0.3806187 ],\n",
       "       [0.48308135, 0.63172148, 0.19263329, 0.38398401],\n",
       "       [0.4353518 , 0.57313843, 0.12122277, 0.38734933],\n",
       "       [0.38762226, 0.51455538, 0.04981225, 0.39071464],\n",
       "       [0.36063197, 0.49803399, 0.14423836, 0.40738137],\n",
       "       [0.33364169, 0.48151259, 0.23866447, 0.42404809],\n",
       "       [0.30665141, 0.4649912 , 0.33309058, 0.44071482],\n",
       "       [0.27555116, 0.38175765, 0.27344218, 0.45845126],\n",
       "       [0.24445092, 0.2985241 , 0.21379378, 0.4761877 ],\n",
       "       [0.21335067, 0.21529056, 0.15414538, 0.49392414],\n",
       "       [0.1988373 , 0.20924851, 0.1717412 , 0.51284935],\n",
       "       [0.18432392, 0.20320646, 0.18933701, 0.53177456],\n",
       "       [0.16981055, 0.19716442, 0.20693283, 0.55069977],\n",
       "       [0.13993137, 0.16118749, 0.20212539, 0.57841273],\n",
       "       [0.11005219, 0.12521057, 0.19731795, 0.60612569],\n",
       "       [0.08017302, 0.08923365, 0.19251051, 0.63383865],\n",
       "       [0.07090443, 0.0754383 , 0.18945226, 0.63730238],\n",
       "       [0.06163584, 0.06164294, 0.18639401, 0.6407661 ],\n",
       "       [0.05236725, 0.04784758, 0.18333577, 0.64422983],\n",
       "       [0.0349115 , 0.03189839, 0.16628721, 0.57285399],\n",
       "       [0.01745575, 0.01594919, 0.14923866, 0.50147815],\n",
       "       [0.        , 0.        , 0.1321901 , 0.43010232],\n",
       "       [0.01508467, 0.00626709, 0.12739093, 0.44107473],\n",
       "       [0.03016934, 0.01253418, 0.12259175, 0.45204715],\n",
       "       [0.04525401, 0.01880127, 0.11779257, 0.46301957]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 4) (12, 4) (13, 4) (99,) (12,) (13,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(6, activation='elu', input_shape=(4,)),\n",
    "    Dense(32, activation='elu'),\n",
    "    Dense(12, activation='elu'),\n",
    "    Dense(1, activation='elu'),\n",
    "])\n",
    "#softplus, selu, elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99 samples, validate on 12 samples\n",
      "Epoch 1/1500\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 740.0318 - val_loss: 715.3510\n",
      "Epoch 2/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 739.9678 - val_loss: 715.2849\n",
      "Epoch 3/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 739.9015 - val_loss: 715.2122\n",
      "Epoch 4/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 739.8292 - val_loss: 715.1382\n",
      "Epoch 5/1500\n",
      "99/99 [==============================] - 0s 105us/step - loss: 739.7568 - val_loss: 715.0648\n",
      "Epoch 6/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 739.6843 - val_loss: 714.9907\n",
      "Epoch 7/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 739.6116 - val_loss: 714.9160\n",
      "Epoch 8/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 739.5386 - val_loss: 714.8403\n",
      "Epoch 9/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 739.4636 - val_loss: 714.7636\n",
      "Epoch 10/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 739.3883 - val_loss: 714.6867\n",
      "Epoch 11/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 739.3136 - val_loss: 714.6077\n",
      "Epoch 12/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 739.2356 - val_loss: 714.5276\n",
      "Epoch 13/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 739.1573 - val_loss: 714.4471\n",
      "Epoch 14/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 739.0778 - val_loss: 714.3647\n",
      "Epoch 15/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 738.9959 - val_loss: 714.2795\n",
      "Epoch 16/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 738.9130 - val_loss: 714.1931\n",
      "Epoch 17/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 738.8272 - val_loss: 714.1046\n",
      "Epoch 18/1500\n",
      "99/99 [==============================] - 0s 121us/step - loss: 738.7401 - val_loss: 714.0137\n",
      "Epoch 19/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 738.6507 - val_loss: 713.9206\n",
      "Epoch 20/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 738.5583 - val_loss: 713.8249\n",
      "Epoch 21/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 738.4628 - val_loss: 713.7266\n",
      "Epoch 22/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 738.3664 - val_loss: 713.6246\n",
      "Epoch 23/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 738.2656 - val_loss: 713.5197\n",
      "Epoch 24/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 738.1606 - val_loss: 713.4114\n",
      "Epoch 25/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 738.0538 - val_loss: 713.2991\n",
      "Epoch 26/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 737.9421 - val_loss: 713.1841\n",
      "Epoch 27/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 737.8281 - val_loss: 713.0646\n",
      "Epoch 28/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 737.7089 - val_loss: 712.9410\n",
      "Epoch 29/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 737.5870 - val_loss: 712.8127\n",
      "Epoch 30/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 737.4589 - val_loss: 712.6801\n",
      "Epoch 31/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 737.3271 - val_loss: 712.5424\n",
      "Epoch 32/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 737.1899 - val_loss: 712.3991\n",
      "Epoch 33/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 737.0473 - val_loss: 712.2505\n",
      "Epoch 34/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 736.8997 - val_loss: 712.0964\n",
      "Epoch 35/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 736.7479 - val_loss: 711.9361\n",
      "Epoch 36/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 736.5878 - val_loss: 711.7698\n",
      "Epoch 37/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 736.4220 - val_loss: 711.5969\n",
      "Epoch 38/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 736.2486 - val_loss: 711.4170\n",
      "Epoch 39/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 736.0708 - val_loss: 711.2287\n",
      "Epoch 40/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 735.8830 - val_loss: 711.0331\n",
      "Epoch 41/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 735.6887 - val_loss: 710.8292\n",
      "Epoch 42/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 735.4853 - val_loss: 710.6172\n",
      "Epoch 43/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 735.2738 - val_loss: 710.3953\n",
      "Epoch 44/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 735.0540 - val_loss: 710.1639\n",
      "Epoch 45/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 734.8227 - val_loss: 709.9231\n",
      "Epoch 46/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 734.5824 - val_loss: 709.6702\n",
      "Epoch 47/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 734.3301 - val_loss: 709.4077\n",
      "Epoch 48/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 734.0705 - val_loss: 709.1325\n",
      "Epoch 49/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 733.7962 - val_loss: 708.8454\n",
      "Epoch 50/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 733.5131 - val_loss: 708.5466\n",
      "Epoch 51/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 733.2152 - val_loss: 708.2349\n",
      "Epoch 52/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 732.9054 - val_loss: 707.9100\n",
      "Epoch 53/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 732.5819 - val_loss: 707.5693\n",
      "Epoch 54/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 732.2427 - val_loss: 707.2139\n",
      "Epoch 55/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 731.8895 - val_loss: 706.8430\n",
      "Epoch 56/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 731.5227 - val_loss: 706.4556\n",
      "Epoch 57/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 731.1389 - val_loss: 706.0521\n",
      "Epoch 58/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 730.7390 - val_loss: 705.6314\n",
      "Epoch 59/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 730.3225 - val_loss: 705.1932\n",
      "Epoch 60/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 729.8819 - val_loss: 704.7370\n",
      "Epoch 61/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 729.4317 - val_loss: 704.2605\n",
      "Epoch 62/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 728.9616 - val_loss: 703.7640\n",
      "Epoch 63/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 728.4695 - val_loss: 703.2471\n",
      "Epoch 64/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 727.9537 - val_loss: 702.7092\n",
      "Epoch 65/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 727.4250 - val_loss: 702.1487\n",
      "Epoch 66/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 726.8655 - val_loss: 701.5661\n",
      "Epoch 67/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 726.2902 - val_loss: 700.9595\n",
      "Epoch 68/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 725.6844 - val_loss: 700.3286\n",
      "Epoch 69/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 725.0562 - val_loss: 699.6717\n",
      "Epoch 70/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 724.4095 - val_loss: 698.9878\n",
      "Epoch 71/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 723.7340 - val_loss: 698.2770\n",
      "Epoch 72/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 723.0209 - val_loss: 697.5393\n",
      "Epoch 73/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 722.2906 - val_loss: 696.7717\n",
      "Epoch 74/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 721.5405 - val_loss: 695.9739\n",
      "Epoch 75/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 720.7380 - val_loss: 695.1478\n",
      "Epoch 76/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 719.9234 - val_loss: 694.2894\n",
      "Epoch 77/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 719.0734 - val_loss: 693.3989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 718.1819 - val_loss: 692.4761\n",
      "Epoch 79/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 717.2762 - val_loss: 691.5179\n",
      "Epoch 80/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 716.3301 - val_loss: 690.5257\n",
      "Epoch 81/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 715.3473 - val_loss: 689.4995\n",
      "Epoch 82/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 714.3306 - val_loss: 688.4380\n",
      "Epoch 83/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 713.2739 - val_loss: 687.3401\n",
      "Epoch 84/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 712.1845 - val_loss: 686.2039\n",
      "Epoch 85/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 711.0665 - val_loss: 685.0283\n",
      "Epoch 86/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 709.8995 - val_loss: 683.8140\n",
      "Epoch 87/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 708.6918 - val_loss: 682.5596\n",
      "Epoch 88/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 707.4504 - val_loss: 681.2630\n",
      "Epoch 89/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 706.1792 - val_loss: 679.9239\n",
      "Epoch 90/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 704.8480 - val_loss: 678.5442\n",
      "Epoch 91/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 703.4884 - val_loss: 677.1213\n",
      "Epoch 92/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 702.0776 - val_loss: 675.6555\n",
      "Epoch 93/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 700.6109 - val_loss: 674.1451\n",
      "Epoch 94/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 699.1338 - val_loss: 672.5856\n",
      "Epoch 95/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 699.004 - 0s 81us/step - loss: 697.5825 - val_loss: 670.9805\n",
      "Epoch 96/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 695.9884 - val_loss: 669.3269\n",
      "Epoch 97/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 694.3626 - val_loss: 667.6227\n",
      "Epoch 98/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 692.6739 - val_loss: 665.8696\n",
      "Epoch 99/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 690.9275 - val_loss: 664.0658\n",
      "Epoch 100/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 689.1647 - val_loss: 662.2078\n",
      "Epoch 101/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 687.3195 - val_loss: 660.3000\n",
      "Epoch 102/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 685.4176 - val_loss: 658.3388\n",
      "Epoch 103/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 683.4778 - val_loss: 656.3201\n",
      "Epoch 104/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 681.4967 - val_loss: 654.2435\n",
      "Epoch 105/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 679.4486 - val_loss: 652.1118\n",
      "Epoch 106/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 677.3135 - val_loss: 649.9250\n",
      "Epoch 107/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 675.1723 - val_loss: 647.6750\n",
      "Epoch 108/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 672.9414 - val_loss: 645.3667\n",
      "Epoch 109/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 670.6657 - val_loss: 642.9971\n",
      "Epoch 110/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 668.3284 - val_loss: 640.5667\n",
      "Epoch 111/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 665.9156 - val_loss: 638.0751\n",
      "Epoch 112/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 663.4510 - val_loss: 635.5185\n",
      "Epoch 113/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 660.9179 - val_loss: 632.8956\n",
      "Epoch 114/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 658.3418 - val_loss: 630.2040\n",
      "Epoch 115/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 655.6887 - val_loss: 627.4468\n",
      "Epoch 116/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 652.9675 - val_loss: 624.6241\n",
      "Epoch 117/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 650.1734 - val_loss: 621.7344\n",
      "Epoch 118/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 647.3028 - val_loss: 618.7750\n",
      "Epoch 119/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 644.3630 - val_loss: 615.7408\n",
      "Epoch 120/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 641.3924 - val_loss: 612.6277\n",
      "Epoch 121/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 638.3103 - val_loss: 609.4421\n",
      "Epoch 122/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 635.1669 - val_loss: 606.1806\n",
      "Epoch 123/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 631.9442 - val_loss: 602.8428\n",
      "Epoch 124/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 628.6489 - val_loss: 599.4271\n",
      "Epoch 125/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 625.2973 - val_loss: 595.9326\n",
      "Epoch 126/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 621.7898 - val_loss: 592.3631\n",
      "Epoch 127/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 618.2816 - val_loss: 588.7031\n",
      "Epoch 128/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 614.6769 - val_loss: 584.9573\n",
      "Epoch 129/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 610.9798 - val_loss: 581.1271\n",
      "Epoch 130/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 607.2063 - val_loss: 577.2115\n",
      "Epoch 131/1500\n",
      "99/99 [==============================] - 0s 131us/step - loss: 603.3401 - val_loss: 573.2111\n",
      "Epoch 132/1500\n",
      "99/99 [==============================] - 0s 121us/step - loss: 599.3965 - val_loss: 569.1238\n",
      "Epoch 133/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 595.3594 - val_loss: 564.9493\n",
      "Epoch 134/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 591.2487 - val_loss: 560.6851\n",
      "Epoch 135/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 586.9965 - val_loss: 556.3321\n",
      "Epoch 136/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 582.7714 - val_loss: 551.8779\n",
      "Epoch 137/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 578.3715 - val_loss: 547.3393\n",
      "Epoch 138/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 573.8471 - val_loss: 542.7117\n",
      "Epoch 139/1500\n",
      "99/99 [==============================] - 0s 131us/step - loss: 569.3238 - val_loss: 537.9810\n",
      "Epoch 140/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 564.6372 - val_loss: 533.1570\n",
      "Epoch 141/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 559.8633 - val_loss: 528.2331\n",
      "Epoch 142/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 555.0137 - val_loss: 523.2040\n",
      "Epoch 143/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 550.0758 - val_loss: 518.0710\n",
      "Epoch 144/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 545.0297 - val_loss: 512.8383\n",
      "Epoch 145/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 539.8512 - val_loss: 507.5071\n",
      "Epoch 146/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 534.6346 - val_loss: 502.0701\n",
      "Epoch 147/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 529.2574 - val_loss: 496.5350\n",
      "Epoch 148/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 523.7800 - val_loss: 490.8942\n",
      "Epoch 149/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 518.2326 - val_loss: 485.1406\n",
      "Epoch 150/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 512.5434 - val_loss: 479.2766\n",
      "Epoch 151/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 506.8208 - val_loss: 473.2961\n",
      "Epoch 152/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 500.8863 - val_loss: 467.2121\n",
      "Epoch 153/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 494.8997 - val_loss: 461.0107\n",
      "Epoch 154/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 488.7572 - val_loss: 454.6933\n",
      "Epoch 155/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 482.5302 - val_loss: 448.2506\n",
      "Epoch 156/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 476.2024 - val_loss: 441.6825\n",
      "Epoch 157/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 469.6995 - val_loss: 434.9935\n",
      "Epoch 158/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 463.1800 - val_loss: 428.1742\n",
      "Epoch 159/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 456.4699 - val_loss: 421.2415\n",
      "Epoch 160/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 449.6011 - val_loss: 414.1930\n",
      "Epoch 161/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 442.6654 - val_loss: 407.0150\n",
      "Epoch 162/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 435.5914 - val_loss: 399.7082\n",
      "Epoch 163/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 428.4266 - val_loss: 392.2699\n",
      "Epoch 164/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 421.0529 - val_loss: 384.7063\n",
      "Epoch 165/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 413.6772 - val_loss: 377.0027\n",
      "Epoch 166/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 406.0658 - val_loss: 369.1764\n",
      "Epoch 167/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 398.3338 - val_loss: 361.2164\n",
      "Epoch 168/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 390.4875 - val_loss: 353.1145\n",
      "Epoch 169/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 382.5072 - val_loss: 344.8679\n",
      "Epoch 170/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 374.4113 - val_loss: 336.4751\n",
      "Epoch 171/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 366.1229 - val_loss: 327.9413\n",
      "Epoch 172/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 357.7385 - val_loss: 319.2581\n",
      "Epoch 173/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 349.2090 - val_loss: 310.4287\n",
      "Epoch 174/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 340.4754 - val_loss: 301.4560\n",
      "Epoch 175/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 331.6739 - val_loss: 292.3266\n",
      "Epoch 176/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 322.6543 - val_loss: 283.0482\n",
      "Epoch 177/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 313.5952 - val_loss: 273.6098\n",
      "Epoch 178/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 304.2328 - val_loss: 264.0293\n",
      "Epoch 179/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 294.8256 - val_loss: 254.2840\n",
      "Epoch 180/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 285.2945 - val_loss: 244.3789\n",
      "Epoch 181/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 275.5477 - val_loss: 234.3268\n",
      "Epoch 182/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 265.6499 - val_loss: 224.1228\n",
      "Epoch 183/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 255.5202 - val_loss: 213.7617\n",
      "Epoch 184/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 245.5059 - val_loss: 203.2179\n",
      "Epoch 185/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 235.0451 - val_loss: 192.5378\n",
      "Epoch 186/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 225.0298 - val_loss: 183.0308\n",
      "Epoch 187/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 216.3448 - val_loss: 179.0787\n",
      "Epoch 188/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 209.2133 - val_loss: 177.5882\n",
      "Epoch 189/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 203.3661 - val_loss: 176.1661\n",
      "Epoch 190/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 198.4046 - val_loss: 174.7951\n",
      "Epoch 191/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 194.2525 - val_loss: 173.4795\n",
      "Epoch 192/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 190.7086 - val_loss: 172.2111\n",
      "Epoch 193/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 187.5871 - val_loss: 171.0060\n",
      "Epoch 194/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 184.7091 - val_loss: 169.8606\n",
      "Epoch 195/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 181.8187 - val_loss: 168.7580\n",
      "Epoch 196/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 179.8590 - val_loss: 167.6719\n",
      "Epoch 197/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 177.6629 - val_loss: 166.6297\n",
      "Epoch 198/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 175.2099 - val_loss: 165.6317\n",
      "Epoch 199/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 173.1836 - val_loss: 164.6530\n",
      "Epoch 200/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 171.3042 - val_loss: 164.3711\n",
      "Epoch 201/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 169.3649 - val_loss: 164.2635\n",
      "Epoch 202/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 165.322 - 0s 71us/step - loss: 167.7558 - val_loss: 164.1511\n",
      "Epoch 203/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 166.2908 - val_loss: 164.0329\n",
      "Epoch 204/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 164.9307 - val_loss: 163.9078\n",
      "Epoch 205/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 163.6647 - val_loss: 163.8818\n",
      "Epoch 206/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 161.9897 - val_loss: 164.4425\n",
      "Epoch 207/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 160.6628 - val_loss: 165.0087\n",
      "Epoch 208/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 159.4647 - val_loss: 165.5750\n",
      "Epoch 209/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 158.2071 - val_loss: 166.1098\n",
      "Epoch 210/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 157.3237 - val_loss: 166.6398\n",
      "Epoch 211/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 156.1679 - val_loss: 167.1189\n",
      "Epoch 212/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 155.2707 - val_loss: 167.5799\n",
      "Epoch 213/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 154.3769 - val_loss: 167.9996\n",
      "Epoch 214/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 153.6121 - val_loss: 168.3829\n",
      "Epoch 215/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 152.5877 - val_loss: 168.7176\n",
      "Epoch 216/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 151.8074 - val_loss: 169.0645\n",
      "Epoch 217/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 151.0809 - val_loss: 169.4285\n",
      "Epoch 218/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 150.3564 - val_loss: 169.7825\n",
      "Epoch 219/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 149.4756 - val_loss: 170.1029\n",
      "Epoch 220/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 149.0569 - val_loss: 170.4351\n",
      "Epoch 221/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 148.1700 - val_loss: 170.7034\n",
      "Epoch 222/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 147.7179 - val_loss: 170.9842\n",
      "Epoch 223/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 147.2823 - val_loss: 171.2504\n",
      "Epoch 224/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 146.9501 - val_loss: 171.4793\n",
      "Epoch 225/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 146.5036 - val_loss: 171.6516\n",
      "Epoch 226/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 146.1923 - val_loss: 171.7797\n",
      "Epoch 227/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 145.7635 - val_loss: 171.8414\n",
      "Epoch 228/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 145.5601 - val_loss: 171.9015\n",
      "Epoch 229/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 145.1619 - val_loss: 171.9077\n",
      "Epoch 230/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 71us/step - loss: 144.9167 - val_loss: 171.9248\n",
      "Epoch 231/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 144.6809 - val_loss: 171.9245\n",
      "Epoch 232/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 144.4396 - val_loss: 171.8728\n",
      "Epoch 233/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 144.1749 - val_loss: 171.7678\n",
      "Epoch 234/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 143.9579 - val_loss: 171.6373\n",
      "Epoch 235/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 143.7322 - val_loss: 171.5043\n",
      "Epoch 236/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 143.5126 - val_loss: 171.3812\n",
      "Epoch 237/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 143.2963 - val_loss: 171.2744\n",
      "Epoch 238/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 143.0805 - val_loss: 171.1852\n",
      "Epoch 239/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 142.8849 - val_loss: 171.1044\n",
      "Epoch 240/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 142.6678 - val_loss: 171.0023\n",
      "Epoch 241/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 142.4524 - val_loss: 170.8704\n",
      "Epoch 242/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 142.2618 - val_loss: 170.7531\n",
      "Epoch 243/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 142.0690 - val_loss: 170.6202\n",
      "Epoch 244/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 141.8523 - val_loss: 170.4686\n",
      "Epoch 245/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 141.6658 - val_loss: 170.3402\n",
      "Epoch 246/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 141.4979 - val_loss: 170.1884\n",
      "Epoch 247/1500\n",
      "99/99 [==============================] - 0s 51us/step - loss: 141.2579 - val_loss: 169.9878\n",
      "Epoch 248/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 141.0536 - val_loss: 169.7963\n",
      "Epoch 249/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 140.8678 - val_loss: 169.5883\n",
      "Epoch 250/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 140.6603 - val_loss: 169.3544\n",
      "Epoch 251/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 140.4652 - val_loss: 169.0950\n",
      "Epoch 252/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 140.2419 - val_loss: 168.8625\n",
      "Epoch 253/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 140.0603 - val_loss: 168.6220\n",
      "Epoch 254/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 139.8578 - val_loss: 168.3999\n",
      "Epoch 255/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 139.6437 - val_loss: 168.1765\n",
      "Epoch 256/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 139.4569 - val_loss: 167.9269\n",
      "Epoch 257/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 139.2580 - val_loss: 167.6705\n",
      "Epoch 258/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 139.0562 - val_loss: 167.4273\n",
      "Epoch 259/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 138.8588 - val_loss: 167.1738\n",
      "Epoch 260/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 138.6597 - val_loss: 166.9361\n",
      "Epoch 261/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 138.4690 - val_loss: 166.6841\n",
      "Epoch 262/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 138.3057 - val_loss: 166.4058\n",
      "Epoch 263/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 138.0678 - val_loss: 166.1821\n",
      "Epoch 264/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 137.8686 - val_loss: 165.9797\n",
      "Epoch 265/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 137.6885 - val_loss: 165.7801\n",
      "Epoch 266/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 137.4790 - val_loss: 165.5300\n",
      "Epoch 267/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 137.2728 - val_loss: 165.3081\n",
      "Epoch 268/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 137.0875 - val_loss: 165.0946\n",
      "Epoch 269/1500\n",
      "99/99 [==============================] - 0s 51us/step - loss: 136.8675 - val_loss: 164.9335\n",
      "Epoch 270/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 136.6637 - val_loss: 164.7514\n",
      "Epoch 271/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 136.4797 - val_loss: 164.5484\n",
      "Epoch 272/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 136.2893 - val_loss: 164.2905\n",
      "Epoch 273/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 136.0830 - val_loss: 164.0423\n",
      "Epoch 274/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 135.8651 - val_loss: 163.7744\n",
      "Epoch 275/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 135.6837 - val_loss: 163.4766\n",
      "Epoch 276/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 135.5217 - val_loss: 163.2012\n",
      "Epoch 277/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 135.2755 - val_loss: 162.9869\n",
      "Epoch 278/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 135.0928 - val_loss: 162.7647\n",
      "Epoch 279/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 134.8906 - val_loss: 162.5390\n",
      "Epoch 280/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 134.7112 - val_loss: 162.3005\n",
      "Epoch 281/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 134.5106 - val_loss: 162.0778\n",
      "Epoch 282/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 134.3234 - val_loss: 161.8357\n",
      "Epoch 283/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 134.1309 - val_loss: 161.6113\n",
      "Epoch 284/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 133.9397 - val_loss: 161.3596\n",
      "Epoch 285/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 133.7373 - val_loss: 161.1000\n",
      "Epoch 286/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 133.5591 - val_loss: 160.8149\n",
      "Epoch 287/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 133.3773 - val_loss: 160.5514\n",
      "Epoch 288/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 133.1746 - val_loss: 160.3201\n",
      "Epoch 289/1500\n",
      "99/99 [==============================] - 0s 51us/step - loss: 132.9925 - val_loss: 160.0702\n",
      "Epoch 290/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 132.7970 - val_loss: 159.8502\n",
      "Epoch 291/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 132.5966 - val_loss: 159.6605\n",
      "Epoch 292/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 132.4132 - val_loss: 159.4606\n",
      "Epoch 293/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 132.2033 - val_loss: 159.2267\n",
      "Epoch 294/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 132.0214 - val_loss: 158.9830\n",
      "Epoch 295/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 131.8232 - val_loss: 158.7278\n",
      "Epoch 296/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 131.6236 - val_loss: 158.4409\n",
      "Epoch 297/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 131.4490 - val_loss: 158.1629\n",
      "Epoch 298/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 131.2430 - val_loss: 157.9272\n",
      "Epoch 299/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 131.0552 - val_loss: 157.7031\n",
      "Epoch 300/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 130.8484 - val_loss: 157.5132\n",
      "Epoch 301/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 130.6573 - val_loss: 157.2902\n",
      "Epoch 302/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 130.4566 - val_loss: 157.0779\n",
      "Epoch 303/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 130.2654 - val_loss: 156.8608\n",
      "Epoch 304/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 130.0692 - val_loss: 156.6507\n",
      "Epoch 305/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 129.8877 - val_loss: 156.4540\n",
      "Epoch 306/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 129.6993 - val_loss: 156.2236\n",
      "Epoch 307/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 129.4897 - val_loss: 156.0291\n",
      "Epoch 308/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 129.2967 - val_loss: 155.8292\n",
      "Epoch 309/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 129.1045 - val_loss: 155.6344\n",
      "Epoch 310/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 128.9057 - val_loss: 155.4346\n",
      "Epoch 311/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 128.7231 - val_loss: 155.2342\n",
      "Epoch 312/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 128.5279 - val_loss: 155.0414\n",
      "Epoch 313/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 128.3425 - val_loss: 154.8716\n",
      "Epoch 314/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 128.1531 - val_loss: 154.6714\n",
      "Epoch 315/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 127.9443 - val_loss: 154.5103\n",
      "Epoch 316/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 127.7701 - val_loss: 154.3476\n",
      "Epoch 317/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 127.5642 - val_loss: 154.1606\n",
      "Epoch 318/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 127.4035 - val_loss: 153.9975\n",
      "Epoch 319/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 127.2009 - val_loss: 153.7814\n",
      "Epoch 320/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 126.9980 - val_loss: 153.5441\n",
      "Epoch 321/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 126.7950 - val_loss: 153.2992\n",
      "Epoch 322/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 126.6193 - val_loss: 153.0346\n",
      "Epoch 323/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 126.4395 - val_loss: 152.7215\n",
      "Epoch 324/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 126.2251 - val_loss: 152.4526\n",
      "Epoch 325/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 126.0327 - val_loss: 152.2111\n",
      "Epoch 326/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 125.8385 - val_loss: 152.0004\n",
      "Epoch 327/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 125.6407 - val_loss: 151.8210\n",
      "Epoch 328/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 125.4441 - val_loss: 151.6195\n",
      "Epoch 329/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 125.2675 - val_loss: 151.4206\n",
      "Epoch 330/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 125.0622 - val_loss: 151.1916\n",
      "Epoch 331/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 124.8627 - val_loss: 150.9425\n",
      "Epoch 332/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 124.6753 - val_loss: 150.6978\n",
      "Epoch 333/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 124.4863 - val_loss: 150.4530\n",
      "Epoch 334/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 124.3126 - val_loss: 150.1791\n",
      "Epoch 335/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 124.0990 - val_loss: 149.9434\n",
      "Epoch 336/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 123.9032 - val_loss: 149.7132\n",
      "Epoch 337/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 123.7155 - val_loss: 149.4771\n",
      "Epoch 338/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 123.5212 - val_loss: 149.2488\n",
      "Epoch 339/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 115.768 - 0s 71us/step - loss: 123.3235 - val_loss: 149.0267\n",
      "Epoch 340/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 123.1287 - val_loss: 148.8210\n",
      "Epoch 341/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 122.9304 - val_loss: 148.6129\n",
      "Epoch 342/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 122.7395 - val_loss: 148.4175\n",
      "Epoch 343/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 122.5472 - val_loss: 148.2085\n",
      "Epoch 344/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 122.3615 - val_loss: 148.0066\n",
      "Epoch 345/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 122.1507 - val_loss: 147.7721\n",
      "Epoch 346/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 122.0067 - val_loss: 147.5266\n",
      "Epoch 347/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 121.7580 - val_loss: 147.3459\n",
      "Epoch 348/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 121.5601 - val_loss: 147.1697\n",
      "Epoch 349/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 121.3768 - val_loss: 146.9946\n",
      "Epoch 350/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 121.1883 - val_loss: 146.8076\n",
      "Epoch 351/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 120.9997 - val_loss: 146.6111\n",
      "Epoch 352/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 120.8230 - val_loss: 146.3948\n",
      "Epoch 353/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 120.6221 - val_loss: 146.2016\n",
      "Epoch 354/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 120.4311 - val_loss: 146.0265\n",
      "Epoch 355/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 120.2414 - val_loss: 145.8436\n",
      "Epoch 356/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 120.0663 - val_loss: 145.6393\n",
      "Epoch 357/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 119.8809 - val_loss: 145.4070\n",
      "Epoch 358/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 119.6843 - val_loss: 145.1974\n",
      "Epoch 359/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 119.4847 - val_loss: 144.9510\n",
      "Epoch 360/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 119.2868 - val_loss: 144.7125\n",
      "Epoch 361/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 119.1033 - val_loss: 144.4632\n",
      "Epoch 362/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 118.9040 - val_loss: 144.2252\n",
      "Epoch 363/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 118.7264 - val_loss: 143.9874\n",
      "Epoch 364/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 118.5173 - val_loss: 143.7723\n",
      "Epoch 365/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 118.3263 - val_loss: 143.5558\n",
      "Epoch 366/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 118.1351 - val_loss: 143.3301\n",
      "Epoch 367/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 117.9551 - val_loss: 143.0863\n",
      "Epoch 368/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 117.7481 - val_loss: 142.8677\n",
      "Epoch 369/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 117.5895 - val_loss: 142.6357\n",
      "Epoch 370/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 117.3688 - val_loss: 142.4440\n",
      "Epoch 371/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 117.1751 - val_loss: 142.2276\n",
      "Epoch 372/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 116.9916 - val_loss: 141.9995\n",
      "Epoch 373/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 116.7905 - val_loss: 141.7926\n",
      "Epoch 374/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 116.5984 - val_loss: 141.5895\n",
      "Epoch 375/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 116.4162 - val_loss: 141.3815\n",
      "Epoch 376/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 116.2134 - val_loss: 141.1595\n",
      "Epoch 377/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 116.0255 - val_loss: 140.9352\n",
      "Epoch 378/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 115.8386 - val_loss: 140.7186\n",
      "Epoch 379/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 115.6411 - val_loss: 140.5204\n",
      "Epoch 380/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 115.4504 - val_loss: 140.3115\n",
      "Epoch 381/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 115.2769 - val_loss: 140.1047\n",
      "Epoch 382/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 61us/step - loss: 115.0734 - val_loss: 139.8574\n",
      "Epoch 383/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 114.8651 - val_loss: 139.6002\n",
      "Epoch 384/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 114.6830 - val_loss: 139.3340\n",
      "Epoch 385/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 114.4814 - val_loss: 139.0191\n",
      "Epoch 386/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 114.3268 - val_loss: 138.7222\n",
      "Epoch 387/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 114.1232 - val_loss: 138.4665\n",
      "Epoch 388/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 113.9401 - val_loss: 138.2275\n",
      "Epoch 389/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 113.7445 - val_loss: 137.9993\n",
      "Epoch 390/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 113.5592 - val_loss: 137.7756\n",
      "Epoch 391/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 113.3669 - val_loss: 137.5388\n",
      "Epoch 392/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 113.1811 - val_loss: 137.3102\n",
      "Epoch 393/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 112.9889 - val_loss: 137.0726\n",
      "Epoch 394/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 112.8184 - val_loss: 136.8421\n",
      "Epoch 395/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 112.6149 - val_loss: 136.6507\n",
      "Epoch 396/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 112.4266 - val_loss: 136.4773\n",
      "Epoch 397/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 112.2252 - val_loss: 136.2923\n",
      "Epoch 398/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 112.0325 - val_loss: 136.1299\n",
      "Epoch 399/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 111.8392 - val_loss: 135.9726\n",
      "Epoch 400/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 111.6486 - val_loss: 135.7959\n",
      "Epoch 401/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 111.4396 - val_loss: 135.5966\n",
      "Epoch 402/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 111.2360 - val_loss: 135.4171\n",
      "Epoch 403/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 111.0576 - val_loss: 135.2391\n",
      "Epoch 404/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 110.8557 - val_loss: 135.0406\n",
      "Epoch 405/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 110.6703 - val_loss: 134.8403\n",
      "Epoch 406/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 110.4724 - val_loss: 134.6252\n",
      "Epoch 407/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 110.2846 - val_loss: 134.4059\n",
      "Epoch 408/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 110.0922 - val_loss: 134.1779\n",
      "Epoch 409/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 109.8934 - val_loss: 133.9698\n",
      "Epoch 410/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 109.7100 - val_loss: 133.7625\n",
      "Epoch 411/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 109.5111 - val_loss: 133.5706\n",
      "Epoch 412/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 109.3221 - val_loss: 133.3602\n",
      "Epoch 413/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 109.1224 - val_loss: 133.1268\n",
      "Epoch 414/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 108.9347 - val_loss: 132.8967\n",
      "Epoch 415/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 108.7440 - val_loss: 132.6806\n",
      "Epoch 416/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 108.5822 - val_loss: 132.4447\n",
      "Epoch 417/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 108.3424 - val_loss: 132.2562\n",
      "Epoch 418/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 108.1647 - val_loss: 132.0772\n",
      "Epoch 419/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 102.852 - 0s 71us/step - loss: 107.9787 - val_loss: 131.8934\n",
      "Epoch 420/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 107.7698 - val_loss: 131.6888\n",
      "Epoch 421/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 107.5805 - val_loss: 131.4940\n",
      "Epoch 422/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 107.3898 - val_loss: 131.2925\n",
      "Epoch 423/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 107.1979 - val_loss: 131.1147\n",
      "Epoch 424/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 106.9981 - val_loss: 130.9293\n",
      "Epoch 425/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 106.8066 - val_loss: 130.7417\n",
      "Epoch 426/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 106.6156 - val_loss: 130.5532\n",
      "Epoch 427/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 106.4342 - val_loss: 130.3823\n",
      "Epoch 428/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 106.2270 - val_loss: 130.1841\n",
      "Epoch 429/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 106.0307 - val_loss: 129.9954\n",
      "Epoch 430/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 105.8370 - val_loss: 129.8282\n",
      "Epoch 431/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 105.6425 - val_loss: 129.6444\n",
      "Epoch 432/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 105.4437 - val_loss: 129.4691\n",
      "Epoch 433/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 105.2719 - val_loss: 129.3167\n",
      "Epoch 434/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 105.0584 - val_loss: 129.1408\n",
      "Epoch 435/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 104.8795 - val_loss: 128.9612\n",
      "Epoch 436/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 104.6825 - val_loss: 128.7524\n",
      "Epoch 437/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 104.4721 - val_loss: 128.5111\n",
      "Epoch 438/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 104.2979 - val_loss: 128.2664\n",
      "Epoch 439/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 104.1014 - val_loss: 128.0532\n",
      "Epoch 440/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 103.9097 - val_loss: 127.8637\n",
      "Epoch 441/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 103.7068 - val_loss: 127.7105\n",
      "Epoch 442/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 103.4990 - val_loss: 127.5484\n",
      "Epoch 443/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 103.3018 - val_loss: 127.3884\n",
      "Epoch 444/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 103.1369 - val_loss: 127.2387\n",
      "Epoch 445/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 102.9429 - val_loss: 127.0618\n",
      "Epoch 446/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 102.7232 - val_loss: 126.8589\n",
      "Epoch 447/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 102.5284 - val_loss: 126.6534\n",
      "Epoch 448/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 102.3258 - val_loss: 126.4668\n",
      "Epoch 449/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 102.1359 - val_loss: 126.2778\n",
      "Epoch 450/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 101.9418 - val_loss: 126.0811\n",
      "Epoch 451/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 101.7347 - val_loss: 125.9173\n",
      "Epoch 452/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 101.5438 - val_loss: 125.7438\n",
      "Epoch 453/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 101.3574 - val_loss: 125.5503\n",
      "Epoch 454/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 101.1875 - val_loss: 125.3368\n",
      "Epoch 455/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 100.9628 - val_loss: 125.0792\n",
      "Epoch 456/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 100.8030 - val_loss: 124.8159\n",
      "Epoch 457/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 100.5650 - val_loss: 124.5926\n",
      "Epoch 458/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 100.3728 - val_loss: 124.3643\n",
      "Epoch 459/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 100.1947 - val_loss: 124.1429\n",
      "Epoch 460/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 100.0189 - val_loss: 123.8786\n",
      "Epoch 461/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 99.7903 - val_loss: 123.6543\n",
      "Epoch 462/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 99.5857 - val_loss: 123.4036\n",
      "Epoch 463/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 99.3968 - val_loss: 123.1581\n",
      "Epoch 464/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 99.2038 - val_loss: 122.8957\n",
      "Epoch 465/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 99.0136 - val_loss: 122.5966\n",
      "Epoch 466/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 98.8135 - val_loss: 122.3178\n",
      "Epoch 467/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 98.6122 - val_loss: 122.0620\n",
      "Epoch 468/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 98.4288 - val_loss: 121.8283\n",
      "Epoch 469/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 98.2438 - val_loss: 121.5837\n",
      "Epoch 470/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 98.0471 - val_loss: 121.3647\n",
      "Epoch 471/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 97.8533 - val_loss: 121.1750\n",
      "Epoch 472/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 97.6516 - val_loss: 120.9811\n",
      "Epoch 473/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 97.4650 - val_loss: 120.7936\n",
      "Epoch 474/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 97.2859 - val_loss: 120.6064\n",
      "Epoch 475/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 97.0993 - val_loss: 120.4089\n",
      "Epoch 476/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 96.9031 - val_loss: 120.1935\n",
      "Epoch 477/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 96.7138 - val_loss: 119.9883\n",
      "Epoch 478/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 96.5323 - val_loss: 119.7691\n",
      "Epoch 479/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 96.3444 - val_loss: 119.5510\n",
      "Epoch 480/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 96.1565 - val_loss: 119.3503\n",
      "Epoch 481/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 95.9560 - val_loss: 119.1726\n",
      "Epoch 482/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 95.7754 - val_loss: 118.9828\n",
      "Epoch 483/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 95.5780 - val_loss: 118.7660\n",
      "Epoch 484/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 95.3914 - val_loss: 118.5374\n",
      "Epoch 485/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 95.1944 - val_loss: 118.3124\n",
      "Epoch 486/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 94.9952 - val_loss: 118.0734\n",
      "Epoch 487/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 94.8034 - val_loss: 117.8345\n",
      "Epoch 488/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 94.6190 - val_loss: 117.5962\n",
      "Epoch 489/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 94.4198 - val_loss: 117.3745\n",
      "Epoch 490/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 94.2240 - val_loss: 117.1371\n",
      "Epoch 491/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 94.0335 - val_loss: 116.9071\n",
      "Epoch 492/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 93.8467 - val_loss: 116.6910\n",
      "Epoch 493/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 93.6461 - val_loss: 116.4734\n",
      "Epoch 494/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 93.4464 - val_loss: 116.2500\n",
      "Epoch 495/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 93.2633 - val_loss: 116.0306\n",
      "Epoch 496/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 93.0686 - val_loss: 115.8058\n",
      "Epoch 497/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 92.8728 - val_loss: 115.5832\n",
      "Epoch 498/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 92.6862 - val_loss: 115.3537\n",
      "Epoch 499/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 92.4888 - val_loss: 115.1070\n",
      "Epoch 500/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 92.3166 - val_loss: 114.8369\n",
      "Epoch 501/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 92.1492 - val_loss: 114.5929\n",
      "Epoch 502/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 91.9149 - val_loss: 114.3898\n",
      "Epoch 503/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 91.7230 - val_loss: 114.1983\n",
      "Epoch 504/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 91.5261 - val_loss: 114.0037\n",
      "Epoch 505/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 91.3673 - val_loss: 113.8048\n",
      "Epoch 506/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 91.1532 - val_loss: 113.5688\n",
      "Epoch 507/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 90.9604 - val_loss: 113.3551\n",
      "Epoch 508/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 90.7582 - val_loss: 113.1320\n",
      "Epoch 509/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 90.5590 - val_loss: 112.9111\n",
      "Epoch 510/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 90.3721 - val_loss: 112.6872\n",
      "Epoch 511/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 90.1894 - val_loss: 112.4700\n",
      "Epoch 512/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 89.9869 - val_loss: 112.2528\n",
      "Epoch 513/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 89.8122 - val_loss: 112.0293\n",
      "Epoch 514/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 89.6277 - val_loss: 111.8358\n",
      "Epoch 515/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 89.4260 - val_loss: 111.6157\n",
      "Epoch 516/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 89.2313 - val_loss: 111.3653\n",
      "Epoch 517/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 89.0224 - val_loss: 111.1338\n",
      "Epoch 518/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 88.8366 - val_loss: 110.9100\n",
      "Epoch 519/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 88.6520 - val_loss: 110.6762\n",
      "Epoch 520/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 88.4499 - val_loss: 110.4336\n",
      "Epoch 521/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 88.3184 - val_loss: 110.1774\n",
      "Epoch 522/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 88.0786 - val_loss: 109.9661\n",
      "Epoch 523/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 87.8876 - val_loss: 109.7505\n",
      "Epoch 524/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 87.7119 - val_loss: 109.5466\n",
      "Epoch 525/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 87.4994 - val_loss: 109.3211\n",
      "Epoch 526/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 87.3158 - val_loss: 109.0894\n",
      "Epoch 527/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 87.1491 - val_loss: 108.8402\n",
      "Epoch 528/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 86.9358 - val_loss: 108.6242\n",
      "Epoch 529/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 86.7607 - val_loss: 108.4055\n",
      "Epoch 530/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 86.5570 - val_loss: 108.2119\n",
      "Epoch 531/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 86.3833 - val_loss: 108.0265\n",
      "Epoch 532/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 86.1720 - val_loss: 107.8238\n",
      "Epoch 533/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 85.9889 - val_loss: 107.6272\n",
      "Epoch 534/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 85.7858 - val_loss: 107.4493\n",
      "Epoch 535/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 71us/step - loss: 85.5920 - val_loss: 107.2837\n",
      "Epoch 536/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 85.3955 - val_loss: 107.1231\n",
      "Epoch 537/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 85.2145 - val_loss: 106.9692\n",
      "Epoch 538/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 85.0141 - val_loss: 106.8063\n",
      "Epoch 539/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 84.8559 - val_loss: 106.6574\n",
      "Epoch 540/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 84.6729 - val_loss: 106.5344\n",
      "Epoch 541/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 84.4694 - val_loss: 106.3538\n",
      "Epoch 542/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 84.2924 - val_loss: 106.1542\n",
      "Epoch 543/1500\n",
      "99/99 [==============================] - 0s 141us/step - loss: 84.0994 - val_loss: 105.9502\n",
      "Epoch 544/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 83.9426 - val_loss: 105.7444\n",
      "Epoch 545/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 83.7191 - val_loss: 105.4291\n",
      "Epoch 546/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 83.5290 - val_loss: 105.0980\n",
      "Epoch 547/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 83.3718 - val_loss: 104.8298\n",
      "Epoch 548/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 83.1449 - val_loss: 104.6011\n",
      "Epoch 549/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 82.9810 - val_loss: 104.3532\n",
      "Epoch 550/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 82.7685 - val_loss: 104.1277\n",
      "Epoch 551/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 82.5705 - val_loss: 103.9037\n",
      "Epoch 552/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 82.3863 - val_loss: 103.6817\n",
      "Epoch 553/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 82.1969 - val_loss: 103.4649\n",
      "Epoch 554/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 82.0126 - val_loss: 103.2418\n",
      "Epoch 555/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 81.8124 - val_loss: 103.0321\n",
      "Epoch 556/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 81.6312 - val_loss: 102.8213\n",
      "Epoch 557/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 81.4532 - val_loss: 102.5910\n",
      "Epoch 558/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 81.2364 - val_loss: 102.3839\n",
      "Epoch 559/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 81.0605 - val_loss: 102.1830\n",
      "Epoch 560/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 80.8599 - val_loss: 101.9778\n",
      "Epoch 561/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 80.6845 - val_loss: 101.7644\n",
      "Epoch 562/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 80.4809 - val_loss: 101.5283\n",
      "Epoch 563/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 80.2881 - val_loss: 101.3002\n",
      "Epoch 564/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 80.0969 - val_loss: 101.0792\n",
      "Epoch 565/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 79.9486 - val_loss: 100.8467\n",
      "Epoch 566/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 79.7107 - val_loss: 100.6501\n",
      "Epoch 567/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 79.5205 - val_loss: 100.4792\n",
      "Epoch 568/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 79.3229 - val_loss: 100.3567\n",
      "Epoch 569/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 79.1528 - val_loss: 100.2084\n",
      "Epoch 570/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 78.9520 - val_loss: 100.1588\n",
      "Epoch 571/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 78.7588 - val_loss: 100.0703\n",
      "Epoch 572/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 78.5708 - val_loss: 99.9027\n",
      "Epoch 573/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 78.3780 - val_loss: 99.6612\n",
      "Epoch 574/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 78.1870 - val_loss: 99.4317\n",
      "Epoch 575/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 77.9973 - val_loss: 99.1771\n",
      "Epoch 576/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 77.8269 - val_loss: 98.8968\n",
      "Epoch 577/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 77.6210 - val_loss: 98.6682\n",
      "Epoch 578/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 77.4486 - val_loss: 98.4093\n",
      "Epoch 579/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 77.2480 - val_loss: 98.1957\n",
      "Epoch 580/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 77.0751 - val_loss: 97.9874\n",
      "Epoch 581/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 76.8749 - val_loss: 97.7415\n",
      "Epoch 582/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 76.6970 - val_loss: 97.4507\n",
      "Epoch 583/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 76.5067 - val_loss: 97.1825\n",
      "Epoch 584/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 76.3369 - val_loss: 96.8624\n",
      "Epoch 585/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 76.1247 - val_loss: 96.6021\n",
      "Epoch 586/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 75.9406 - val_loss: 96.3574\n",
      "Epoch 587/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 75.7418 - val_loss: 96.0625\n",
      "Epoch 588/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 75.5581 - val_loss: 95.7946\n",
      "Epoch 589/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 75.3840 - val_loss: 95.5304\n",
      "Epoch 590/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 75.1699 - val_loss: 95.2834\n",
      "Epoch 591/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 74.9646 - val_loss: 95.0348\n",
      "Epoch 592/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 74.7804 - val_loss: 94.7797\n",
      "Epoch 593/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 74.5864 - val_loss: 94.5293\n",
      "Epoch 594/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 74.4333 - val_loss: 94.2783\n",
      "Epoch 595/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 74.2019 - val_loss: 94.0505\n",
      "Epoch 596/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 74.0159 - val_loss: 93.8180\n",
      "Epoch 597/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 73.8295 - val_loss: 93.5778\n",
      "Epoch 598/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 73.6394 - val_loss: 93.3337\n",
      "Epoch 599/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 73.4546 - val_loss: 93.0772\n",
      "Epoch 600/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 73.2589 - val_loss: 92.8281\n",
      "Epoch 601/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 73.0821 - val_loss: 92.5842\n",
      "Epoch 602/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 72.8849 - val_loss: 92.3515\n",
      "Epoch 603/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 72.7351 - val_loss: 92.1203\n",
      "Epoch 604/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 72.5177 - val_loss: 91.9142\n",
      "Epoch 605/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 72.3400 - val_loss: 91.7098\n",
      "Epoch 606/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 72.1616 - val_loss: 91.5036\n",
      "Epoch 607/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 71.9787 - val_loss: 91.2923\n",
      "Epoch 608/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 71.7858 - val_loss: 91.0886\n",
      "Epoch 609/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 71.6154 - val_loss: 90.8759\n",
      "Epoch 610/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 71.4322 - val_loss: 90.6533\n",
      "Epoch 611/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 71.2764 - val_loss: 90.4470\n",
      "Epoch 612/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 71.0595 - val_loss: 90.2110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 613/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 70.8864 - val_loss: 89.9754\n",
      "Epoch 614/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 70.6986 - val_loss: 89.7528\n",
      "Epoch 615/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 70.5196 - val_loss: 89.5361\n",
      "Epoch 616/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 70.3315 - val_loss: 89.3164\n",
      "Epoch 617/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 70.1537 - val_loss: 89.0920\n",
      "Epoch 618/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 69.9737 - val_loss: 88.8750\n",
      "Epoch 619/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 69.8098 - val_loss: 88.6769\n",
      "Epoch 620/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 69.5954 - val_loss: 88.4554\n",
      "Epoch 621/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 69.4103 - val_loss: 88.2415\n",
      "Epoch 622/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 69.2265 - val_loss: 88.0338\n",
      "Epoch 623/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 69.0279 - val_loss: 87.8451\n",
      "Epoch 624/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 68.8309 - val_loss: 87.6500\n",
      "Epoch 625/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 68.6884 - val_loss: 87.4526\n",
      "Epoch 626/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 68.5053 - val_loss: 87.2357\n",
      "Epoch 627/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 68.2890 - val_loss: 86.9928\n",
      "Epoch 628/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 68.1224 - val_loss: 86.7390\n",
      "Epoch 629/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 67.9100 - val_loss: 86.4991\n",
      "Epoch 630/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 67.7318 - val_loss: 86.2624\n",
      "Epoch 631/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 67.5645 - val_loss: 86.0293\n",
      "Epoch 632/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 67.3746 - val_loss: 85.7970\n",
      "Epoch 633/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 67.2315 - val_loss: 85.5623\n",
      "Epoch 634/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 67.0220 - val_loss: 85.3507\n",
      "Epoch 635/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 66.8469 - val_loss: 85.1439\n",
      "Epoch 636/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 66.6574 - val_loss: 84.9183\n",
      "Epoch 637/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 66.4799 - val_loss: 84.6881\n",
      "Epoch 638/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 66.2950 - val_loss: 84.4599\n",
      "Epoch 639/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 66.1188 - val_loss: 84.2316\n",
      "Epoch 640/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 65.9486 - val_loss: 83.9953\n",
      "Epoch 641/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 65.7513 - val_loss: 83.7656\n",
      "Epoch 642/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 65.5890 - val_loss: 83.5296\n",
      "Epoch 643/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 65.3908 - val_loss: 83.3009\n",
      "Epoch 644/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 65.2275 - val_loss: 83.0680\n",
      "Epoch 645/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 65.0461 - val_loss: 82.8303\n",
      "Epoch 646/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 64.8811 - val_loss: 82.6085\n",
      "Epoch 647/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 64.7053 - val_loss: 82.3788\n",
      "Epoch 648/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 64.5583 - val_loss: 82.1642\n",
      "Epoch 649/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 64.3908 - val_loss: 81.9550\n",
      "Epoch 650/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 64.2400 - val_loss: 81.7631\n",
      "Epoch 651/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 64.0545 - val_loss: 81.5694\n",
      "Epoch 652/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 63.8936 - val_loss: 81.3866\n",
      "Epoch 653/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 63.7105 - val_loss: 81.2018\n",
      "Epoch 654/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 63.5329 - val_loss: 81.0223\n",
      "Epoch 655/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 63.3589 - val_loss: 80.8483\n",
      "Epoch 656/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 63.1796 - val_loss: 80.6752\n",
      "Epoch 657/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 63.0288 - val_loss: 80.5118\n",
      "Epoch 658/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 62.9098 - val_loss: 80.3450\n",
      "Epoch 659/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 62.7136 - val_loss: 80.1582\n",
      "Epoch 660/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 62.6166 - val_loss: 79.9692\n",
      "Epoch 661/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 62.4160 - val_loss: 79.7515\n",
      "Epoch 662/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 62.2632 - val_loss: 79.5424\n",
      "Epoch 663/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 62.1163 - val_loss: 79.3414\n",
      "Epoch 664/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 61.9579 - val_loss: 79.1386\n",
      "Epoch 665/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 61.8059 - val_loss: 78.9224\n",
      "Epoch 666/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 61.7006 - val_loss: 78.6886\n",
      "Epoch 667/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 61.5101 - val_loss: 78.4745\n",
      "Epoch 668/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 61.3559 - val_loss: 78.2660\n",
      "Epoch 669/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 61.2151 - val_loss: 78.0612\n",
      "Epoch 670/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 61.1345 - val_loss: 77.8441\n",
      "Epoch 671/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 60.9125 - val_loss: 77.6584\n",
      "Epoch 672/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 60.7833 - val_loss: 77.4682\n",
      "Epoch 673/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 60.6142 - val_loss: 77.2917\n",
      "Epoch 674/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 60.4768 - val_loss: 77.1091\n",
      "Epoch 675/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 60.3671 - val_loss: 76.9224\n",
      "Epoch 676/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 60.2512 - val_loss: 76.7319\n",
      "Epoch 677/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 60.1363 - val_loss: 76.5317\n",
      "Epoch 678/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 59.9505 - val_loss: 76.3097\n",
      "Epoch 679/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 59.8472 - val_loss: 76.0767\n",
      "Epoch 680/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 59.7210 - val_loss: 75.8696\n",
      "Epoch 681/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 59.5539 - val_loss: 75.6941\n",
      "Epoch 682/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 59.4212 - val_loss: 75.5159\n",
      "Epoch 683/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 59.2926 - val_loss: 75.3441\n",
      "Epoch 684/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 59.1633 - val_loss: 75.1724\n",
      "Epoch 685/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 59.0592 - val_loss: 74.9982\n",
      "Epoch 686/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 58.9626 - val_loss: 74.8198\n",
      "Epoch 687/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 58.8933 - val_loss: 74.6399\n",
      "Epoch 688/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 58.7742 - val_loss: 74.4820\n",
      "Epoch 689/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 58.6956 - val_loss: 74.3211\n",
      "Epoch 690/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 58.6120 - val_loss: 74.1607\n",
      "Epoch 691/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 81us/step - loss: 58.5159 - val_loss: 74.0062\n",
      "Epoch 692/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 58.4484 - val_loss: 73.8495\n",
      "Epoch 693/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 58.3600 - val_loss: 73.6982\n",
      "Epoch 694/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 58.2791 - val_loss: 73.5375\n",
      "Epoch 695/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 58.2030 - val_loss: 73.3794\n",
      "Epoch 696/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 58.1413 - val_loss: 73.2226\n",
      "Epoch 697/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 58.0840 - val_loss: 73.0676\n",
      "Epoch 698/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 58.0134 - val_loss: 72.9248\n",
      "Epoch 699/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 57.9341 - val_loss: 72.7898\n",
      "Epoch 700/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 57.8581 - val_loss: 72.6555\n",
      "Epoch 701/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 57.7820 - val_loss: 72.5197\n",
      "Epoch 702/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 57.7178 - val_loss: 72.3882\n",
      "Epoch 703/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 57.6368 - val_loss: 72.2618\n",
      "Epoch 704/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 57.5542 - val_loss: 72.1362\n",
      "Epoch 705/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 57.5007 - val_loss: 72.0034\n",
      "Epoch 706/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 57.4278 - val_loss: 71.8817\n",
      "Epoch 707/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 57.3814 - val_loss: 71.7747\n",
      "Epoch 708/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 57.2947 - val_loss: 71.6508\n",
      "Epoch 709/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 57.2289 - val_loss: 71.5303\n",
      "Epoch 710/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 57.1811 - val_loss: 71.4148\n",
      "Epoch 711/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 57.1171 - val_loss: 71.2849\n",
      "Epoch 712/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 57.0282 - val_loss: 71.1663\n",
      "Epoch 713/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.9685 - val_loss: 71.0385\n",
      "Epoch 714/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.9135 - val_loss: 70.9019\n",
      "Epoch 715/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 56.8356 - val_loss: 70.7523\n",
      "Epoch 716/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.7598 - val_loss: 70.6060\n",
      "Epoch 717/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 56.6935 - val_loss: 70.4632\n",
      "Epoch 718/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 56.6188 - val_loss: 70.3356\n",
      "Epoch 719/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.5690 - val_loss: 70.2067\n",
      "Epoch 720/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.5087 - val_loss: 70.0856\n",
      "Epoch 721/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 56.4485 - val_loss: 69.9712\n",
      "Epoch 722/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.3913 - val_loss: 69.8530\n",
      "Epoch 723/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 56.3313 - val_loss: 69.7389\n",
      "Epoch 724/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 56.2745 - val_loss: 69.6311\n",
      "Epoch 725/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.2214 - val_loss: 69.5264\n",
      "Epoch 726/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.1529 - val_loss: 69.4143\n",
      "Epoch 727/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 56.1040 - val_loss: 69.3018\n",
      "Epoch 728/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 56.0517 - val_loss: 69.1844\n",
      "Epoch 729/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.9799 - val_loss: 69.0634\n",
      "Epoch 730/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.9447 - val_loss: 68.9441\n",
      "Epoch 731/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 55.8812 - val_loss: 68.8156\n",
      "Epoch 732/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.8447 - val_loss: 68.6951\n",
      "Epoch 733/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.7641 - val_loss: 68.5706\n",
      "Epoch 734/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.7419 - val_loss: 68.4542\n",
      "Epoch 735/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.7326 - val_loss: 68.3413\n",
      "Epoch 736/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 55.6804 - val_loss: 68.2519\n",
      "Epoch 737/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 55.6275 - val_loss: 68.1703\n",
      "Epoch 738/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.5871 - val_loss: 68.0963\n",
      "Epoch 739/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.5537 - val_loss: 68.0184\n",
      "Epoch 740/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.5114 - val_loss: 67.9261\n",
      "Epoch 741/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 55.4703 - val_loss: 67.8382\n",
      "Epoch 742/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.4453 - val_loss: 67.7518\n",
      "Epoch 743/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.4034 - val_loss: 67.6524\n",
      "Epoch 744/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 55.3921 - val_loss: 67.5479\n",
      "Epoch 745/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.3619 - val_loss: 67.4551\n",
      "Epoch 746/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.3336 - val_loss: 67.3680\n",
      "Epoch 747/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 55.3042 - val_loss: 67.2845\n",
      "Epoch 748/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.2794 - val_loss: 67.2032\n",
      "Epoch 749/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 55.2732 - val_loss: 67.1210\n",
      "Epoch 750/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.2281 - val_loss: 67.0285\n",
      "Epoch 751/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 55.2390 - val_loss: 66.9459\n",
      "Epoch 752/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.2157 - val_loss: 66.8502\n",
      "Epoch 753/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 55.1852 - val_loss: 66.7743\n",
      "Epoch 754/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.1565 - val_loss: 66.7101\n",
      "Epoch 755/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.1184 - val_loss: 66.6474\n",
      "Epoch 756/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.0769 - val_loss: 66.5877\n",
      "Epoch 757/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 55.0816 - val_loss: 66.5363\n",
      "Epoch 758/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 55.0149 - val_loss: 66.4686\n",
      "Epoch 759/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 54.9846 - val_loss: 66.3888\n",
      "Epoch 760/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.9570 - val_loss: 66.3117\n",
      "Epoch 761/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.9322 - val_loss: 66.2306\n",
      "Epoch 762/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.9055 - val_loss: 66.1519\n",
      "Epoch 763/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.8726 - val_loss: 66.0734\n",
      "Epoch 764/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.8483 - val_loss: 65.9922\n",
      "Epoch 765/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.8239 - val_loss: 65.9116\n",
      "Epoch 766/1500\n",
      "99/99 [==============================] - 0s 313us/step - loss: 54.8234 - val_loss: 65.8415\n",
      "Epoch 767/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.7945 - val_loss: 65.7905\n",
      "Epoch 768/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 54.7597 - val_loss: 65.7530\n",
      "Epoch 769/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.7328 - val_loss: 65.7195\n",
      "Epoch 770/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.7034 - val_loss: 65.7572\n",
      "Epoch 771/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.7337 - val_loss: 65.8651\n",
      "Epoch 772/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.6699 - val_loss: 65.8863\n",
      "Epoch 773/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.6548 - val_loss: 65.8770\n",
      "Epoch 774/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 54.6182 - val_loss: 65.9060\n",
      "Epoch 775/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 54.6473 - val_loss: 65.9229\n",
      "Epoch 776/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 54.5908 - val_loss: 65.8382\n",
      "Epoch 777/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 54.5876 - val_loss: 65.7245\n",
      "Epoch 778/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.5402 - val_loss: 65.6538\n",
      "Epoch 779/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.5071 - val_loss: 65.5322\n",
      "Epoch 780/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 54.4641 - val_loss: 65.3520\n",
      "Epoch 781/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 54.4597 - val_loss: 65.1618\n",
      "Epoch 782/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 54.4481 - val_loss: 65.0000\n",
      "Epoch 783/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.4678 - val_loss: 64.8355\n",
      "Epoch 784/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 54.4579 - val_loss: 64.7330\n",
      "Epoch 785/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 54.4414 - val_loss: 64.6823\n",
      "Epoch 786/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 54.4233 - val_loss: 64.6457\n",
      "Epoch 787/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.4036 - val_loss: 64.6439\n",
      "Epoch 788/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 54.3874 - val_loss: 64.6822\n",
      "Epoch 789/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.3640 - val_loss: 64.7128\n",
      "Epoch 790/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.3320 - val_loss: 64.7409\n",
      "Epoch 791/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 54.2991 - val_loss: 64.8110\n",
      "Epoch 792/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 54.3338 - val_loss: 64.9108\n",
      "Epoch 793/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.2834 - val_loss: 64.9236\n",
      "Epoch 794/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.2698 - val_loss: 64.8683\n",
      "Epoch 795/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.2279 - val_loss: 64.8828\n",
      "Epoch 796/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.2139 - val_loss: 64.9063\n",
      "Epoch 797/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.2054 - val_loss: 64.9549\n",
      "Epoch 798/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 54.1833 - val_loss: 64.9843\n",
      "Epoch 799/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.1653 - val_loss: 65.0261\n",
      "Epoch 800/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.1518 - val_loss: 65.0719\n",
      "Epoch 801/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 54.1502 - val_loss: 65.1281\n",
      "Epoch 802/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.1491 - val_loss: 65.1126\n",
      "Epoch 803/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 54.1455 - val_loss: 65.0412\n",
      "Epoch 804/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.1137 - val_loss: 64.9806\n",
      "Epoch 805/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.1126 - val_loss: 64.8863\n",
      "Epoch 806/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 54.0801 - val_loss: 64.8270\n",
      "Epoch 807/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.0751 - val_loss: 64.7511\n",
      "Epoch 808/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.0463 - val_loss: 64.7129\n",
      "Epoch 809/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 54.0371 - val_loss: 64.6520\n",
      "Epoch 810/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 54.0184 - val_loss: 64.6188\n",
      "Epoch 811/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 54.0027 - val_loss: 64.6047\n",
      "Epoch 812/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.9970 - val_loss: 64.5900\n",
      "Epoch 813/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.9774 - val_loss: 64.6042\n",
      "Epoch 814/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.9688 - val_loss: 64.5986\n",
      "Epoch 815/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.9944 - val_loss: 64.5800\n",
      "Epoch 816/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.9427 - val_loss: 64.4583\n",
      "Epoch 817/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.9483 - val_loss: 64.3252\n",
      "Epoch 818/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.9083 - val_loss: 64.2624\n",
      "Epoch 819/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.9055 - val_loss: 64.1885\n",
      "Epoch 820/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.8967 - val_loss: 64.1728\n",
      "Epoch 821/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.8855 - val_loss: 64.2083\n",
      "Epoch 822/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.8564 - val_loss: 64.3132\n",
      "Epoch 823/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.8373 - val_loss: 64.4352\n",
      "Epoch 824/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.8530 - val_loss: 64.5331\n",
      "Epoch 825/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.8853 - val_loss: 64.5786\n",
      "Epoch 826/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.8963 - val_loss: 64.5699\n",
      "Epoch 827/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.8944 - val_loss: 64.4951\n",
      "Epoch 828/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.8563 - val_loss: 64.4036\n",
      "Epoch 829/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.8126 - val_loss: 64.2530\n",
      "Epoch 830/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.8387 - val_loss: 64.0590\n",
      "Epoch 831/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.8042 - val_loss: 63.9624\n",
      "Epoch 832/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.7704 - val_loss: 63.9423\n",
      "Epoch 833/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.7623 - val_loss: 63.9057\n",
      "Epoch 834/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.7667 - val_loss: 63.8384\n",
      "Epoch 835/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.7425 - val_loss: 63.8314\n",
      "Epoch 836/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.7383 - val_loss: 63.8620\n",
      "Epoch 837/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.7185 - val_loss: 63.8667\n",
      "Epoch 838/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.7107 - val_loss: 63.8788\n",
      "Epoch 839/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.6992 - val_loss: 63.8824\n",
      "Epoch 840/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.6925 - val_loss: 63.8781\n",
      "Epoch 841/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.6812 - val_loss: 63.8506\n",
      "Epoch 842/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.6730 - val_loss: 63.8114\n",
      "Epoch 843/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.6630 - val_loss: 63.7469\n",
      "Epoch 844/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.6559 - val_loss: 63.7065\n",
      "Epoch 845/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.6547 - val_loss: 63.6292\n",
      "Epoch 846/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.6376 - val_loss: 63.5928\n",
      "Epoch 847/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 71us/step - loss: 53.6312 - val_loss: 63.5693\n",
      "Epoch 848/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.6232 - val_loss: 63.5458\n",
      "Epoch 849/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.6180 - val_loss: 63.5055\n",
      "Epoch 850/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.6103 - val_loss: 63.5047\n",
      "Epoch 851/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.6038 - val_loss: 63.4970\n",
      "Epoch 852/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.6180 - val_loss: 63.4840\n",
      "Epoch 853/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.5715 - val_loss: 63.5960\n",
      "Epoch 854/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.5533 - val_loss: 63.7084\n",
      "Epoch 855/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.6138 - val_loss: 63.8399\n",
      "Epoch 856/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.5571 - val_loss: 63.8548\n",
      "Epoch 857/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.5720 - val_loss: 63.8315\n",
      "Epoch 858/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.5724 - val_loss: 63.7169\n",
      "Epoch 859/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.5291 - val_loss: 63.6586\n",
      "Epoch 860/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.5113 - val_loss: 63.5681\n",
      "Epoch 861/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.5460 - val_loss: 63.4506\n",
      "Epoch 862/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.5083 - val_loss: 63.4462\n",
      "Epoch 863/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.4994 - val_loss: 63.4565\n",
      "Epoch 864/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.4907 - val_loss: 63.4896\n",
      "Epoch 865/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.5018 - val_loss: 63.5260\n",
      "Epoch 866/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.4778 - val_loss: 63.5065\n",
      "Epoch 867/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.4638 - val_loss: 63.5317\n",
      "Epoch 868/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.4517 - val_loss: 63.5421\n",
      "Epoch 869/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.4422 - val_loss: 63.5710\n",
      "Epoch 870/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.4504 - val_loss: 63.5842\n",
      "Epoch 871/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.4371 - val_loss: 63.5250\n",
      "Epoch 872/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.4188 - val_loss: 63.4385\n",
      "Epoch 873/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.4538 - val_loss: 63.3445\n",
      "Epoch 874/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.4260 - val_loss: 63.3586\n",
      "Epoch 875/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.4089 - val_loss: 63.3744\n",
      "Epoch 876/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.3966 - val_loss: 63.4259\n",
      "Epoch 877/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3793 - val_loss: 63.5207\n",
      "Epoch 878/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.4265 - val_loss: 63.6117\n",
      "Epoch 879/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.4087 - val_loss: 63.5789\n",
      "Epoch 880/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3929 - val_loss: 63.4860\n",
      "Epoch 881/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.3810 - val_loss: 63.3928\n",
      "Epoch 882/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.3680 - val_loss: 63.3330\n",
      "Epoch 883/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.3637 - val_loss: 63.3178\n",
      "Epoch 884/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3709 - val_loss: 63.3539\n",
      "Epoch 885/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3551 - val_loss: 63.3369\n",
      "Epoch 886/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.3449 - val_loss: 63.3636\n",
      "Epoch 887/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.3382 - val_loss: 63.4314\n",
      "Epoch 888/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3307 - val_loss: 63.4679\n",
      "Epoch 889/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3345 - val_loss: 63.4996\n",
      "Epoch 890/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.3262 - val_loss: 63.4849\n",
      "Epoch 891/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3273 - val_loss: 63.4632\n",
      "Epoch 892/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.3072 - val_loss: 63.3804\n",
      "Epoch 893/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3171 - val_loss: 63.2810\n",
      "Epoch 894/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.3115 - val_loss: 63.2310\n",
      "Epoch 895/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.3136 - val_loss: 63.2288\n",
      "Epoch 896/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2997 - val_loss: 63.2867\n",
      "Epoch 897/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.2878 - val_loss: 63.3601\n",
      "Epoch 898/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.2985 - val_loss: 63.4299\n",
      "Epoch 899/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.2912 - val_loss: 63.4412\n",
      "Epoch 900/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.2787 - val_loss: 63.3873\n",
      "Epoch 901/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.2667 - val_loss: 63.3739\n",
      "Epoch 902/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 53.2598 - val_loss: 63.3934\n",
      "Epoch 903/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2589 - val_loss: 63.4196\n",
      "Epoch 904/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.2655 - val_loss: 63.4067\n",
      "Epoch 905/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2457 - val_loss: 63.4414\n",
      "Epoch 906/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2649 - val_loss: 63.4706\n",
      "Epoch 907/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 53.2326 - val_loss: 63.5645\n",
      "Epoch 908/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2380 - val_loss: 63.6546\n",
      "Epoch 909/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2534 - val_loss: 63.7011\n",
      "Epoch 910/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2895 - val_loss: 63.7479\n",
      "Epoch 911/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.2617 - val_loss: 63.6624\n",
      "Epoch 912/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.2549 - val_loss: 63.5583\n",
      "Epoch 913/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2276 - val_loss: 63.4948\n",
      "Epoch 914/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.2202 - val_loss: 63.4665\n",
      "Epoch 915/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 53.2092 - val_loss: 63.4790\n",
      "Epoch 916/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.2097 - val_loss: 63.5421\n",
      "Epoch 917/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1950 - val_loss: 63.5424\n",
      "Epoch 918/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 53.1867 - val_loss: 63.5091\n",
      "Epoch 919/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1802 - val_loss: 63.4454\n",
      "Epoch 920/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1945 - val_loss: 63.3840\n",
      "Epoch 921/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1642 - val_loss: 63.4097\n",
      "Epoch 922/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 53.1586 - val_loss: 63.4213\n",
      "Epoch 923/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.1575 - val_loss: 63.4429\n",
      "Epoch 924/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.1519 - val_loss: 63.4343\n",
      "Epoch 925/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 71us/step - loss: 53.1583 - val_loss: 63.4045\n",
      "Epoch 926/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.1449 - val_loss: 63.4171\n",
      "Epoch 927/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1606 - val_loss: 63.3918\n",
      "Epoch 928/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.1424 - val_loss: 63.2754\n",
      "Epoch 929/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.1674 - val_loss: 63.2225\n",
      "Epoch 930/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1526 - val_loss: 63.2766\n",
      "Epoch 931/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1247 - val_loss: 63.3026\n",
      "Epoch 932/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.1352 - val_loss: 63.3428\n",
      "Epoch 933/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.1114 - val_loss: 63.3230\n",
      "Epoch 934/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.1072 - val_loss: 63.3388\n",
      "Epoch 935/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 53.1290 - val_loss: 63.3354\n",
      "Epoch 936/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.0986 - val_loss: 63.4260\n",
      "Epoch 937/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0927 - val_loss: 63.4809\n",
      "Epoch 938/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.0996 - val_loss: 63.5022\n",
      "Epoch 939/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1008 - val_loss: 63.5208\n",
      "Epoch 940/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.0970 - val_loss: 63.5490\n",
      "Epoch 941/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.1002 - val_loss: 63.5311\n",
      "Epoch 942/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0915 - val_loss: 63.5141\n",
      "Epoch 943/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.0767 - val_loss: 63.4363\n",
      "Epoch 944/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.0678 - val_loss: 63.3093\n",
      "Epoch 945/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.1015 - val_loss: 63.2159\n",
      "Epoch 946/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0900 - val_loss: 63.2298\n",
      "Epoch 947/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.0575 - val_loss: 63.3462\n",
      "Epoch 948/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 53.0345 - val_loss: 63.4423\n",
      "Epoch 949/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0307 - val_loss: 63.5460\n",
      "Epoch 950/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0754 - val_loss: 63.6375\n",
      "Epoch 951/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.0672 - val_loss: 63.6443\n",
      "Epoch 952/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.0659 - val_loss: 63.6293\n",
      "Epoch 953/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 53.0564 - val_loss: 63.5583\n",
      "Epoch 954/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 53.0412 - val_loss: 63.4678\n",
      "Epoch 955/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.0010 - val_loss: 63.2962\n",
      "Epoch 956/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.0112 - val_loss: 63.1029\n",
      "Epoch 957/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0119 - val_loss: 62.9822\n",
      "Epoch 958/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0423 - val_loss: 62.8833\n",
      "Epoch 959/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.0420 - val_loss: 62.8495\n",
      "Epoch 960/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 53.0420 - val_loss: 62.8236\n",
      "Epoch 961/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.0431 - val_loss: 62.8361\n",
      "Epoch 962/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0318 - val_loss: 62.8395\n",
      "Epoch 963/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0493 - val_loss: 62.8698\n",
      "Epoch 964/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.0146 - val_loss: 62.8527\n",
      "Epoch 965/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0623 - val_loss: 62.8540\n",
      "Epoch 966/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 53.0043 - val_loss: 62.7654\n",
      "Epoch 967/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 53.0062 - val_loss: 62.7243\n",
      "Epoch 968/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 53.0205 - val_loss: 62.7209\n",
      "Epoch 969/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.9946 - val_loss: 62.6787\n",
      "Epoch 970/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.9943 - val_loss: 62.6525\n",
      "Epoch 971/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.9968 - val_loss: 62.6410\n",
      "Epoch 972/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.9788 - val_loss: 62.7043\n",
      "Epoch 973/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.9673 - val_loss: 62.8072\n",
      "Epoch 974/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.9533 - val_loss: 62.9032\n",
      "Epoch 975/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.9348 - val_loss: 62.9633\n",
      "Epoch 976/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.9294 - val_loss: 62.9543\n",
      "Epoch 977/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.9006 - val_loss: 63.0128\n",
      "Epoch 978/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.9091 - val_loss: 63.0689\n",
      "Epoch 979/1500\n",
      "99/99 [==============================] - 0s 121us/step - loss: 52.8987 - val_loss: 63.1604\n",
      "Epoch 980/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.9640 - val_loss: 63.2510\n",
      "Epoch 981/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.9377 - val_loss: 63.2025\n",
      "Epoch 982/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.9093 - val_loss: 63.0819\n",
      "Epoch 983/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.8708 - val_loss: 62.9019\n",
      "Epoch 984/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.8966 - val_loss: 62.7322\n",
      "Epoch 985/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.8946 - val_loss: 62.6533\n",
      "Epoch 986/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.9000 - val_loss: 62.6297\n",
      "Epoch 987/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.8945 - val_loss: 62.6659\n",
      "Epoch 988/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.9180 - val_loss: 62.7803\n",
      "Epoch 989/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.8584 - val_loss: 62.8324\n",
      "Epoch 990/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.8423 - val_loss: 62.8686\n",
      "Epoch 991/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.8435 - val_loss: 62.8947\n",
      "Epoch 992/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.8359 - val_loss: 62.8813\n",
      "Epoch 993/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.8365 - val_loss: 62.8701\n",
      "Epoch 994/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.8266 - val_loss: 62.8069\n",
      "Epoch 995/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.8399 - val_loss: 62.7675\n",
      "Epoch 996/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.8379 - val_loss: 62.8068\n",
      "Epoch 997/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.8092 - val_loss: 62.7791\n",
      "Epoch 998/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.8055 - val_loss: 62.7546\n",
      "Epoch 999/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.8084 - val_loss: 62.7192\n",
      "Epoch 1000/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.7975 - val_loss: 62.6452\n",
      "Epoch 1001/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.7970 - val_loss: 62.5496\n",
      "Epoch 1002/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.8180 - val_loss: 62.4798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1003/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.8134 - val_loss: 62.4755\n",
      "Epoch 1004/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.8185 - val_loss: 62.4882\n",
      "Epoch 1005/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.7993 - val_loss: 62.4732\n",
      "Epoch 1006/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.7962 - val_loss: 62.4866\n",
      "Epoch 1007/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.7941 - val_loss: 62.5368\n",
      "Epoch 1008/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.7695 - val_loss: 62.5697\n",
      "Epoch 1009/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.7867 - val_loss: 62.6275\n",
      "Epoch 1010/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.7525 - val_loss: 62.6037\n",
      "Epoch 1011/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.7491 - val_loss: 62.5851\n",
      "Epoch 1012/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.7428 - val_loss: 62.5963\n",
      "Epoch 1013/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.7372 - val_loss: 62.6083\n",
      "Epoch 1014/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.7597 - val_loss: 62.6161\n",
      "Epoch 1015/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.7332 - val_loss: 62.5498\n",
      "Epoch 1016/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.7236 - val_loss: 62.5206\n",
      "Epoch 1017/1500\n",
      "99/99 [==============================] - 0s 121us/step - loss: 52.7204 - val_loss: 62.5062\n",
      "Epoch 1018/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.7153 - val_loss: 62.4914\n",
      "Epoch 1019/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.7092 - val_loss: 62.4740\n",
      "Epoch 1020/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.7074 - val_loss: 62.4558\n",
      "Epoch 1021/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.7000 - val_loss: 62.4180\n",
      "Epoch 1022/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.7125 - val_loss: 62.3826\n",
      "Epoch 1023/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.6967 - val_loss: 62.4312\n",
      "Epoch 1024/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.6841 - val_loss: 62.4566\n",
      "Epoch 1025/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.6839 - val_loss: 62.4924\n",
      "Epoch 1026/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6859 - val_loss: 62.4810\n",
      "Epoch 1027/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6901 - val_loss: 62.4554\n",
      "Epoch 1028/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.6845 - val_loss: 62.3423\n",
      "Epoch 1029/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.6885 - val_loss: 62.2790\n",
      "Epoch 1030/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.6717 - val_loss: 62.3085\n",
      "Epoch 1031/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.6548 - val_loss: 62.4087\n",
      "Epoch 1032/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.6588 - val_loss: 62.5549\n",
      "Epoch 1033/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.6888 - val_loss: 62.6113\n",
      "Epoch 1034/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6932 - val_loss: 62.5777\n",
      "Epoch 1035/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6940 - val_loss: 62.4775\n",
      "Epoch 1036/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6627 - val_loss: 62.3766\n",
      "Epoch 1037/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.6624 - val_loss: 62.2835\n",
      "Epoch 1038/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6368 - val_loss: 62.2799\n",
      "Epoch 1039/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6262 - val_loss: 62.2193\n",
      "Epoch 1040/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.6296 - val_loss: 62.1136\n",
      "Epoch 1041/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.6836 - val_loss: 62.0680\n",
      "Epoch 1042/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.6419 - val_loss: 62.1626\n",
      "Epoch 1043/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6021 - val_loss: 62.2369\n",
      "Epoch 1044/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.6048 - val_loss: 62.3044\n",
      "Epoch 1045/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.6241 - val_loss: 62.3104\n",
      "Epoch 1046/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.6087 - val_loss: 62.2128\n",
      "Epoch 1047/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.5862 - val_loss: 62.1444\n",
      "Epoch 1048/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5788 - val_loss: 62.1244\n",
      "Epoch 1049/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5757 - val_loss: 62.1344\n",
      "Epoch 1050/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.5936 - val_loss: 62.1697\n",
      "Epoch 1051/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.5737 - val_loss: 62.1233\n",
      "Epoch 1052/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.5875 - val_loss: 62.0957\n",
      "Epoch 1053/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5564 - val_loss: 62.1322\n",
      "Epoch 1054/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.5859 - val_loss: 62.1903\n",
      "Epoch 1055/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5639 - val_loss: 62.1406\n",
      "Epoch 1056/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.5467 - val_loss: 62.0326\n",
      "Epoch 1057/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5353 - val_loss: 61.8983\n",
      "Epoch 1058/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5810 - val_loss: 61.7391\n",
      "Epoch 1059/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.5892 - val_loss: 61.7303\n",
      "Epoch 1060/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5905 - val_loss: 61.7861\n",
      "Epoch 1061/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.5620 - val_loss: 61.8683\n",
      "Epoch 1062/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.5359 - val_loss: 62.0066\n",
      "Epoch 1063/1500\n",
      "99/99 [==============================] - 0s 222us/step - loss: 52.5153 - val_loss: 62.2186\n",
      "Epoch 1064/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5474 - val_loss: 62.3559\n",
      "Epoch 1065/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.5813 - val_loss: 62.3640\n",
      "Epoch 1066/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5827 - val_loss: 62.2341\n",
      "Epoch 1067/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.5241 - val_loss: 62.1271\n",
      "Epoch 1068/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5175 - val_loss: 61.9935\n",
      "Epoch 1069/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.5018 - val_loss: 61.9206\n",
      "Epoch 1070/1500\n",
      "99/99 [==============================] - 0s 121us/step - loss: 52.5130 - val_loss: 61.8806\n",
      "Epoch 1071/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5141 - val_loss: 61.9130\n",
      "Epoch 1072/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4972 - val_loss: 61.9902\n",
      "Epoch 1073/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4818 - val_loss: 62.0865\n",
      "Epoch 1074/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4868 - val_loss: 62.1916\n",
      "Epoch 1075/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.5444 - val_loss: 62.2464\n",
      "Epoch 1076/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.5545 - val_loss: 62.1375\n",
      "Epoch 1077/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4830 - val_loss: 62.1173\n",
      "Epoch 1078/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4770 - val_loss: 62.0626\n",
      "Epoch 1079/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4725 - val_loss: 61.9685\n",
      "Epoch 1080/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4633 - val_loss: 61.9010\n",
      "Epoch 1081/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4675 - val_loss: 61.8408\n",
      "Epoch 1082/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4787 - val_loss: 61.7961\n",
      "Epoch 1083/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4835 - val_loss: 61.8091\n",
      "Epoch 1084/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4736 - val_loss: 61.8738\n",
      "Epoch 1085/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4645 - val_loss: 61.9637\n",
      "Epoch 1086/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4382 - val_loss: 62.0281\n",
      "Epoch 1087/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4506 - val_loss: 62.0952\n",
      "Epoch 1088/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4558 - val_loss: 62.0981\n",
      "Epoch 1089/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4472 - val_loss: 62.0262\n",
      "Epoch 1090/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.4317 - val_loss: 61.9802\n",
      "Epoch 1091/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4277 - val_loss: 61.9377\n",
      "Epoch 1092/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4249 - val_loss: 61.9253\n",
      "Epoch 1093/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4224 - val_loss: 61.9163\n",
      "Epoch 1094/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4318 - val_loss: 61.9325\n",
      "Epoch 1095/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4159 - val_loss: 61.9026\n",
      "Epoch 1096/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4226 - val_loss: 61.9006\n",
      "Epoch 1097/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4109 - val_loss: 61.9692\n",
      "Epoch 1098/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4013 - val_loss: 62.0806\n",
      "Epoch 1099/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4040 - val_loss: 62.2254\n",
      "Epoch 1100/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4308 - val_loss: 62.2287\n",
      "Epoch 1101/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4431 - val_loss: 62.0150\n",
      "Epoch 1102/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.3945 - val_loss: 61.9593\n",
      "Epoch 1103/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.3935 - val_loss: 61.8853\n",
      "Epoch 1104/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4123 - val_loss: 61.8658\n",
      "Epoch 1105/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3908 - val_loss: 61.7966\n",
      "Epoch 1106/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.4533 - val_loss: 61.7795\n",
      "Epoch 1107/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3976 - val_loss: 61.9401\n",
      "Epoch 1108/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3739 - val_loss: 62.2117\n",
      "Epoch 1109/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 49.45 - 0s 71us/step - loss: 52.4062 - val_loss: 62.3588\n",
      "Epoch 1110/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.4216 - val_loss: 62.2664\n",
      "Epoch 1111/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3909 - val_loss: 62.1408\n",
      "Epoch 1112/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3753 - val_loss: 61.9884\n",
      "Epoch 1113/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3769 - val_loss: 61.9211\n",
      "Epoch 1114/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3624 - val_loss: 61.9669\n",
      "Epoch 1115/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3577 - val_loss: 62.0367\n",
      "Epoch 1116/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.3696 - val_loss: 62.1638\n",
      "Epoch 1117/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4210 - val_loss: 62.1634\n",
      "Epoch 1118/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.4665 - val_loss: 62.3348\n",
      "Epoch 1119/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3845 - val_loss: 62.1312\n",
      "Epoch 1120/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3482 - val_loss: 62.0356\n",
      "Epoch 1121/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3438 - val_loss: 61.8820\n",
      "Epoch 1122/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3611 - val_loss: 61.8472\n",
      "Epoch 1123/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.3560 - val_loss: 61.8977\n",
      "Epoch 1124/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.3427 - val_loss: 61.9792\n",
      "Epoch 1125/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3481 - val_loss: 62.0424\n",
      "Epoch 1126/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.3329 - val_loss: 62.2615\n",
      "Epoch 1127/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.3680 - val_loss: 62.3718\n",
      "Epoch 1128/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.3456 - val_loss: 62.2569\n",
      "Epoch 1129/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3270 - val_loss: 62.1940\n",
      "Epoch 1130/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.3619 - val_loss: 62.1324\n",
      "Epoch 1131/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3312 - val_loss: 62.2314\n",
      "Epoch 1132/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3350 - val_loss: 62.2065\n",
      "Epoch 1133/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.3128 - val_loss: 62.2868\n",
      "Epoch 1134/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3256 - val_loss: 62.3107\n",
      "Epoch 1135/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3208 - val_loss: 62.2012\n",
      "Epoch 1136/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3128 - val_loss: 62.1441\n",
      "Epoch 1137/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3277 - val_loss: 62.1272\n",
      "Epoch 1138/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3139 - val_loss: 62.2375\n",
      "Epoch 1139/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3007 - val_loss: 62.2341\n",
      "Epoch 1140/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.2975 - val_loss: 62.1990\n",
      "Epoch 1141/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3167 - val_loss: 62.1642\n",
      "Epoch 1142/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3165 - val_loss: 62.2684\n",
      "Epoch 1143/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2911 - val_loss: 62.2154\n",
      "Epoch 1144/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3077 - val_loss: 62.1347\n",
      "Epoch 1145/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2954 - val_loss: 61.9074\n",
      "Epoch 1146/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.3001 - val_loss: 61.8553\n",
      "Epoch 1147/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3176 - val_loss: 61.8479\n",
      "Epoch 1148/1500\n",
      "99/99 [==============================] - 0s 323us/step - loss: 52.3119 - val_loss: 61.9615\n",
      "Epoch 1149/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3031 - val_loss: 62.0948\n",
      "Epoch 1150/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3084 - val_loss: 62.1274\n",
      "Epoch 1151/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.3139 - val_loss: 61.9850\n",
      "Epoch 1152/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.3033 - val_loss: 62.0767\n",
      "Epoch 1153/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.2795 - val_loss: 62.0942\n",
      "Epoch 1154/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2693 - val_loss: 62.0782\n",
      "Epoch 1155/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2689 - val_loss: 62.0830\n",
      "Epoch 1156/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2642 - val_loss: 62.0960\n",
      "Epoch 1157/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 61us/step - loss: 52.2697 - val_loss: 62.1348\n",
      "Epoch 1158/1500\n",
      "99/99 [==============================] - 0s 141us/step - loss: 52.2527 - val_loss: 62.2818\n",
      "Epoch 1159/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2740 - val_loss: 62.3860\n",
      "Epoch 1160/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2597 - val_loss: 62.2993\n",
      "Epoch 1161/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2530 - val_loss: 62.2376\n",
      "Epoch 1162/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2470 - val_loss: 62.1804\n",
      "Epoch 1163/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2532 - val_loss: 62.1365\n",
      "Epoch 1164/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.2457 - val_loss: 62.1817\n",
      "Epoch 1165/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2683 - val_loss: 62.3206\n",
      "Epoch 1166/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2682 - val_loss: 62.2957\n",
      "Epoch 1167/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.2378 - val_loss: 62.4081\n",
      "Epoch 1168/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2626 - val_loss: 62.4644\n",
      "Epoch 1169/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2587 - val_loss: 62.2981\n",
      "Epoch 1170/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2395 - val_loss: 62.1927\n",
      "Epoch 1171/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.2671 - val_loss: 62.1643\n",
      "Epoch 1172/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 51.74 - 0s 81us/step - loss: 52.2201 - val_loss: 62.3069\n",
      "Epoch 1173/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2416 - val_loss: 62.4513\n",
      "Epoch 1174/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2350 - val_loss: 62.4123\n",
      "Epoch 1175/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2242 - val_loss: 62.2439\n",
      "Epoch 1176/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.2649 - val_loss: 61.9375\n",
      "Epoch 1177/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2303 - val_loss: 61.8976\n",
      "Epoch 1178/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2371 - val_loss: 61.9460\n",
      "Epoch 1179/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.2276 - val_loss: 62.0549\n",
      "Epoch 1180/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2186 - val_loss: 62.2064\n",
      "Epoch 1181/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.2082 - val_loss: 62.3162\n",
      "Epoch 1182/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2000 - val_loss: 62.4617\n",
      "Epoch 1183/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2175 - val_loss: 62.5002\n",
      "Epoch 1184/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.2229 - val_loss: 62.4273\n",
      "Epoch 1185/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2029 - val_loss: 62.2082\n",
      "Epoch 1186/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.2146 - val_loss: 61.9246\n",
      "Epoch 1187/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.2138 - val_loss: 61.9079\n",
      "Epoch 1188/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2177 - val_loss: 62.0154\n",
      "Epoch 1189/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1892 - val_loss: 62.2742\n",
      "Epoch 1190/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1840 - val_loss: 62.4842\n",
      "Epoch 1191/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.2209 - val_loss: 62.5798\n",
      "Epoch 1192/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2157 - val_loss: 62.4810\n",
      "Epoch 1193/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2102 - val_loss: 62.2889\n",
      "Epoch 1194/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.1784 - val_loss: 62.1741\n",
      "Epoch 1195/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1844 - val_loss: 62.0600\n",
      "Epoch 1196/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2151 - val_loss: 62.0337\n",
      "Epoch 1197/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.1807 - val_loss: 62.2604\n",
      "Epoch 1198/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1824 - val_loss: 62.4870\n",
      "Epoch 1199/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1866 - val_loss: 62.5296\n",
      "Epoch 1200/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1928 - val_loss: 62.4168\n",
      "Epoch 1201/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.1761 - val_loss: 62.3274\n",
      "Epoch 1202/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1800 - val_loss: 62.3263\n",
      "Epoch 1203/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1806 - val_loss: 62.2188\n",
      "Epoch 1204/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1729 - val_loss: 62.2736\n",
      "Epoch 1205/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1879 - val_loss: 62.3148\n",
      "Epoch 1206/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.1578 - val_loss: 62.5024\n",
      "Epoch 1207/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1608 - val_loss: 62.5823\n",
      "Epoch 1208/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1706 - val_loss: 62.6104\n",
      "Epoch 1209/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1760 - val_loss: 62.5324\n",
      "Epoch 1210/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1552 - val_loss: 62.3027\n",
      "Epoch 1211/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.1556 - val_loss: 62.1497\n",
      "Epoch 1212/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1971 - val_loss: 62.0969\n",
      "Epoch 1213/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1621 - val_loss: 62.2673\n",
      "Epoch 1214/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1630 - val_loss: 62.5055\n",
      "Epoch 1215/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1915 - val_loss: 62.8619\n",
      "Epoch 1216/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1955 - val_loss: 62.9357\n",
      "Epoch 1217/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.1947 - val_loss: 63.0196\n",
      "Epoch 1218/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2252 - val_loss: 63.0435\n",
      "Epoch 1219/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2139 - val_loss: 63.0806\n",
      "Epoch 1220/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2239 - val_loss: 62.8518\n",
      "Epoch 1221/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1673 - val_loss: 62.6328\n",
      "Epoch 1222/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1282 - val_loss: 62.3952\n",
      "Epoch 1223/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1171 - val_loss: 62.0838\n",
      "Epoch 1224/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.1713 - val_loss: 61.8454\n",
      "Epoch 1225/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2341 - val_loss: 61.8376\n",
      "Epoch 1226/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.2346 - val_loss: 62.0512\n",
      "Epoch 1227/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.1848 - val_loss: 62.4966\n",
      "Epoch 1228/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1028 - val_loss: 62.7801\n",
      "Epoch 1229/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1323 - val_loss: 63.0510\n",
      "Epoch 1230/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1914 - val_loss: 63.2621\n",
      "Epoch 1231/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.2417 - val_loss: 63.2451\n",
      "Epoch 1232/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.2344 - val_loss: 63.0718\n",
      "Epoch 1233/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.2126 - val_loss: 62.8210\n",
      "Epoch 1234/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 0s 81us/step - loss: 52.1447 - val_loss: 62.6089\n",
      "Epoch 1235/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0911 - val_loss: 62.2761\n",
      "Epoch 1236/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1505 - val_loss: 61.9473\n",
      "Epoch 1237/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.1877 - val_loss: 61.9126\n",
      "Epoch 1238/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 52.2489 - val_loss: 62.1648\n",
      "Epoch 1239/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.1262 - val_loss: 62.2324\n",
      "Epoch 1240/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1258 - val_loss: 62.3199\n",
      "Epoch 1241/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0956 - val_loss: 62.5577\n",
      "Epoch 1242/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1567 - val_loss: 62.7411\n",
      "Epoch 1243/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1340 - val_loss: 62.6553\n",
      "Epoch 1244/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.1329 - val_loss: 62.4721\n",
      "Epoch 1245/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0985 - val_loss: 62.3899\n",
      "Epoch 1246/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0973 - val_loss: 62.3621\n",
      "Epoch 1247/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.1005 - val_loss: 62.3411\n",
      "Epoch 1248/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1017 - val_loss: 62.3985\n",
      "Epoch 1249/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0931 - val_loss: 62.4067\n",
      "Epoch 1250/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1189 - val_loss: 62.4412\n",
      "Epoch 1251/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0870 - val_loss: 62.6276\n",
      "Epoch 1252/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1115 - val_loss: 62.7062\n",
      "Epoch 1253/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.1390 - val_loss: 62.6633\n",
      "Epoch 1254/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1172 - val_loss: 62.6556\n",
      "Epoch 1255/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1020 - val_loss: 62.4837\n",
      "Epoch 1256/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0878 - val_loss: 62.2419\n",
      "Epoch 1257/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1062 - val_loss: 62.1296\n",
      "Epoch 1258/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1167 - val_loss: 61.9257\n",
      "Epoch 1259/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.1278 - val_loss: 62.0112\n",
      "Epoch 1260/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0913 - val_loss: 62.2523\n",
      "Epoch 1261/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0857 - val_loss: 62.5559\n",
      "Epoch 1262/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0798 - val_loss: 62.6970\n",
      "Epoch 1263/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1167 - val_loss: 62.7984\n",
      "Epoch 1264/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1135 - val_loss: 62.7039\n",
      "Epoch 1265/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0868 - val_loss: 62.4545\n",
      "Epoch 1266/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1081 - val_loss: 62.1482\n",
      "Epoch 1267/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0828 - val_loss: 62.0823\n",
      "Epoch 1268/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0871 - val_loss: 62.0571\n",
      "Epoch 1269/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0901 - val_loss: 62.1422\n",
      "Epoch 1270/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 52.71 - 0s 81us/step - loss: 52.0847 - val_loss: 62.2366\n",
      "Epoch 1271/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0700 - val_loss: 62.2774\n",
      "Epoch 1272/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.1102 - val_loss: 62.3228\n",
      "Epoch 1273/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0614 - val_loss: 62.1893\n",
      "Epoch 1274/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0630 - val_loss: 62.0873\n",
      "Epoch 1275/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0668 - val_loss: 62.0366\n",
      "Epoch 1276/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0933 - val_loss: 62.0268\n",
      "Epoch 1277/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0597 - val_loss: 62.2278\n",
      "Epoch 1278/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0755 - val_loss: 62.4314\n",
      "Epoch 1279/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0647 - val_loss: 62.4068\n",
      "Epoch 1280/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0689 - val_loss: 62.3239\n",
      "Epoch 1281/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0620 - val_loss: 62.3156\n",
      "Epoch 1282/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0462 - val_loss: 62.2123\n",
      "Epoch 1283/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0514 - val_loss: 62.1200\n",
      "Epoch 1284/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0533 - val_loss: 62.0844\n",
      "Epoch 1285/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0542 - val_loss: 62.1207\n",
      "Epoch 1286/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0500 - val_loss: 62.1289\n",
      "Epoch 1287/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0570 - val_loss: 62.1070\n",
      "Epoch 1288/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0428 - val_loss: 61.9949\n",
      "Epoch 1289/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0662 - val_loss: 61.9423\n",
      "Epoch 1290/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0477 - val_loss: 62.0509\n",
      "Epoch 1291/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0618 - val_loss: 62.1378\n",
      "Epoch 1292/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0697 - val_loss: 62.4025\n",
      "Epoch 1293/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.0762 - val_loss: 62.4343\n",
      "Epoch 1294/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0496 - val_loss: 62.2169\n",
      "Epoch 1295/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0501 - val_loss: 61.8721\n",
      "Epoch 1296/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0540 - val_loss: 61.7759\n",
      "Epoch 1297/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0675 - val_loss: 61.8771\n",
      "Epoch 1298/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0345 - val_loss: 62.1766\n",
      "Epoch 1299/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0235 - val_loss: 62.4075\n",
      "Epoch 1300/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0484 - val_loss: 62.5029\n",
      "Epoch 1301/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0603 - val_loss: 62.4071\n",
      "Epoch 1302/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0194 - val_loss: 62.0912\n",
      "Epoch 1303/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0214 - val_loss: 61.8090\n",
      "Epoch 1304/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0518 - val_loss: 61.7215\n",
      "Epoch 1305/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0803 - val_loss: 61.7589\n",
      "Epoch 1306/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 50.87 - 0s 71us/step - loss: 52.0564 - val_loss: 62.0173\n",
      "Epoch 1307/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0273 - val_loss: 62.2326\n",
      "Epoch 1308/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0107 - val_loss: 62.4017\n",
      "Epoch 1309/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0207 - val_loss: 62.5314\n",
      "Epoch 1310/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0387 - val_loss: 62.6244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1311/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.0581 - val_loss: 62.6211\n",
      "Epoch 1312/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0455 - val_loss: 62.4379\n",
      "Epoch 1313/1500\n",
      "99/99 [==============================] - 0s 121us/step - loss: 52.0134 - val_loss: 62.1474\n",
      "Epoch 1314/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0165 - val_loss: 61.9229\n",
      "Epoch 1315/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0285 - val_loss: 61.9001\n",
      "Epoch 1316/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0210 - val_loss: 62.0521\n",
      "Epoch 1317/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.1119 - val_loss: 62.2561\n",
      "Epoch 1318/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0104 - val_loss: 62.1779\n",
      "Epoch 1319/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 52.0089 - val_loss: 62.1802\n",
      "Epoch 1320/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0046 - val_loss: 62.1517\n",
      "Epoch 1321/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0403 - val_loss: 62.1470\n",
      "Epoch 1322/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0048 - val_loss: 61.9652\n",
      "Epoch 1323/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0089 - val_loss: 61.9191\n",
      "Epoch 1324/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0122 - val_loss: 61.9807\n",
      "Epoch 1325/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0065 - val_loss: 61.9982\n",
      "Epoch 1326/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 52.0089 - val_loss: 61.9819\n",
      "Epoch 1327/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0063 - val_loss: 62.0828\n",
      "Epoch 1328/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0269 - val_loss: 62.1354\n",
      "Epoch 1329/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 53.92 - 0s 71us/step - loss: 52.0202 - val_loss: 62.0292\n",
      "Epoch 1330/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9923 - val_loss: 62.0934\n",
      "Epoch 1331/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9983 - val_loss: 62.2277\n",
      "Epoch 1332/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9891 - val_loss: 62.2624\n",
      "Epoch 1333/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9961 - val_loss: 62.2471\n",
      "Epoch 1334/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 52.0084 - val_loss: 62.3034\n",
      "Epoch 1335/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0006 - val_loss: 62.1995\n",
      "Epoch 1336/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9852 - val_loss: 62.1751\n",
      "Epoch 1337/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 52.0066 - val_loss: 62.1248\n",
      "Epoch 1338/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9856 - val_loss: 62.2157\n",
      "Epoch 1339/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9830 - val_loss: 62.2169\n",
      "Epoch 1340/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9780 - val_loss: 62.1484\n",
      "Epoch 1341/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9883 - val_loss: 62.0440\n",
      "Epoch 1342/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9830 - val_loss: 62.0568\n",
      "Epoch 1343/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 51.9763 - val_loss: 62.0075\n",
      "Epoch 1344/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9748 - val_loss: 61.8919\n",
      "Epoch 1345/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9796 - val_loss: 61.8379\n",
      "Epoch 1346/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9891 - val_loss: 61.9147\n",
      "Epoch 1347/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 51.9622 - val_loss: 62.1479\n",
      "Epoch 1348/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9883 - val_loss: 62.3275\n",
      "Epoch 1349/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9846 - val_loss: 62.2861\n",
      "Epoch 1350/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 52.0012 - val_loss: 62.1751\n",
      "Epoch 1351/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9657 - val_loss: 62.1788\n",
      "Epoch 1352/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 51.9665 - val_loss: 62.1739\n",
      "Epoch 1353/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 51.9875 - val_loss: 62.1054\n",
      "Epoch 1354/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9583 - val_loss: 61.8753\n",
      "Epoch 1355/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 51.9733 - val_loss: 61.7685\n",
      "Epoch 1356/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9734 - val_loss: 61.8199\n",
      "Epoch 1357/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9677 - val_loss: 61.8851\n",
      "Epoch 1358/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9595 - val_loss: 61.9516\n",
      "Epoch 1359/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9524 - val_loss: 61.9796\n",
      "Epoch 1360/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9497 - val_loss: 62.0641\n",
      "Epoch 1361/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9475 - val_loss: 62.1952\n",
      "Epoch 1362/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9666 - val_loss: 62.3018\n",
      "Epoch 1363/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9618 - val_loss: 62.1645\n",
      "Epoch 1364/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9727 - val_loss: 61.9343\n",
      "Epoch 1365/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9509 - val_loss: 61.8465\n",
      "Epoch 1366/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9465 - val_loss: 61.8970\n",
      "Epoch 1367/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9427 - val_loss: 61.9417\n",
      "Epoch 1368/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 51.9590 - val_loss: 62.0094\n",
      "Epoch 1369/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9377 - val_loss: 61.9280\n",
      "Epoch 1370/1500\n",
      "99/99 [==============================] - ETA: 0s - loss: 51.06 - 0s 81us/step - loss: 51.9447 - val_loss: 61.8697\n",
      "Epoch 1371/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9962 - val_loss: 61.7556\n",
      "Epoch 1372/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9483 - val_loss: 61.8974\n",
      "Epoch 1373/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9408 - val_loss: 61.9872\n",
      "Epoch 1374/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9349 - val_loss: 61.9873\n",
      "Epoch 1375/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9332 - val_loss: 61.9125\n",
      "Epoch 1376/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9377 - val_loss: 61.8905\n",
      "Epoch 1377/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9267 - val_loss: 61.9413\n",
      "Epoch 1378/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9297 - val_loss: 61.9502\n",
      "Epoch 1379/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9462 - val_loss: 62.0340\n",
      "Epoch 1380/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9293 - val_loss: 62.2767\n",
      "Epoch 1381/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9393 - val_loss: 62.3268\n",
      "Epoch 1382/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9478 - val_loss: 62.2539\n",
      "Epoch 1383/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9285 - val_loss: 62.1641\n",
      "Epoch 1384/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9221 - val_loss: 62.0621\n",
      "Epoch 1385/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9231 - val_loss: 61.8789\n",
      "Epoch 1386/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9238 - val_loss: 61.7942\n",
      "Epoch 1387/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9474 - val_loss: 61.8318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1388/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9413 - val_loss: 62.0366\n",
      "Epoch 1389/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 51.9469 - val_loss: 62.1066\n",
      "Epoch 1390/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9133 - val_loss: 61.9892\n",
      "Epoch 1391/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9347 - val_loss: 61.8877\n",
      "Epoch 1392/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9278 - val_loss: 61.9716\n",
      "Epoch 1393/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9355 - val_loss: 62.2241\n",
      "Epoch 1394/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9292 - val_loss: 62.2838\n",
      "Epoch 1395/1500\n",
      "99/99 [==============================] - 0s 111us/step - loss: 51.9169 - val_loss: 62.1633\n",
      "Epoch 1396/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9108 - val_loss: 62.0624\n",
      "Epoch 1397/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9060 - val_loss: 61.9737\n",
      "Epoch 1398/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9029 - val_loss: 61.8449\n",
      "Epoch 1399/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9083 - val_loss: 61.8102\n",
      "Epoch 1400/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9171 - val_loss: 61.8545\n",
      "Epoch 1401/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9063 - val_loss: 62.0682\n",
      "Epoch 1402/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9469 - val_loss: 62.2136\n",
      "Epoch 1403/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9155 - val_loss: 62.1394\n",
      "Epoch 1404/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8991 - val_loss: 62.1509\n",
      "Epoch 1405/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9027 - val_loss: 62.1189\n",
      "Epoch 1406/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9086 - val_loss: 62.1234\n",
      "Epoch 1407/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9529 - val_loss: 62.1943\n",
      "Epoch 1408/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8911 - val_loss: 62.0155\n",
      "Epoch 1409/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 51.8791 - val_loss: 61.7723\n",
      "Epoch 1410/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8898 - val_loss: 61.6611\n",
      "Epoch 1411/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9097 - val_loss: 61.6678\n",
      "Epoch 1412/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9119 - val_loss: 61.7766\n",
      "Epoch 1413/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8984 - val_loss: 62.0370\n",
      "Epoch 1414/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8935 - val_loss: 62.1809\n",
      "Epoch 1415/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9050 - val_loss: 62.2271\n",
      "Epoch 1416/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 51.8883 - val_loss: 62.1340\n",
      "Epoch 1417/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9069 - val_loss: 61.9384\n",
      "Epoch 1418/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 51.8764 - val_loss: 61.8543\n",
      "Epoch 1419/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8777 - val_loss: 61.8174\n",
      "Epoch 1420/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8891 - val_loss: 61.8469\n",
      "Epoch 1421/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8654 - val_loss: 62.0463\n",
      "Epoch 1422/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8658 - val_loss: 62.2751\n",
      "Epoch 1423/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8780 - val_loss: 62.4916\n",
      "Epoch 1424/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9186 - val_loss: 62.6542\n",
      "Epoch 1425/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9732 - val_loss: 62.7159\n",
      "Epoch 1426/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.9586 - val_loss: 62.5092\n",
      "Epoch 1427/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8987 - val_loss: 62.1952\n",
      "Epoch 1428/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9161 - val_loss: 61.7268\n",
      "Epoch 1429/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8951 - val_loss: 61.5364\n",
      "Epoch 1430/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.9193 - val_loss: 61.6813\n",
      "Epoch 1431/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8865 - val_loss: 62.0175\n",
      "Epoch 1432/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8648 - val_loss: 62.2491\n",
      "Epoch 1433/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.9393 - val_loss: 62.3750\n",
      "Epoch 1434/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8819 - val_loss: 62.1447\n",
      "Epoch 1435/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 51.8560 - val_loss: 61.9580\n",
      "Epoch 1436/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8879 - val_loss: 61.7533\n",
      "Epoch 1437/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.8674 - val_loss: 61.7615\n",
      "Epoch 1438/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8543 - val_loss: 61.8959\n",
      "Epoch 1439/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8461 - val_loss: 62.0300\n",
      "Epoch 1440/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8757 - val_loss: 62.2077\n",
      "Epoch 1441/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8670 - val_loss: 62.2091\n",
      "Epoch 1442/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8540 - val_loss: 62.0267\n",
      "Epoch 1443/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8482 - val_loss: 61.8015\n",
      "Epoch 1444/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 51.8444 - val_loss: 61.5951\n",
      "Epoch 1445/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8649 - val_loss: 61.4181\n",
      "Epoch 1446/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8862 - val_loss: 61.5261\n",
      "Epoch 1447/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8544 - val_loss: 61.7158\n",
      "Epoch 1448/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.8574 - val_loss: 61.8734\n",
      "Epoch 1449/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8814 - val_loss: 61.9021\n",
      "Epoch 1450/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.8589 - val_loss: 61.7152\n",
      "Epoch 1451/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8347 - val_loss: 61.6199\n",
      "Epoch 1452/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.8444 - val_loss: 61.5802\n",
      "Epoch 1453/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8356 - val_loss: 61.4896\n",
      "Epoch 1454/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8410 - val_loss: 61.5350\n",
      "Epoch 1455/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8557 - val_loss: 61.6195\n",
      "Epoch 1456/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8259 - val_loss: 61.5784\n",
      "Epoch 1457/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.8339 - val_loss: 61.5861\n",
      "Epoch 1458/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8208 - val_loss: 61.7439\n",
      "Epoch 1459/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8289 - val_loss: 61.8980\n",
      "Epoch 1460/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8366 - val_loss: 61.9876\n",
      "Epoch 1461/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8574 - val_loss: 61.9634\n",
      "Epoch 1462/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8685 - val_loss: 61.7068\n",
      "Epoch 1463/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8193 - val_loss: 61.5938\n",
      "Epoch 1464/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.8311 - val_loss: 61.5074\n",
      "Epoch 1465/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 51.8386 - val_loss: 61.5631\n",
      "Epoch 1466/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8269 - val_loss: 61.5109\n",
      "Epoch 1467/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8223 - val_loss: 61.6175\n",
      "Epoch 1468/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8105 - val_loss: 61.8068\n",
      "Epoch 1469/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8502 - val_loss: 62.0292\n",
      "Epoch 1470/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8284 - val_loss: 62.0304\n",
      "Epoch 1471/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 51.8265 - val_loss: 62.0216\n",
      "Epoch 1472/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8218 - val_loss: 61.8979\n",
      "Epoch 1473/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8226 - val_loss: 61.8010\n",
      "Epoch 1474/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.7974 - val_loss: 61.5531\n",
      "Epoch 1475/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8069 - val_loss: 61.3312\n",
      "Epoch 1476/1500\n",
      "99/99 [==============================] - 0s 61us/step - loss: 51.8411 - val_loss: 61.3449\n",
      "Epoch 1477/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8306 - val_loss: 61.5531\n",
      "Epoch 1478/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8541 - val_loss: 61.7917\n",
      "Epoch 1479/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.8084 - val_loss: 61.8101\n",
      "Epoch 1480/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.8253 - val_loss: 61.9006\n",
      "Epoch 1481/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8029 - val_loss: 61.8345\n",
      "Epoch 1482/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8030 - val_loss: 61.7179\n",
      "Epoch 1483/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.7872 - val_loss: 61.4753\n",
      "Epoch 1484/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.8077 - val_loss: 61.3266\n",
      "Epoch 1485/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 51.8284 - val_loss: 61.4231\n",
      "Epoch 1486/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.8007 - val_loss: 61.5531\n",
      "Epoch 1487/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.7914 - val_loss: 61.6288\n",
      "Epoch 1488/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.7875 - val_loss: 61.6229\n",
      "Epoch 1489/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.7858 - val_loss: 61.6194\n",
      "Epoch 1490/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.7842 - val_loss: 61.6093\n",
      "Epoch 1491/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.7833 - val_loss: 61.5628\n",
      "Epoch 1492/1500\n",
      "99/99 [==============================] - 0s 101us/step - loss: 51.7866 - val_loss: 61.5077\n",
      "Epoch 1493/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.7803 - val_loss: 61.5913\n",
      "Epoch 1494/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.7948 - val_loss: 61.7238\n",
      "Epoch 1495/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.7881 - val_loss: 61.7265\n",
      "Epoch 1496/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.7927 - val_loss: 61.7967\n",
      "Epoch 1497/1500\n",
      "99/99 [==============================] - 0s 81us/step - loss: 51.7914 - val_loss: 61.7367\n",
      "Epoch 1498/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.7791 - val_loss: 61.7488\n",
      "Epoch 1499/1500\n",
      "99/99 [==============================] - 0s 91us/step - loss: 51.7816 - val_loss: 61.7926\n",
      "Epoch 1500/1500\n",
      "99/99 [==============================] - 0s 71us/step - loss: 51.7804 - val_loss: 61.7506\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=64, epochs=1500,\n",
    "          validation_data=(X_val, Y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
